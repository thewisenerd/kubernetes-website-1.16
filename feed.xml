<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Production-Grade Container Orchestration</title>
    <link>https://kubernetes.io/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://kubernetes.io/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: When you&#39;re in the release team, you&#39;re family: the Kubernetes 1.16 release interview</title>
      <link>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</guid>
      <description>
        
        
        &lt;p&gt;&lt;b&gt;Author&lt;/b&gt;: Craig Box (Google)&lt;/p&gt;

&lt;p&gt;It is a pleasure to co-host the weekly &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; with Adam Glick.  We get to talk to friends old and new from the community, as well as give people a download on the Cloud Native news every week.&lt;/p&gt;

&lt;p&gt;It was also a pleasure to see Lachlan Evenson, the release team lead for Kubernetes 1.16, &lt;a href=&#34;https://www.cncf.io/announcement/2019/11/19/cloud-native-computing-foundation-announces-2019-community-awards-winners/&#34; target=&#34;_blank&#34;&gt;win the CNCF &amp;ldquo;Top Ambassador&amp;rdquo; award&lt;/a&gt; at KubeCon.  We &lt;a href=&#34;https://kubernetespodcast.com/episode/072-kubernetes-1.16/&#34; target=&#34;_blank&#34;&gt;talked with Lachie&lt;/a&gt; when 1.16 was released, and as is &lt;a href=&#34;https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/&#34; target=&#34;_blank&#34;&gt;becoming&lt;/a&gt; a &lt;a href=&#34;https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/&#34; target=&#34;_blank&#34;&gt;tradition&lt;/a&gt;, we are delighted to share an abridged version of that interview with the readers of the Kubernetes Blog.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re paying attention to the release calendar, you&amp;rsquo;ll see 1.17 is due out soon. &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;Subscribe to our show&lt;/a&gt; in your favourite podcast player for another release interview!&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Lachie, I&amp;rsquo;ve been looking forward to chatting to you for some time. We first met at KubeCon Berlin in 2017 when you were with Deis. Let&amp;rsquo;s start with a question on everyone&amp;rsquo;s ears&amp;ndash; which part of England are you from?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The prison part! See, we didn&amp;rsquo;t have a choice about going to Australia, but I&amp;rsquo;d like to say we got the upper hand in the long run. We got that beautiful country, so yes, from Australia, the southern part of England&amp;ndash; the southern tip.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We did set that question up a little bit. I&amp;rsquo;m actually in Australia this week, and I&amp;rsquo;ll let you know it&amp;rsquo;s quite a nice place. I can&amp;rsquo;t imagine why you would have left.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, it seems fitting that you&amp;rsquo;re interviewing an Australian from Australia, and that Australian is in San Francisco.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Oh, well, thank you very much for joining us and making it work. This is the third in our occasional series of release lead interviews. We talked to Josh and Tim from Red Hat and VMware, respectively, in &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;episode 10&lt;/a&gt;, and we talked to Aaron from Google in &lt;a href=&#34;https://kubernetespodcast.com/episode/046-kubernetes-1.14/&#34; target=&#34;_blank&#34;&gt;episode 46&lt;/a&gt;. And we asked all three how their journey in cloud-native started. What was your start in cloud-native?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I remember back in early 2014, I was working for a company called Lithium Technologies. We&amp;rsquo;d been using containers for quite some time, and my boss at the time had put a challenge out to me&amp;ndash; go and find a way to orchestrate these containers, because they seem to be providing quite a bit of value to our developer velocity.&lt;/p&gt;

&lt;p&gt;He gave me a week, and he said, go and check out both Mesos and Kubernetes. And at the end of that week, I had Kubernetes up and running, and I had workloads scheduled. I was a little bit more challenged on the Mesos side, but Kubernetes was there, and I had it up and running. And from there, I actually went and was offered to speak at the Kubernetes 1.0 launch in OSCOM in Portland in 2014, I believe.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: So, a real early adopter?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Really, really early. I remember, I think, I started in 0.8, before CrashLoopBackOff was a thing. I remember writing that thing myself.&lt;/p&gt;

&lt;p&gt;[LAUGHING]&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: You were contributing to the code at that point as well?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I was just a user. I was part of the community at that point, but from a user perspective. I showed up to things like the community meeting. I remember meeting Sarah Novotny in the very early years of the community meeting, and I spent some time in SIG Apps, so really looking at how people were putting workloads onto Kubernetes&amp;ndash; so going through that whole process.&lt;/p&gt;

&lt;p&gt;It turned out we built some tools like Helm, before Helm existed, to facilitate rollout and putting applications onto Kubernetes. And then, once Helm existed, that&amp;rsquo;s when I met the folks from Deis, and I said, hey, I think you want to get rid of this code that we&amp;rsquo;ve built internally and then go and use the open-source code that Helm provided.&lt;/p&gt;

&lt;p&gt;So we got into the Helm ecosystem there, and I subsequently went and worked for Deis, specifically on professional services&amp;ndash; helping people out in the community with their Kubernetes journey. And that was when we actually met, Craig, back in Berlin. It seems, you know, I say container years are like dog years; it&amp;rsquo;s 7:1.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Right.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Seven years ago, we were about 50 years&amp;ndash; much younger.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That sounds like the same ratio as kangaroos to people in Australia.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It&amp;rsquo;s much the same arithmetic, yes.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What was the most interesting implementation that you ran into at that time?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: There wasn&amp;rsquo;t a lot of the workload APIs. Back in 1.0, there wasn&amp;rsquo;t even Deployments. There wasn&amp;rsquo;t Ingress. Back in the day, there were a lot of people in those points trying to build those workload APIs on top of Kubernetes, but they didn&amp;rsquo;t actually have any way to extend Kubernetes itself. There were no third-party resources. There were no operators, no custom resources.&lt;/p&gt;

&lt;p&gt;A lot of people are actually trying to figure out how to interact with the Kubernetes API and deliver things like deployments, because you just had&amp;ndash; in those days, you didn&amp;rsquo;t have replica sets. You had a ReplicationController that we called the RC, back in the day. You didn&amp;rsquo;t have a lot of these things that we take for granted today. There wasn&amp;rsquo;t RBAC. There wasn&amp;rsquo;t a lot of the things that we have today.&lt;/p&gt;

&lt;p&gt;So it&amp;rsquo;s great to have seen and been a part of the Kubernetes community from 0.8 to 1.16, and actually leading that release. So I&amp;rsquo;ve seen a lot, and it&amp;rsquo;s been a wonderful part of my adventures in open-source.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You were also part of the Deis team that transitioned and became a part of the Microsoft team. What was that transition like, from small startup to joining a large player in the cloud and technology community?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It was fantastic. When we came on board with Microsoft, they didn&amp;rsquo;t have a managed Kubernetes offering, and we were brought on to try and seed that. There was also a bigger part that we were actually building open-source tools to help people in the community integrate. We had the autonomy with&amp;ndash; Brendan Burns was on the team. We had Gabe Monroy. And we really had that top-down autonomy that was believing and placing a bet on open-source and helping us build tools and give us that autonomy to go and solve problems in open-source, along with contributing to things like Kubernetes.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m part of the upstream team from a PM perspective, and we have a bunch of engineers, a bunch of PMs that are actually working on these things in the Cloud Native Compute Foundation to help folks integrate their workloads into things like Kubernetes and build and aid their cloud-native journeys.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: There are a number of new tools, and specifications, and so on that are still coming out from Microsoft under the Deis brand. That must be exciting to you as one of the people who joined from Deis initially.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, absolutely. We really took that Deis brand&amp;ndash; it&amp;rsquo;s now Deis Labs&amp;ndash; but we really wanted this a home to signal to the community that we were building things in the hope to put them out into foundation. You may see things like CNAB, Cloud Native Application Bundles. I know &lt;a href=&#34;https://kubernetespodcast.com/episode/061-cnab/&#34; target=&#34;_blank&#34;&gt;you&amp;rsquo;ve had both Ralph and Jeremy on the show before&lt;/a&gt; talking about CNAB, SMI - Service Mesh Interface, other tooling in the ecosystem where we want to signal to the community that we want to go give that to a foundation. We really want a neutral place to begin that nascent work, but then things, for example, Virtual Kubelet started there as well, and it went out into the Cloud Native Compute Foundation.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Is there any consternation about the fact that Phippy has become the character people look to rather than the actual &amp;ldquo;Captain Kube&amp;rdquo; owl, in the &lt;a href=&#34;https://www.cncf.io/phippy/&#34; target=&#34;_blank&#34;&gt;family of donated characters&lt;/a&gt;?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yes, so it&amp;rsquo;s interesting because I didn&amp;rsquo;t actually work on that project back at Deis, but the Deis folks, Karen Chu and Matt Butcher actually created &amp;ldquo;The Children&amp;rsquo;s Guide to Kubernetes,&amp;rdquo; which I thought was fantastic.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Totally.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Because I could sit down and read it to my parents, as well, and tell them&amp;ndash; it wasn&amp;rsquo;t for children. It was more for the adults in my life, I like to say. And so when I give out a copy of that book, I&amp;rsquo;m like, take it home and read it to mum. She might actually understand what you do by the end of that book.&lt;/p&gt;

&lt;p&gt;But it was really a creative way, because this was back in that nascent Kubernetes where people were trying to get their head around those concepts&amp;ndash; what is a pod? What is a secret? What is a namespace? Having that vehicle of a fun set of characters&amp;ndash;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Yep.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: And Phippy is a PHP app. Remember them? So yeah, it&amp;rsquo;s totally in line with the things that we&amp;rsquo;re seeing people want to containerize and put onto Kubernetes at that. But Phippy is still cute. I was questioned last week about Captain Kube, as well, on the release logo, so we could talk about that a little bit more. But there&amp;rsquo;s a swag of characters in there that are quite cute and illustrate the fun concept behind the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: &lt;a href=&#34;https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/&#34; target=&#34;_blank&#34;&gt;1.16 has just been released&lt;/a&gt;. You were the release team lead for that&amp;ndash; congratulations.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Thank you very much. It was a pleasure to serve the community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: What are the headline announcements in Kubernetes 1.16?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Well, I think there are a few. Custom Resources hit GA. Now, that is a big milestone for extensibility and Kubernetes. I know we&amp;rsquo;ve spoken about them for some time&amp;ndash; custom resources were introduced in 1.7, and we&amp;rsquo;ve been trying to work through that ecosystem to bring the API up to a GA standard. So it hit GA, and I think a lot of the features that went in as part of the GA release will help people in the community that are writing operators.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot of lifecycle management, a lot of tooling that you can put into the APIs themselves. Doing strict dependency checks&amp;ndash; you can do typing, you can do validation, you can do pruning superfluous fields, and allowing for that ecosystem of operators and extensibility in the community to exist on top of Kubernetes.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been a long road to get to GA for Custom Resources, but it&amp;rsquo;s great now that they&amp;rsquo;re here and people can really bank on that being an API they can use to extend Kubernetes. So I&amp;rsquo;d say that&amp;rsquo;s a large headline feature. The metrics overhaul, as well&amp;ndash; I know this was on the release blog.&lt;/p&gt;

&lt;p&gt;The metrics team have actually tried to standardize the metrics in Kubernetes and put them through the same paces as all other enhancements that go into Kubernetes. So they&amp;rsquo;re really trying to put through, what are the criteria? How do we make them standard? How do we test them? How to make sure that they&amp;rsquo;re extensible? So it was great to see that team actually step up and create stable metrics that everybody can build and stack on.&lt;/p&gt;

&lt;p&gt;Finally, there were some other additions to CSI, as well. Volume resizing was added. This is a maturity story around the Container Storage Interface, which was introduced several releases ago in GA. But really, you&amp;rsquo;ve seen volume providers actually build on that interface and that interface get a little bit more broader to adopt things like &amp;ldquo;I want to resize dynamically at runtime on my storage volume&amp;rdquo;.  That&amp;rsquo;s a great story as well, for those providers out there.&lt;/p&gt;

&lt;p&gt;I think they&amp;rsquo;re the big headline features for 1.16, but there are a slew. There were 31 enhancements that went into Kubernetes 1.16. And I know there have been questions out there in the community saying, well, how do we decide what&amp;rsquo;s stable? Eight of those were stable, eight of those were beta, and the rest of those features, the 15 remaining, were actually in alpha. There were quite a few things that went from alpha into beta and beta into stable, so I think that&amp;rsquo;s a good progression for the release, as well.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: As you&amp;rsquo;ve looked at all these, which of them is your personal favorite?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I probably have two. One is a little bit biased, but I personally worked on, with the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dual-stack/&#34; target=&#34;_blank&#34;&gt;dual-stack&lt;/a&gt; team in the community. Dual-stack is the ability to give IPv4 and IPv6 addresses to both pods and services. And I think where this is interesting in the community is Kubernetes is becoming a runtime that is going to new spaces. Think IoT, think edge, think cloud edge.&lt;/p&gt;

&lt;p&gt;When you&amp;rsquo;re pushing Kubernetes into these new operational environments, things like addressing may become a problem, where you might want to run thousands and thousands of pods which all need IP addresses. So, having that same crossover point where I can have v4 and v6 at the same time, get comfortable with v6, I think Kubernetes may be an accelerator to v6 adoption through things like IoT workloads on top of Kubernetes.&lt;/p&gt;

&lt;p&gt;The other one is &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/&#34; target=&#34;_blank&#34;&gt;Endpoint Slices&lt;/a&gt;. Endpoint slices is about scaling. As you may know, services have endpoints attached to them, and endpoints are all the pod IPs that actually match that label selector on a service. Now, when you have large clusters, you can imagine the number of pod IPs being attached to that service growing to tens of thousands. And when you update that, everything that actually watches those service endpoints needs to get an update, which is the delta change over time, which gets rather large as things are being attached, added, and removed, as is the dynamic nature of Kubernetes.&lt;/p&gt;

&lt;p&gt;But what endpoint slices makes available is you can actually slice those endpoints up into groups of 100 and then only update the ones that you really need to worry about, which means as a scaling factor, we don&amp;rsquo;t need to update everybody listening into tens of thousands of updates. We only need to update a subsection. So I&amp;rsquo;d say they&amp;rsquo;re my two highlights, yeah.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Are there any early stage or alpha features that you&amp;rsquo;re excited to see where they go personally?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Personally, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/&#34; target=&#34;_blank&#34;&gt;ephemeral containers&lt;/a&gt;. The tooling that you have available at runtime in a pod is dependent on the constituents or the containers that are part of that pod. And what we&amp;rsquo;ve seen in containers being built by scratch and tools like &lt;a href=&#34;https://github.com/GoogleContainerTools/distroless&#34; target=&#34;_blank&#34;&gt;distroless&lt;/a&gt; from the folks out of Google, where you can build scratch containers that don&amp;rsquo;t actually have any tooling inside them but just the raw compiled binaries, if you want to go in and debug that at runtime, it&amp;rsquo;s incredibly difficult to insert something in.&lt;/p&gt;

&lt;p&gt;And this is where ephemeral containers come in. I can actually insert a container into a running pod&amp;ndash; and let&amp;rsquo;s just call that a debug container&amp;ndash; that has all my slew of tools that I need to debug that running workload, and I can insert that into a pod at runtime. So I think ephemeral containers is a really interesting feature that&amp;rsquo;s been included in 1.16 in alpha, which allows a greater debugging story for the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What feature that slipped do you wish would have made it into the release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The feature that slipped that I was a little disappointed about was &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md&#34; target=&#34;_blank&#34;&gt;sidecar containers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Right.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: In the world of service meshes, you may want to order the start of some containers, and it&amp;rsquo;s very specific to things like service meshes in the case of the data plane. I need the Envoy sidecar to start before everything else so that it can wire up the networking.&lt;/p&gt;

&lt;p&gt;The inverse is true as well. I need it to stop last. Sidecar containers gave you that ordered start. And what we see a lot of people doing in the ecosystem is just laying down one sidecar per node as a DaemonSet, and they want that to start before all the other pods on the machine. Or if it&amp;rsquo;s inside the pod, or the context of one pod, they want to say that sidecar needs to stop before all the other containers in a pod. So giving you that ordered guarantee, I think, is really interesting and is really hot, especially given the service mesh ecosystem heating up.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: This release &lt;a href=&#34;https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/&#34; target=&#34;_blank&#34;&gt;deprecates a few beta API groups&lt;/a&gt;, for things like ReplicaSets and Deployments. That will break deployment for the group of people who have just taken example code off the web and don&amp;rsquo;t really understand it. The GA version of these APIs were released in 1.9, so it&amp;rsquo;s obviously a long time ago. There&amp;rsquo;s been a lot of preparation going into this. But what considerations and concerns have we had about the fact that these are now being deprecated in this particular release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Let me start by saying that this is the first release that we&amp;rsquo;ve had a big API deprecation, so the proof is going to be in the pudding. And we do have an API deprecation policy. So as you mentioned, Craig, the apps/v1 group has been around since 1.9. If you go and read the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;API deprecation policy&lt;/a&gt;, you can see that we have a three-release announcement. Around the 1.12, 1.13 time frame, we actually went and announced this deprecation, and over the last few releases, we&amp;rsquo;ve been reiterating that.&lt;/p&gt;

&lt;p&gt;But really, what we want to do is get the whole community on those stable APIs because it really starts to become a problem when we&amp;rsquo;re supporting all these many now-deprecated APIs, and people are building tooling around them and trying to build reliable tooling. So this is the first test for us to move people, and I&amp;rsquo;m sure it will break a lot of tools that depend on things. But I think in the long run, once we get onto those stable APIs, people can actually guarantee that their tools work, and it&amp;rsquo;s going to become easier in the long run.&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ve put quite a bit of work in announcing this. There was a blog sent out about six months ago by Valerie Lancey in the Kubernetes community which said, hey, go use &amp;lsquo;kubectl convert&amp;rsquo;, where you can actually say, I want to convert this resource from this API version to that API version, and it actually makes that really easy. But I think there&amp;rsquo;ll be some problems in the ecosystem, but we need to do this going forward, pruning out the old APIs and making sure that people are on the stable ones.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Congratulations on the release of 1.16. Obviously, that&amp;rsquo;s a big thing. It must have been a lot of work for you. Can you talk a little bit about what went into leading this release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The job of the release lead is to oversee throughout the process of the release and make sure that the release gets out the door on a specific schedule. So really, what that is is wrangling a lot of different resources and a lot of different people in the community, and making sure that they show up and do the things that they are committed to as part of their duties as either SIG chairs or other roles in the community, and making sure that enhancements are in the right state, and code shows up at the right time, and that things are looking green.&lt;/p&gt;

&lt;p&gt;A lot of it is just making sure you know who to contact and how to contact them, and ask them to actually show up. But when I was asked at the end of the 1.15 release cycle if I would lead, you have to consider how much time it&amp;rsquo;s going to take and the scheduling, where hours a week are dedicated to making sure that this release actually hits the shelves on time and is of a certain quality. So there is lots of pieces to that.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Had you been on the path through the shadow program for release management?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, I had. I actually joined the shadow program&amp;ndash; so the shadow program for the release team. The Kubernetes release team is tasked with staffing a specific release, and I came in the 1.14 release under the lead of Aaron Crickenberger. And I was an enhancement shadow at that point. I was really interested in how KEPs worked, so the Kubernetes Enhancement Proposal work. I wanted to make sure that I understood that part of the release team, and I came in and helped in that release.&lt;/p&gt;

&lt;p&gt;And then, in 1.15, I was asked if I could be a lead shadow. And the lead shadow is to stand alongside the lead and help the lead fill their duties. So if they&amp;rsquo;re out, if they need people to wrangle different parts of the community, I would go out and do that. I&amp;rsquo;ve served on three releases at this point&amp;ndash; 1.14, 1.15, and 1.16.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Thank you for your service.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Absolutely, it&amp;rsquo;s my pleasure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Release lead emeritus is the next role for you, I assume?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: [LAUGHS] Yes. We also have a new role on the release lead team called Emeritus Advisors, which are actually to go back and help answer the questions of, why was this decision made? How can we do better? What was this like in the previous release? So we do have that continuity, and in 1.17, we have the old release lead from 1.15. Claire Lawrence is coming back to fill in as emeritus advisor. So that is something we do take.&lt;/p&gt;

&lt;p&gt;And I think for the shadow program in general, the release team is a really good example of how you can actually build continuity across releases in an open-source fashion. We &lt;a href=&#34;https://www.youtube.com/watch?v=ritHCLd2xeE&#34; target=&#34;_blank&#34;&gt;actually have a session at KubeCon San Diego&lt;/a&gt; on how that shadowing program works. But it&amp;rsquo;s really to get people excited about how we can do mentoring in open-source communities and make sure that the project goes on after all of us have rolled on and off the team.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Speaking of the team, &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.16&#34; target=&#34;_blank&#34;&gt;there were 32 people involved&lt;/a&gt;, including yourself, in this release. What is it like to coordinate that group? That sounds like a full time job.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It is a full time job. And let me say that this release team in 1.16 represented five different continents. We can count Antarctica as not having anybody, but we didn&amp;rsquo;t have anybody from South America for that release, which was unfortunate. But we had people from Australia, China, India, Tanzania. We have a good spread&amp;ndash; Europe, North America. It&amp;rsquo;s great to have that spread and that continuity, which allowed for us to get things done throughout the day.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Until you want to schedule a meeting.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Scheduling a meeting was extremely difficult. Typically, on the release team, we run one Europe, Western Europe, and North American-friendly meeting, and then we ask the team if they would like to hold another meeting. Now, in the case of 1.16, they didn&amp;rsquo;t want to hold another meeting. We actually put it out to survey. But in previous releases, we held an EU in the morning so that people in India, as well, or maybe even late-night in China, could be involved.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Any interesting facts about the team, besides the incredible geographic diversity that you had, to work around that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I really appreciate about the release team that we&amp;rsquo;re from all different backgrounds, from all different parts of the world and all different companies. There are people who are doing this on their own time, There are people who are doing this on company time, but we all come together with that shared common goal of shipping that release.&lt;/p&gt;

&lt;p&gt;This release was we had the five continents. It was really exciting in 1.17 that we have in the lead roles, it was represented mainly by women. So 1.17, watch out&amp;ndash; most of the leads for 1.17 are women, which is a great result, and that&amp;rsquo;s through that shadow program that we can foster different types of talent. I&amp;rsquo;m excited to see future releases benefiting from different diverse groups of people from the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: What are you going to put in the proverbial envelope for the 1.17 team?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: We&amp;rsquo;ve had this theme of a lot of roles in the release team being cut and dry, right? We have these release handbooks, so for each of the members of the team, they&amp;rsquo;re cut into different roles. There&amp;rsquo;s seven different roles on the team. There&amp;rsquo;s the lead. There&amp;rsquo;s the CI signal role. There&amp;rsquo;s bug triage. There&amp;rsquo;s comms. There&amp;rsquo;s docs. And there&amp;rsquo;s release notes. And there&amp;rsquo;s also the release branch managers who actually cut the code and make sure that they have shipped and it ends up in all the repositories.&lt;/p&gt;

&lt;p&gt;What we did in the previous 1.15, we actually had a role call the test-infra role. And thanks to the wonderful work of the folks of the test-infra team out of Google&amp;ndash; &lt;a href=&#34;https://kubernetespodcast.com/episode/077-eng-prod-and-testing/&#34; target=&#34;_blank&#34;&gt;Katharine Berry&lt;/a&gt;, and &lt;a href=&#34;https://kubernetespodcast.com/episode/069-kind/&#34; target=&#34;_blank&#34;&gt;Ben Elder&lt;/a&gt;, and other folks&amp;ndash; they actually automated this role completely that we could get rid of it in the 1.16 release and still have our same&amp;ndash; and be able to get a release out the door.&lt;/p&gt;

&lt;p&gt;I think a lot of these things are ripe for automation, and therefore, we can have a lot less of a footprint going forward. Let&amp;rsquo;s automate the bits of the process that we can and actually refine the process to make sure that the people that are involved are not doing the repetitive tasks over and over again. In the era of enhancements, we could streamline that process. CI signal and bug triage, there are places we could actually go in and automate that as well. I think one place that&amp;rsquo;s been done really well in 1.16 was in the release notes.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know if you&amp;rsquo;ve seen &lt;a href=&#34;https://relnotes.k8s.io&#34; target=&#34;_blank&#34;&gt;relnotes.k8s.io&lt;/a&gt;, but you can go and check out the release notes and now, basically, annotated PRs show up as release notes that are searchable and sortable, all through an automated means, whereas that was previously some YAML jockeying to make sure that that would actually happen and be digestible to the users.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Come on, Lachie, all Kubernetes is just YAML jockeying.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;[LAUGHING]&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, but it&amp;rsquo;s great to have an outcome where we can actually make that searchable and get people out of the mundaneness of things like, let&amp;rsquo;s make sure we&amp;rsquo;re copying and pasting YAML from left to right.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: After the release, you had a &lt;a href=&#34;https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#heading=h.ipohe1hgr315&#34; target=&#34;_blank&#34;&gt;retrospective meeting&lt;/a&gt;. What was the takeaway from that meeting?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: At the end of each release, we do have a retrospective. It&amp;rsquo;s during the community meeting. That retrospective, it was good. I was just really excited to see that there were so many positives. It&amp;rsquo;s a typical retrospective where we go, what did we say we were going to do last release? Did we do that? What was great? What can we do better? And some actions out of that.&lt;/p&gt;

&lt;p&gt;It was great to see people giving other people on the team so many compliments. It was really, really deep and rich, saying, thank you for doing this, thank you for doing that. People showed up and pulled their weight in the release team, and other people were acknowledging that. That was great.&lt;/p&gt;

&lt;p&gt;I think one thing we want to do is&amp;ndash; we have a code freeze as part of the release process, which is where we make sure that code basically stops going into master in Kubernetes. Only things destined for the release can actually be put in there. But we don&amp;rsquo;t actually stop the test infrastructure from changing, so the test infrastructure has a lifecycle of its own.&lt;/p&gt;

&lt;p&gt;One of the things that was proposed was that we actually code freeze the test infrastructure as well, to make sure that we&amp;rsquo;re not actually looking at changes in the test-infra causing jobs to fail while we&amp;rsquo;re trying to stabilize the code. I think that&amp;rsquo;s something we have some high level agreement about, but getting down into the low-level nitty-gritty would be great in 1.17 and beyond.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: We talked about sidecar containers slipping out of this release. Most of the features are on a release train, and are put in when they&amp;rsquo;re ready. What does it mean for the process of managing a release when those things happen?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Basically, we have an enhancements freeze, and that says that enhancements&amp;ndash; so the KEPs that are backing these enhancements&amp;ndash; so the sidecar containers would have had an enhancement proposal. And the SIG that owns that code would then need to sign off and say that this is in a state called &amp;ldquo;implementable.&amp;rdquo; When we&amp;rsquo;ve agreed on the high-level details, you can go and proceed and implement that.&lt;/p&gt;

&lt;p&gt;Now, that had actually happened in the case of sidecar containers. The challenge was you still need to write the code and get the code actually implemented, and there&amp;rsquo;s a month gap between enhancement freeze and code freeze. If the code doesn&amp;rsquo;t show up, or the code shows up and needs to be reviewed a little bit more, you may miss that deadline.&lt;/p&gt;

&lt;p&gt;I think that&amp;rsquo;s what happened in the case of this specific feature. It went all the way through to code freeze, the code wasn&amp;rsquo;t complete at that time, and we basically had to make a call&amp;ndash; do we want to grant it an exception? In this case, they didn&amp;rsquo;t ask for an exception. They said, let&amp;rsquo;s just move it to 1.17.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s still a lot of people and SIGs show up at the start of a new release and put forward the whole release of all the things they want to ship, and obviously, throughout the release, a lot of those things get plucked off. I think we started with something like 60 enhancements, and then what we got out the door was 31. They either fall off as part of the enhancement freeze or as part of the code freeze, and that is absolutely typical of any release.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Do you think that a three-month wait is acceptable for something that might have had a one- or two-week slip, or would you like to see enhancements be able to be released in point releases between the three-month releases?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, there&amp;rsquo;s back and forth about this in the community, about how can we actually roll things at different cadences, I think, is the high-level question. Tim Hockin actually put out, how about we do stability cycles as well? Because there are a lot of new features going in, and there are a lot of stability features going in. But if you look at it, half of the features were beta or stable, and the other half were alpha, which means we&amp;rsquo;re still introducing a lot more complexity and largely untested code into alpha state&amp;ndash; which, as much as we wouldn&amp;rsquo;t like to admit, it does affect the stability of the system.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s talk of LTS. There&amp;rsquo;s talk of stability releases as well. I think they&amp;rsquo;re all things that are interesting now that Kubernetes has that momentum, and you are seeing a lot of things go to GA. People are like, &amp;ldquo;I don&amp;rsquo;t need to be drinking from the firehose as fast. I have CRDs in GA. I have all these other things in GA. Do I actually need to consume this at the rate?&amp;rdquo; So I think&amp;ndash; stay tuned. If you&amp;rsquo;re interested in those discussions, the upstream community is having those. Show up there and voice your opinion.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Is this the first release with its own &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/sig-release/master/releases/release-1.16/116_unlimited_breadsticks_for_all.png&#34; target=&#34;_blank&#34;&gt;release mascot&lt;/a&gt;?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I think that release mascot goes back to&amp;ndash; I would like to say 1.11? If you &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/README.md&#34; target=&#34;_blank&#34;&gt;go back to 1.11&lt;/a&gt;, you can actually see the different mascots. I remember 1.11 being &amp;ldquo;The Hobbit.&amp;rdquo; So it&amp;rsquo;s the Hobbiton front door of Bilbo Baggins with the Kubernetes Helm on the front of it, and that was called 11ty-one&amp;ndash;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Uh-huh.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: A long-expected release. So they go through from each release, and you can actually go check them out on the SIG release repository upstream.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: I do think this is the first time that&amp;rsquo;s managed to make it into a blog post, though.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I do think it is the case. I wanted to have a little bit of fun with the release team, so typically you will see the release teams have a t-shirt. I have, &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.14/README.md&#34; target=&#34;_blank&#34;&gt;from 1.14, the Caternetes&lt;/a&gt;, which Aaron designed, which has a bunch of cats kind of trying to look at a Kubernetes logo.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We had a fun conversation with Aaron about his love of cats.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: [LAUGHS] And it becomes a token of, hey, remember this hard work that you put together? It becomes a badge of honor for everybody that participated in the release.  I wanted to highlight it as a release mascot. I don&amp;rsquo;t think a lot of people knew that we did have those across the last few releases. But it&amp;rsquo;s just a bit of fun, and I wanted to put my own spin on things just so that the team could come together. A lot of it was around the laughs that we had as a team throughout this release&amp;ndash; and my love of Olive Garden.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Your love of Olive Garden feels like it may have become a meme to a community which might need a little explanation for our audience. For those who are not familiar with American fine dining, can we start with&amp;ndash; what exactly is Olive Garden?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Olive Garden is the finest Italian dining experience you will have in the continental United States. I see everybody&amp;rsquo;s faces saying, is he sure about that? I&amp;rsquo;m sure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That might require a slight justification on behalf of some of our Italian-American listeners.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Is it the unlimited breadsticks and salad that really does it for you, or is the plastic boat that it comes in?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I think it&amp;rsquo;s a combination of all three things. You know, the tour of Italy, you can&amp;rsquo;t go past. The free breadsticks are fantastic. But Olive Garden just represents the large chain restaurant and that kind of childhood I had growing up and thinking about these large-scale chain restaurants. You don&amp;rsquo;t get to choose your meme. And the legacy&amp;ndash; I would have liked to have had a different mascot.&lt;/p&gt;

&lt;p&gt;But I just had a run with the meme of Olive Garden. And this came about, I would like to say, about three or four months ago. Paris Pittman from Google, who is another member of the Kubernetes community, kind of put out there, what&amp;rsquo;s your favorite sit-down large-scale restaurant? And of course, I pitched in very early and said, it&amp;rsquo;s got to be the Olive Garden.&lt;/p&gt;

&lt;p&gt;And then everybody kind of jumped onto that. And my inbox is full of free Olive Garden gift certificates now, and it&amp;rsquo;s taken on a life of its own. And at this point, I&amp;rsquo;m just embracing it&amp;ndash; so much so that we might even have the 1.16 release party at an Olive Garden in San Diego, if it can accommodate 10,000 people.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: &lt;a href=&#34;https://www.youtube.com/watch?v=9ZJF5-EyjXs&#34; target=&#34;_blank&#34;&gt;When you&amp;rsquo;re there, are you family?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yes. Absolutely, absolutely. And I would have loved to put that. I think the release name was &amp;ldquo;unlimited breadsticks for all.&amp;rdquo; I would have liked to have done, &amp;ldquo;When you&amp;rsquo;re here, you&amp;rsquo;re family,&amp;rdquo; but that is, sadly, trademarked.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Aww. What&amp;rsquo;s next for you in the community?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I&amp;rsquo;ve really been looking at Cluster API a lot&amp;ndash; so building Kubernetes clusters on top of a declarative approach. I&amp;rsquo;ve been taking a look at what we can do in the Cluster API ecosystem. I&amp;rsquo;m also a chair of SIG PM, so helping foster the KEP process as well&amp;ndash; making sure that that continues to happen and continues to be fruitful for the community.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;i&gt;&lt;a href=&#34;https://twitter.com/lachlanevenson&#34; target=&#34;_blank&#34;&gt;Lachlan Evenson&lt;/a&gt; is a Principal Program Manager at Microsoft and an Australian living in the US, and most recently served as the Kubernetes 1.16 release team lead.&lt;/p&gt;

&lt;p&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34; target=&#34;_blank&#34;&gt;@kubernetespod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.&lt;/i&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Gardener Project Update</title>
      <link>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;mailto:rafael.franzke@sap.com&#34; target=&#34;_blank&#34;&gt;Rafael Franzke&lt;/a&gt; (SAP), &lt;a href=&#34;mailto:vasu.chandrasekhara@sap.com&#34; target=&#34;_blank&#34;&gt;Vasu
Chandrasekhara&lt;/a&gt; (SAP)&lt;/p&gt;

&lt;p&gt;Last year, we introduced &lt;a href=&#34;https://gardener.cloud&#34; target=&#34;_blank&#34;&gt;Gardener&lt;/a&gt; in the &lt;a href=&#34;https://www.youtube.com/watch?v=DpFTcTnBxbM&amp;amp;feature=youtu.be&amp;amp;t=1642&#34; target=&#34;_blank&#34;&gt;Kubernetes
Community
Meeting&lt;/a&gt;
and in a post on the &lt;a href=&#34;https://kubernetes.io/blog/2018/05/17/gardener/&#34; target=&#34;_blank&#34;&gt;Kubernetes
Blog&lt;/a&gt;. At SAP, we have been
running Gardener for more than two years, and are successfully managing
thousands of &lt;a href=&#34;https://k8s-testgrid.appspot.com/conformance-gardener&#34; target=&#34;_blank&#34;&gt;conformant&lt;/a&gt;
clusters in various versions on all major hyperscalers as well as in numerous
infrastructures and private clouds that typically join an enterprise via
acquisitions.&lt;/p&gt;

&lt;p&gt;We are often asked why a handful of dynamically scalable clusters would not
suffice. We also started our journey into Kubernetes with a similar mindset. But
we realized that applying the architecture and principles of Kubernetes to
productive scenarios, our internal and external customers very quickly required
the rational separation of concerns and ownership, which in most circumstances
led to the use of multiple clusters. Therefore, a scalable and managed
Kubernetes as a service solution is often also the basis for adoption.
Particularly, when a larger organization runs multiple products on different
providers and in different regions, the number of clusters will quickly rise to
the hundreds or even thousands.&lt;/p&gt;

&lt;p&gt;Today, we want to give an update on what we have implemented in the past year
regarding extensibility and customizability, and what we plan to work on for our
next milestone.&lt;/p&gt;

&lt;h2 id=&#34;short-recap-what-is-gardener&#34;&gt;Short Recap: What Is Gardener?&lt;/h2&gt;

&lt;p&gt;Gardener&amp;rsquo;s main principle is to leverage Kubernetes primitives for all of its
operations, commonly described as inception or kubeception. The feedback from
the community was that initially our &lt;a href=&#34;https://github.com/gardener/documentation/wiki/Architecture&#34; target=&#34;_blank&#34;&gt;architecture
diagram&lt;/a&gt; looks
&amp;ldquo;overwhelming&amp;rdquo;, but after some little digging into the material, everything we
do is the &amp;ldquo;Kubernetes way&amp;rdquo;. One can re-use all learnings with respect to APIs,
control loops, etc. &lt;br /&gt;
The essential idea is that so-called &lt;strong&gt;seed&lt;/strong&gt; clusters are used to host the
control planes of end-user clusters (botanically named &lt;strong&gt;shoots&lt;/strong&gt;). &lt;br /&gt;
Gardener provides vanilla Kubernetes clusters as a service independent of the
underlying infrastructure provider in a homogenous way, utilizing the upstream
provided &lt;code&gt;k8s.gcr.io/*&lt;/code&gt; images as open distribution. The project is built
entirely on top of Kubernetes extension concepts, and as such adds a custom API
server, a controller-manager, and a scheduler to create and manage the lifecycle
of Kubernetes clusters. It extends the Kubernetes API with custom resources,
most prominently the Gardener cluster specification (&lt;code&gt;Shoot&lt;/code&gt; resource), that can
be used to &amp;ldquo;order&amp;rdquo; a Kubernetes cluster in a declarative way (for day-1, but
also reconcile all management activities for day-2).&lt;/p&gt;

&lt;p&gt;By leveraging Kubernetes as base infrastructure, we were able to devise a
combined &lt;a href=&#34;https://github.com/gardener/hvpa-controller&#34; target=&#34;_blank&#34;&gt;Horizontal and Vertical Pod Autoscaler
(HVPA)&lt;/a&gt; that, when configured with
custom heuristics, scales all control plane components up/down or out/in
automatically. This enables a fast scale-out, even beyond the capacity of
typically some fixed number of master nodes. This architectural feature is one
of the main differences compared to many other Kubernetes cluster provisioning
tools. But in our production, Gardener does not only effectively reduce the
total costs of ownership by bin-packing control planes. It also simplifies
implementation of &amp;ldquo;day-2 operations&amp;rdquo; (like cluster updates or robustness
qualities). Again, essentially by relying on all the mature Kubernetes features
and capabilities.&lt;/p&gt;

&lt;p&gt;The newly introduced extension concepts for Gardener now enable providers to
only maintain their specific extension without the necessity to develop inside
the core source tree.&lt;/p&gt;

&lt;h2 id=&#34;extensibility&#34;&gt;Extensibility&lt;/h2&gt;

&lt;p&gt;As result of its growth over the past years, the Kubernetes code base contained
a numerous amount of provider-specific code that is now being externalized from
its core source tree. The same has happened with Project Gardener: over time,
lots of specifics for cloud providers, operating systems, network plugins, etc.
have been accumulated. Generally, this leads to a significant increase of
efforts when it comes to maintainability, testability, or to new releases. Our
community member &lt;a href=&#34;https://www.packet.com&#34; target=&#34;_blank&#34;&gt;Packet&lt;/a&gt; contributed &lt;a href=&#34;https://www.packet.com/kubernetes/&#34; target=&#34;_blank&#34;&gt;Gardener
support&lt;/a&gt; for their infrastructure in-tree,
and suffered from the mentioned downsides.&lt;/p&gt;

&lt;p&gt;Consequently, similar to how the Kubernetes community decided to move their
cloud-controller-managers out-of-tree, or volumes plugins to CSI, etc., the
Gardener community
&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md&#34; target=&#34;_blank&#34;&gt;proposed&lt;/a&gt;
and implemented likewise extension concepts. The Gardener core source-tree is
now devoid of any provider specifics, allowing vendors to solely focus on their
infrastructure specifics, and enabling core contributors becoming more agile
again.&lt;/p&gt;

&lt;p&gt;Typically, setting up a cluster requires a flow of interdependent steps,
beginning with the generation of certificates and preparation of the
infrastructure, continuing with the provisioning of the control plane and the
worker nodes, and ending with the deployment of system components. We would like
to emphasize here that all these steps are necessary (cf. &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes the Hard
Way&lt;/a&gt;) and all
Kubernetes cluster creation tools implement the same steps (automated to some
degree) in one way or another.&lt;/p&gt;

&lt;p&gt;The general idea of Gardener&amp;rsquo;s extensibility concept was to make &lt;a href=&#34;https://github.com/gardener/gardener/blob/0.31.1/pkg/controllermanager/controller/shoot/shoot_control_reconcile.go#L69-L298&#34; target=&#34;_blank&#34;&gt;this
flow&lt;/a&gt;
more generic and to carve out custom resources for each step which can serve as
ideal extension points.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/flow.png&#34;
         alt=&#34;Cluster reconciliation flow with extension points&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 1 Cluster reconciliation flow with extension points.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With Gardener&amp;rsquo;s flow framework we implicitly have a reproducible state machine
for all infrastructures and all possible states of a cluster.&lt;/p&gt;

&lt;p&gt;The Gardener extensibility approach defines custom resources that serve as ideal
extension points for the following categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DNS providers (e.g., Route53, CloudDNS, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Blob storage providers (e.g., S3, GCS, ABS,&amp;hellip;),&lt;/li&gt;
&lt;li&gt;Infrastructure providers (e.g., AWS, GCP, Azure, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Operating systems (e.g., CoreOS Container Linux, Ubuntu, FlatCar Linux, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Network plugins (e.g., Calico, Flannel, Cilium, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Non-essential extensions (e.g., Let&amp;rsquo;s Encrypt certificate service).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;extension-points&#34;&gt;Extension Points&lt;/h3&gt;

&lt;p&gt;Besides leveraging custom resource definitions, we also effectively use mutating
/ validating webhooks in the seed clusters. Extension controllers themselves run
in these clusters and react on CRDs and workload resources (like &lt;code&gt;Deployment&lt;/code&gt;,
&lt;code&gt;StatefulSet&lt;/code&gt;, etc.) they are responsible for. Similar to the &lt;a href=&#34;https://cluster-api.sigs.k8s.io&#34; target=&#34;_blank&#34;&gt;Cluster
API&lt;/a&gt;&amp;rsquo;s approach, these CRDs may also contain
provider specific information.&lt;/p&gt;

&lt;p&gt;The steps 2. - 10. [cf. Figure 1] involve infrastructure specific meta data
referring to infrastructure specific implementations, e.g. for DNS records there
might be &lt;code&gt;aws-route53&lt;/code&gt;, &lt;code&gt;google-clouddns&lt;/code&gt;, or for isolated networks even
&lt;code&gt;openstack-designate&lt;/code&gt;, and many more. We are going to  examine the steps 4 and 6
in the next paragraphs as examples for the general concepts (based on the
implementation for AWS). If you&amp;rsquo;re interested you can read up the fully
documented API contract in our &lt;a href=&#34;https://github.com/gardener/gardener/tree/master/docs/extensions&#34; target=&#34;_blank&#34;&gt;extensibility
documents&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example-infrastructure-crd&#34;&gt;Example: &lt;code&gt;Infrastructure&lt;/code&gt; CRD&lt;/h3&gt;

&lt;p&gt;Kubernetes clusters on AWS require a certain infrastructure preparation before
they can be used. This includes, for example, the creation of a VPC, subnets,
etc. The purpose of the &lt;code&gt;Infrastructure&lt;/code&gt; CRD is to trigger this preparation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;region:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west&lt;span style=&#34;color:#666&#34;&gt;-1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;secretRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloudprovider&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar—aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;sshPublicKey:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;c3NoLXJzYSBBQUFBQ...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;providerConfig:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;InfrastructureConfig&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;networks:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;vpc:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;cidr:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.0.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;zones:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1a&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;internal:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.112.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;public:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.96.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;workers:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.0.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;19&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Based on the &lt;code&gt;Shoot&lt;/code&gt; resource, Gardener creates this &lt;code&gt;Infrastructure&lt;/code&gt; resource
as part of its reconciliation flow. The AWS-specific &lt;code&gt;providerConfig&lt;/code&gt; is part of
the end-user&amp;rsquo;s configuration in the &lt;code&gt;Shoot&lt;/code&gt; resource and not evaluated by
Gardener but just passed to the extension controller in the seed cluster.&lt;/p&gt;

&lt;p&gt;In its current implementation, the AWS extension creates a new VPC and three
subnets in the &lt;code&gt;eu-west-1a&lt;/code&gt; zones. Also, it creates a NAT and an internet
gateway, elastic IPs, routing tables, security groups, IAM roles, instances
profiles, and an EC2 key pair.&lt;/p&gt;

&lt;p&gt;After it has completed its tasks it will report the status and some
provider-specific output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;status:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;lastOperation:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Reconcile&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Succeeded&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;providerStatus:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;InfrastructureStatus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;ec2:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;keyName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws-ssh-publickey&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;iam:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;instanceProfiles:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws-nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;roles:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;arn:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;arn:aws:iam::&amp;lt;accountID&amp;gt;:role/shoot...&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;vpc:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vpc&lt;span style=&#34;color:#666&#34;&gt;-0815&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;securityGroups:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;sg&lt;span style=&#34;color:#666&#34;&gt;-0246&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;subnets:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;subnet&lt;span style=&#34;color:#666&#34;&gt;-1234&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;zone:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1b&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;subnet&lt;span style=&#34;color:#666&#34;&gt;-5678&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;public&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;zone:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The information inside the &lt;code&gt;providerStatus&lt;/code&gt; can be used in subsequent steps,
e.g. to configure the cloud-controller-manager or to instrument the
machine-controller-manager.&lt;/p&gt;

&lt;h3 id=&#34;example-deployment-of-the-cluster-control-plane&#34;&gt;Example: Deployment of the Cluster Control Plane&lt;/h3&gt;

&lt;p&gt;One of the major features of Gardener is the homogeneity of the clusters it
manages across different infrastructures. Consequently, it is still in charge of
deploying the provider-independent control plane components into the seed
cluster (like etcd, kube-apiserver). The deployment of provider-specific control
plane components like cloud-controller-manager or CSI controllers is triggered
by a dedicated &lt;code&gt;ControlPlane&lt;/code&gt; CRD. In this paragraph, however, we want to focus
on the customization of the standard components.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s focus on both the kube-apiserver and the kube-controller-manager
&lt;code&gt;Deployment&lt;/code&gt;s. Our AWS extension for Gardener is not yet using CSI but relying
on the in-tree EBS volume plugin. Hence, it needs to enable the
&lt;code&gt;PersistentVolumeLabel&lt;/code&gt; admission plugin and to provide the cloud provider
config to the kube-apiserver. Similarly, the kube-controller-manager will be
instructed to use its in-tree volume plugin.&lt;/p&gt;

&lt;p&gt;The kube-apiserver &lt;code&gt;Deployment&lt;/code&gt; incorporates the &lt;code&gt;kube-apiserver&lt;/code&gt; container and
is deployed by Gardener like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/hyperkube&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiserver&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--enable-admission-plugins=Priority,...,NamespaceLifecycle&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--allow-privileged=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--anonymous-auth=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using a &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt; the AWS extension injects the mentioned
flags and modifies the spec as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/hyperkube&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiserver&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--enable-admission-plugins=Priority,...,NamespaceLifecycle,PersistentVolumeLabel&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--allow-privileged=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--anonymous-auth=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-provider=aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--endpoint-reconciler-type=none&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes/cloudprovider&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;configMap:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;defaultMode:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;420&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The kube-controller-manager &lt;code&gt;Deployment&lt;/code&gt; is handled in a similar way.&lt;/p&gt;

&lt;p&gt;Webhooks in the seed cluster can be used to mutate anything related to the shoot
cluster control plane deployed by Gardener or any other extension. There is a
similar webhook concept for resources in shoot clusters in case extension
controllers need to customize system components deployed by Gardener.&lt;/p&gt;

&lt;h3 id=&#34;registration-of-extension-controllers&#34;&gt;Registration of Extension Controllers&lt;/h3&gt;

&lt;p&gt;The Gardener API uses two special resources to register and install extensions.
The registration itself is declared via the &lt;code&gt;ControllerRegistration&lt;/code&gt; resource.
The easiest option is to define the Helm chart as well as some values to render
the chart, however, any other deployment mechanism is supported via custom code
as well.&lt;/p&gt;

&lt;p&gt;Gardener determines whether an extension controller is required in a specific
seed cluster, and creates a &lt;code&gt;ControllerInstallation&lt;/code&gt; that is used to trigger the
deployment.&lt;/p&gt;

&lt;p&gt;To date, every registered extension controller is deployed to every seed cluster
which is not necessary in general. In the future, Gardener will become more
selective to only deploy those extensions required on the specific seed
clusters.&lt;/p&gt;

&lt;p&gt;Our dynamic registration approach allows to add or remove extensions in the
running system - without the necessity to rebuild or restart any component.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/architecture.png&#34;
         alt=&#34;Gardener architecture with extension controllers&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 2 Gardener architecture with extension controllers.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;status-quo&#34;&gt;Status Quo&lt;/h3&gt;

&lt;p&gt;We have recently introduced the new &lt;code&gt;core.gardener.cloud&lt;/code&gt; API group that
incorporates fully forwards and backwards compatible &lt;code&gt;Shoot&lt;/code&gt; resources, and that
allows providers to use Gardener without modifying anything in its core source
tree.&lt;/p&gt;

&lt;p&gt;We have already adapted all controllers to use this new API group and have
deprecated the old API. Eventually, after a few months we will remove it, so
end-users are advised to start migrating to the new API soon.&lt;/p&gt;

&lt;p&gt;Apart from that, we have enabled all relevant extensions to contribute to the
shoot health status and implemented the respective contract. The basic idea is
that the CRDs may have &lt;code&gt;.status.conditions&lt;/code&gt; that are picked up by Gardener and
merged with its standard health checks into the &lt;code&gt;Shoot&lt;/code&gt; status field.&lt;/p&gt;

&lt;p&gt;Also, we want to implement some easy-to-use library functions facilitating
defaulting and validation webhooks for the CRDs in order to validate the
&lt;code&gt;providerConfig&lt;/code&gt; field controlled by end-users.&lt;/p&gt;

&lt;p&gt;Finally, we will split the
&lt;a href=&#34;https://github.com/gardener/gardener-extensions&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;gardener/gardener-extensions&lt;/code&gt;&lt;/a&gt;
repository into separate repositories and keep it only for the generic library
functions that can be used to write extension controllers.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Kubernetes has externalized many of the infrastructural management challenges.
The inception design solves most of them by delegating lifecycle operations to a
separate management plane (seed clusters). But what if the garden cluster or a
seed cluster goes down? How do we scale beyond tens of thousands of managed
clusters that need to be reconciled in parallel? We are further investing into
hardening the Gardener scalability and disaster recovery features. Let&amp;rsquo;s briefly
highlight three of the features in more detail:&lt;/p&gt;

&lt;h3 id=&#34;gardenlet&#34;&gt;Gardenlet&lt;/h3&gt;

&lt;p&gt;Right from the beginning of the Gardener Project we started implementing the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34; target=&#34;_blank&#34;&gt;operator
pattern&lt;/a&gt;: We
have a custom controller-manager that acts on our own custom resources. Now,
when you start thinking about the &lt;a href=&#34;https://github.com/gardener/documentation/wiki/Architecture&#34; target=&#34;_blank&#34;&gt;Gardener
architecture&lt;/a&gt;, you
will recognize some interesting similarity with respect to the Kubernetes
architecture: Shoot clusters can be compared with pods, and seed clusters can be
seen as worker nodes. Guided by this observation we introduced the
&lt;strong&gt;gardener-scheduler&lt;/strong&gt;. Its main task is to find an appropriate seed cluster to
host the control-plane for newly ordered clusters, similar to how the
kube-scheduler finds an appropriate node for newly created pods. By providing
multiple seed clusters for a region (or provider) and distributing the workload,
we reduce the blast-radius of potential hick-ups as well.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet.png&#34;
         alt=&#34;Similarities between Kubernetes and Gardener architecture&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 3 Similarities between Kubernetes and Gardener architecture.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yet, there is still a significant difference between the Kubernetes and the
Gardener architectures: Kubernetes runs a primary &amp;ldquo;agent&amp;rdquo; on every node, the
kubelet, which is mainly responsible for managing pods and containers on its
particular node. Gardener uses its controller-manager which is responsible for
all shoot clusters on all seed clusters, and it is performing its reconciliation
loops centrally from the garden cluster.&lt;/p&gt;

&lt;p&gt;While this works well at scale for thousands of clusters today, our goal is to
enable true scalability following the Kubernetes principles (beyond the capacity
of a single controller-manager): We are now working on distributing the logic
(or the Gardener operator) into the seed cluster and will introduce a
corresponding component, adequately named the &lt;strong&gt;gardenlet&lt;/strong&gt;. It will be
Gardener&amp;rsquo;s primary &amp;ldquo;agent&amp;rdquo; on every seed cluster and will be only responsible
for shoot clusters located in its particular seed cluster.&lt;/p&gt;

&lt;p&gt;The gardener-controller-manager will still keep its control loops for other
resources of the Gardener API, however, it will no longer talk to seed/shoot
clusters.&lt;/p&gt;

&lt;p&gt;Reversing the control flow will even allow placing seed/shoot clusters behind
firewalls without the necessity of direct accessibility (via VPN tunnels)
anymore.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet-detailed.png&#34;
         alt=&#34;Detailed architecture with Gardenlet&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 4 Detailed architecture with Gardenlet.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;control-plane-migration-between-seed-clusters&#34;&gt;Control Plane Migration between Seed Clusters&lt;/h3&gt;

&lt;p&gt;When a seed cluster fails, the user&amp;rsquo;s static workload will continue to operate.
However, administrating the cluster won&amp;rsquo;t be possible anymore because the shoot
cluster&amp;rsquo;s API server running in the failed seed is no longer reachable.&lt;/p&gt;

&lt;p&gt;We have implemented the relocation of failed control planes hit by some seed
disaster to another seed and are now working on fully automating this unique
capability. In fact, this approach is not only feasible, we have performed the
fail-over procedure multiple times in our production.&lt;/p&gt;

&lt;p&gt;The automated failover capability will enable us to implement even more
comprehensive disaster recovery and scalability qualities, e.g., the automated
provisioning and re-balancing of seed clusters or automated migrations for all
non-foreseeable cases. Again, think about the similarities with Kubernetes with
respect to pod eviction and node drains.&lt;/p&gt;

&lt;h3 id=&#34;gardener-ring&#34;&gt;Gardener Ring&lt;/h3&gt;

&lt;p&gt;The Gardener Ring is our novel approach for provisioning and managing Kubernetes
clusters without relying on an external provision tool for the initial cluster.
By using Kubernetes in a recursive manner, we can drastically reduce the
management complexity by avoiding imperative tool sets, while creating new
qualities with a self-stabilizing circular system.&lt;/p&gt;

&lt;p&gt;The Ring approach is conceptually different from self-hosting and static pod
based deployments. The idea is to create a ring of three (or more) shoot
clusters that each host the control plane of its successor.&lt;/p&gt;

&lt;p&gt;An outage of one cluster will not affect the stability and availability of the
Ring, and as the control plane is externalized the failed cluster can be
automatically recovered by Gardener&amp;rsquo;s self-healing capabilities. As long as
there is a quorum of at least &lt;code&gt;n/2+1&lt;/code&gt; available clusters the Ring will always
stabilize itself. Running these clusters on different cloud providers (or at
least in different regions / data centers) reduces the potential for quorum
losses.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/ring.png&#34;
         alt=&#34;Self-stabilizing ring of Kubernetes clusters&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 5 Self-stabilizing ring of Kubernetes clusters.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The way how the distributed instances of Gardener can share the same data is by
deploying separate kube-apiserver instances talking to the same etcd cluster.
These kube-apiservers are forming a node-less Kubernetes cluster that can be
used as &amp;ldquo;data container&amp;rdquo; for Gardener and its associated applications.&lt;/p&gt;

&lt;p&gt;We are running test landscapes internally protected by the ring and it has saved
us from manual interventions. With the automated control plane migration in
place we can easily bootstrap the Ring and will solve the &amp;ldquo;initial cluster
problem&amp;rdquo; as well as improve the overall robustness.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started!&lt;/h2&gt;

&lt;p&gt;If you are interested in writing an extension, you might want to check out the
following resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md&#34; target=&#34;_blank&#34;&gt;GEP-1: Extensibility proposal
document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md&#34; target=&#34;_blank&#34;&gt;GEP-4: New &lt;code&gt;core.gardener.cloud/v1alpha1&lt;/code&gt;
API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener-extensions/tree/master/controllers/provider-aws&#34; target=&#34;_blank&#34;&gt;Example extension controller implementation for
AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://godoc.org/github.com/gardener/gardener-extensions/pkg&#34; target=&#34;_blank&#34;&gt;Gardener Extensions Golang
library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/tree/master/docs/extensions&#34; target=&#34;_blank&#34;&gt;Extension contract
documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gardener.cloud/api-reference/&#34; target=&#34;_blank&#34;&gt;Gardener API Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, any other contribution to our project is very welcome as well! We are
always looking for new community members.&lt;/p&gt;

&lt;p&gt;If you want to try out Gardener, please check out our &lt;a href=&#34;https://gardener.cloud/installer/&#34; target=&#34;_blank&#34;&gt;quick installation
guide&lt;/a&gt;. This installer will setup a complete
Gardener environment ready to be used for testing and evaluation within just a
few minutes.&lt;/p&gt;

&lt;h2 id=&#34;contributions-welcome&#34;&gt;Contributions Welcome!&lt;/h2&gt;

&lt;p&gt;The Gardener project is developed as Open Source and hosted on GitHub:
&lt;a href=&#34;https://github.com/gardener&#34; target=&#34;_blank&#34;&gt;https://github.com/gardener&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you see the potential of the Gardener project, please join us via GitHub.&lt;/p&gt;

&lt;p&gt;We are having a weekly &lt;a href=&#34;https://docs.google.com/document/d/1314v8ziVNQPjdBrWp-Y4BYrTDlv7dq2cWDFIa9SMaP4&#34; target=&#34;_blank&#34;&gt;public community
meeting&lt;/a&gt;
scheduled every Friday 10-11 a.m. CET, and a public &lt;a href=&#34;https://kubernetes.slack.com/messages/gardener&#34; target=&#34;_blank&#34;&gt;#gardener
Slack&lt;/a&gt; channel in the Kubernetes
workspace. Also, we are planning a &lt;a href=&#34;https://docs.google.com/document/d/1EQ_kt70gwybiL7FY8F7Dx--GtiNwdv0oRDwqQqAIYMk/edit#heading=h.a43vkkp847f1&#34; target=&#34;_blank&#34;&gt;Gardener Hackathon in Q1
2020&lt;/a&gt;
and are looking forward meeting you there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Develop a Kubernetes controller in Java</title>
      <link>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Min Kim (Ant Financial), Tony Ado (Ant Financial)&lt;/p&gt;

&lt;p&gt;The official &lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;Kubernetes Java SDK&lt;/a&gt; project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.&lt;/p&gt;

&lt;h2 id=&#34;overall&#34;&gt;Overall&lt;/h2&gt;

&lt;p&gt;Java is no doubt one of the most popular programming languages in the world but
it&amp;rsquo;s been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there&amp;rsquo;re already some excellent controller
frameworks, for example, &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller runtime&lt;/a&gt;,
&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34; target=&#34;_blank&#34;&gt;operator SDK&lt;/a&gt;. These
existing Golang frameworks are relying on the various utilities from the
&lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;Kubernetes Golang SDK&lt;/a&gt; proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.&lt;/p&gt;

&lt;h2 id=&#34;backgrounds&#34;&gt;Backgrounds&lt;/h2&gt;

&lt;p&gt;Why use Java to implement Kubernetes tooling? You might pick Java for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Integrating legacy enterprise Java systems&lt;/strong&gt;: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;More open-source community resources&lt;/strong&gt;: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-use&#34;&gt;How to use?&lt;/h2&gt;

&lt;p&gt;Take maven project as example, adding the following dependencies into your dependencies:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.kubernetes&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;client-java-extended&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;6.0.1&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example &lt;a href=&#34;https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
    &lt;span style=&#34;&#34;&gt;Reconciler reconciler &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Reconciler() {
      &lt;span style=&#34;color:#a2f&#34;&gt;@Override&lt;/span&gt;
      &lt;span style=&#34;&#34;&gt;public Result &lt;/span&gt;reconcile(&lt;span style=&#34;&#34;&gt;Request request&lt;/span&gt;) {
        &lt;span style=&#34;&#34;&gt;V1Node node &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nodeLister.&lt;span style=&#34;color:#b44&#34;&gt;get&lt;/span&gt;(request.&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        System.&lt;span style=&#34;color:#b44&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#b44&#34;&gt;println&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;triggered reconciling &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; node.&lt;span style=&#34;color:#b44&#34;&gt;getMetadata&lt;/span&gt;().&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Result(&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;);
      }
    };
    &lt;span style=&#34;&#34;&gt;Controller controller &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
        ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;defaultBuilder&lt;/span&gt;(informerFactory)
            .&lt;span style=&#34;color:#b44&#34;&gt;watch&lt;/span&gt;(
                (workQueue) &lt;span style=&#34;color:#666&#34;&gt;-&amp;gt;&lt;/span&gt; ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;controllerWatchBuilder&lt;/span&gt;(V1Node.&lt;span style=&#34;color:#b44&#34;&gt;class&lt;/span&gt;, workQueue).&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;())
            .&lt;span style=&#34;color:#b44&#34;&gt;withReconciler&lt;/span&gt;(nodeReconciler) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// required, set the actual reconciler
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withName&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;node-printing-controller&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set name for controller for logging, thread-tracing
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withWorkerCount&lt;/span&gt;(4) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set worker thread count
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withReadyFunc&lt;/span&gt;( nodeInformer&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;hasSynced) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, only starts controller when the cache has synced up
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you notice, the new Java controller framework learnt a lot from the design of
&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller-runtime&lt;/a&gt; which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.&lt;/p&gt;

&lt;p&gt;As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.&lt;/p&gt;

&lt;h2 id=&#34;future-steps&#34;&gt;Future steps&lt;/h2&gt;

&lt;p&gt;The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo &lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;kubernetes-client/java&lt;/a&gt;.
Feel free to share also your feedback with us, through Issues or &lt;a href=&#34;http://kubernetes.slack.com/messages/kubernetes-client/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Running Kubernetes locally on Linux with Microk8s</title>
      <link>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt;, Developer Advocate, Cloud Native Computing Foundation; &lt;a href=&#34;https://twitter.com/carminerimi&#34; target=&#34;_blank&#34;&gt;Carmine Rimi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article, the second in a &lt;a href=&#34;https://kubernetes.io/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/&#34;&gt;series&lt;/a&gt; about local deployment options on Linux, and covers &lt;a href=&#34;https://microk8s.io/&#34; target=&#34;_blank&#34;&gt;MicroK8s&lt;/a&gt;. Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.&lt;/p&gt;

&lt;p&gt;While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses &lt;a href=&#34;https://snapcraft.io/&#34; target=&#34;_blank&#34;&gt;snap&lt;/a&gt; packages, an application packaging and isolation technology.&lt;/p&gt;

&lt;p&gt;This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions &lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;that support snaps&lt;/a&gt;. Most popular Linux distributions are supported.&lt;/p&gt;

&lt;p&gt;Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!&lt;/p&gt;

&lt;p&gt;Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide &lt;a href=&#34;https://snapcraft.io/docs/channels&#34; target=&#34;_blank&#34;&gt;channels&lt;/a&gt; that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.&lt;/p&gt;

&lt;p&gt;But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.&lt;/p&gt;

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it&amp;rsquo;s official &lt;a href=&#34;https://microk8s.io/docs/&#34; target=&#34;_blank&#34;&gt;webpage&lt;/a&gt;, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;A Linux distribution that &lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;supports snaps&lt;/a&gt;, is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out &lt;a href=&#34;https://multipass.run&#34; target=&#34;_blank&#34;&gt;Multipass&lt;/a&gt; to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.&lt;/p&gt;

&lt;h2 id=&#34;microk8s-installation&#34;&gt;MicroK8s installation&lt;/h2&gt;

&lt;p&gt;MicroK8s installation is straightforward:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap install microk8s --classic&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.&lt;/p&gt;

&lt;p&gt;You may verify the MicroK8s status with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo microk8s.status&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;using-microk8s&#34;&gt;Using microk8s&lt;/h2&gt;

&lt;p&gt;Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a &lt;code&gt;kubectl&lt;/code&gt; binary, which can be accessed by running the &lt;code&gt;microk8s.kubectl&lt;/code&gt; command. As an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;microk8s.kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;While using the prefix &lt;code&gt;microk8s.kubectl&lt;/code&gt; allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the &lt;code&gt;snap alias&lt;/code&gt; command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap &lt;span style=&#34;color:#a2f&#34;&gt;alias&lt;/span&gt; microk8s.kubectl kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will allow you to simply use &lt;code&gt;kubectl&lt;/code&gt; after. You can revert this change using the &lt;code&gt;snap unalias&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;microk8s-addons&#34;&gt;MicroK8s addons&lt;/h2&gt;

&lt;p&gt;One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.&lt;/p&gt;

&lt;p&gt;The full list of extensions can be checked by running the &lt;code&gt;microk8s.status&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo microk8s.status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As of the time of writing this article, the following add-ons are supported:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;More add-ons are being created and contributed by the community all the time, it definitely helps to check often!&lt;/p&gt;

&lt;h2 id=&#34;release-channels&#34;&gt;Release channels&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap info microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;installing-the-sample-application&#34;&gt;Installing the sample application&lt;/h2&gt;

&lt;p&gt;In this tutorial we’ll use NGINX as a sample application (&lt;a href=&#34;https://hub.docker.com/_/nginx&#34; target=&#34;_blank&#34;&gt;the official Docker Hub image&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It will be installed as a Kubernetes deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create deployment nginx --image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;nginx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To verify the installation, let’s run the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deployments&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Also, we can retrieve the full output of all available objects within our Kubernetes cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get all --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;uninstalling-microk8s&#34;&gt;Uninstalling MicroK8s&lt;/h2&gt;

&lt;p&gt;Uninstalling your microk8s cluster is so easy as uninstalling the snap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap remove microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;screencast&#34;&gt;Screencast&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/263394&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/263394.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Grokkin&#39; the Docs</title>
      <link>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/aimee-ukasick/&#34; target=&#34;_blank&#34;&gt;Aimee Ukasick&lt;/a&gt;, Independent Contributor&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/grokkin-the-docs/grok-definition.png&#34;
         alt=&#34;grok: to understand profoundly and intuitively&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Definition courtesy of Merriam Webster online dictionary&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;intro-observations-of-a-new-sig-docs-contributor&#34;&gt;Intro - Observations of a new SIG Docs contributor&lt;/h2&gt;

&lt;p&gt;I began contributing to the SIG Docs community in August 2019. Sometimes I feel
like I am a stranger in a strange land adapting to a new community:
investigating community organization, understanding contributor society,
learning new lessons, and incorporating new jargon. I&amp;rsquo;m an observer as well as a
contributor.&lt;/p&gt;

&lt;h2 id=&#34;observation-1&#34;&gt;Observation 01: Read the &lt;em&gt;Contribute&lt;/em&gt; pages!&lt;/h2&gt;

&lt;p&gt;I contributed code and documentation to OpenStack, OPNFV, and Acumos, so I
thought contributing to the Kubernetes documentation would be the same. I was
wrong. I should have thoroughly &lt;strong&gt;read&lt;/strong&gt; the &lt;a href=&#34;https://kubernetes.io/docs/contribute/&#34; target=&#34;_blank&#34;&gt;Contribute to Kubernetes
docs&lt;/a&gt; pages instead of skimming them.&lt;/p&gt;

&lt;p&gt;I am very familiar with the git/gerrit workflow. With those tools, a contributor clones
the &lt;code&gt;master&lt;/code&gt; repo and then creates a local branch. Kubernetes uses a different
approach, called &lt;em&gt;Fork and Pull&lt;/em&gt;. Each contributor &lt;code&gt;forks&lt;/code&gt; the master repo, and
then the contributor pushes work to their fork before creating a pull request. I
created a simple pull request (PR), following the instructions in the &lt;strong&gt;Start
contributing&lt;/strong&gt; page&amp;rsquo;s &lt;a href=&#34;https://kubernetes.io/docs/contribute/start/#submit-a-pull-request&#34; target=&#34;_blank&#34;&gt;Submit a pull
request&lt;/a&gt;
section. This section describes how to make a documentation change using the
GitHub UI. I learned that this method is fine for a change that requires a
single commit to fix. However, this method becomes complicated when you have to
make additional updates to your PR.  GitHub creates a new commit for each change
made using the GitHub UI. The Kubernetes GitHub org requires squashing commits.
The &lt;strong&gt;Start contributing&lt;/strong&gt; page didn&amp;rsquo;t mention squashing commits, so I looked at
the GitHub and git documentation. I could not squash my commits using the GitHub
UI. I had to &lt;code&gt;git fetch&lt;/code&gt; and &lt;code&gt;git checkout&lt;/code&gt; my pull request locally, squash the
commits using the command line, and then push my changes. If the &lt;strong&gt;Start
contributing&lt;/strong&gt; had mentioned squashing commits, I would have worked from a local
clone instead of using the GitHub UI.&lt;/p&gt;

&lt;h2 id=&#34;observation-2&#34;&gt;Observation 02: Reach out and ping someone&lt;/h2&gt;

&lt;p&gt;While working on my first PRs, I had questions about working from a local clone
and about keeping my fork updated from &lt;code&gt;upstream master&lt;/code&gt;. I turned to searching
the internet instead of asking on the &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes Slack&lt;/a&gt;
#sig-docs channel. I used the wrong process to update my fork, so I had to &lt;code&gt;git
rebase&lt;/code&gt; my PRs, which did not go well at all. As a result, I closed those PRs
and submitted new ones.  When I asked for help on the #sig-docs channel,
contributors posted useful links, what my local git config file should look
like, and the exact set of git commands to run. The process used by contributors
was different than the one defined in the &lt;strong&gt;Intermediate contributing&lt;/strong&gt; page.
I would have saved myself so much time if I had asked what GitHub workflow to
use. The more community knowledge that is documented, the easier it is for new
contributors to be productive quickly.&lt;/p&gt;

&lt;h2 id=&#34;observation-3&#34;&gt;Observation 03: Don&amp;rsquo;t let conflicting information ruin your day&lt;/h2&gt;

&lt;p&gt;The Kubernetes community has a contributor guide for
&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/contributors/guide&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt;
and another one for &lt;a href=&#34;https://kubernetes.io/docs/contribute/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;. The
guides contain conflicting information on the same topic. For example, the SIG
Docs GitHub process recommends creating a local branch based on
&lt;code&gt;upstream/master&lt;/code&gt;.  The &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Contributor
Guide&lt;/a&gt;
advocates updating your fork from upstream and then creating a local branch
based on your fork. Which process should a new contributor follow? Are the two
processes interchangeable? The best place to ask questions about conflicting
information is the #sig-docs or #sig-contribex channels. I asked for
clarification about the GitHub workflows in the #sig-contribex channel.
@cblecker provided an extremely detailed response, which I used to update the
&lt;strong&gt;Intermediate contributing&lt;/strong&gt; page.&lt;/p&gt;

&lt;h2 id=&#34;observation-4&#34;&gt;Observation 04: Information may be scattered&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s common for large open source projects to have information scattered around
various repos or duplicated between repos. Sometimes groups work in silos, and
information is not shared. Other times, a person leaves to work on a
different project without passing on specialized knowledge.
Documentation gaps exist and may never be rectified because of higher priority
items. So new contributors may have difficulty finding basic information, such
as meeting details.&lt;/p&gt;

&lt;p&gt;Attending SIG Docs meetings is a great way to become involved. However, people
have had a hard time locating the meeting URL. Most new contributors ask in the
#sig-docs channel, but I decided to locate the meeting information in the docs.
This required several clicks over multiple pages. How many new contributors miss
meetings because they can&amp;rsquo;t locate the meeting details?&lt;/p&gt;

&lt;h2 id=&#34;observation-5&#34;&gt;Observation 05: Patience is a virtue&lt;/h2&gt;

&lt;p&gt;A contributor may wait days for feedback on a larger PR. The process from
submission to final approval may take weeks instead of days. There are two
reasons for this: 1) most reviewers work part-time on SIG Docs; and 2) reviewers
want to provide meaningful reviews. &amp;ldquo;Drive-by reviewing&amp;rdquo; doesn&amp;rsquo;t happen in SIG
Docs! Reviewers check for the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do the commit message and PR description adequately describe the change?&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does the PR follow the guidelines in the style and content guides?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall, is the grammar and punctuation correct?&lt;/li&gt;
&lt;li&gt;Is the content clear, concise, and appropriate for non-native speakers?&lt;/li&gt;
&lt;li&gt;Does the content stylistically fit in with the rest of the documentation?&lt;/li&gt;
&lt;li&gt;Does the flow of the content make sense?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Can anything be changed to make the content better, such as using a Hugo shortcode?&lt;/li&gt;
&lt;li&gt;Does the content render correctly?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Is the content technically correct?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sometimes the review process made me feel defensive, annoyed, and frustrated. I&amp;rsquo;m
sure other contributors have felt the same way. Contributors need to be patient!
Writing excellent documentation is an iterative process. Reviewers scrutinize
PRs because they want to maintain a high level of quality in the documentation,
not because they want to annoy contributors!&lt;/p&gt;

&lt;h2 id=&#34;observation-6&#34;&gt;Observation 06: Make every word count&lt;/h2&gt;

&lt;p&gt;Non-native English speakers read and contribute to the Kubernetes documentation.
When you are writing content, use simple, direct language in clear, concise
sentences.  Every sentence you write may be translated into another language, so
remove words that don&amp;rsquo;t add substance. I admit that implementing these
guidelines is challenging at times.&lt;/p&gt;

&lt;p&gt;Issues and pull requests aren&amp;rsquo;t translated into other languages. However, you
should still follow the aforementioned guidelines when you write the description
for an issue or pull request. You should add details and background
information to an issue so the person doing triage doesn&amp;rsquo;t have to apply the
&lt;code&gt;triage/needs-information&lt;/code&gt; label. Likewise, when you create a pull request, you
should add enough information about the content change that reviewers don&amp;rsquo;t have
to figure out the reason for the pull request. Providing details in clear,
concise language speeds up the process.&lt;/p&gt;

&lt;h2 id=&#34;observation-7&#34;&gt;Observation 07: Triaging issues is more difficult than it should be&lt;/h2&gt;

&lt;p&gt;In SIG Docs, triaging issues requires the ability to distinguish between
support, bug, and feature requests not only for the documentation but also for
Kubernetes code projects. How to route, label, and prioritize issues has become
easier week by week. I&amp;rsquo;m still not 100% clear on which SIG and/or project is
responsible for which parts of the documentation. The SIGs and Working Groups
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt; helps,
but it is not enough. At a page level in the documentation, it&amp;rsquo;s not
always obvious which SIG or project has domain expertise. The page&amp;rsquo;s front
matter sometimes list reviewers but never lists a SIG or project. Each page should
indicate who is responsible for content, so that SIG Docs triagers know where to
route issues.&lt;/p&gt;

&lt;h2 id=&#34;observation-8&#34;&gt;Observation 08: SIG Docs is understaffed&lt;/h2&gt;

&lt;p&gt;Documentation is the number one driver of software adoption&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Many contributors devote a small amount of time to SIG Docs but only a handful
are trained technical writers. Few companies have hired tech writers to work on
Kubernetes docs at least half-time. That&amp;rsquo;s very disheartening for online
documentation that has had over 53 million unique page views from readers in 229
countries year to date in 2019.&lt;/p&gt;

&lt;p&gt;SIG Docs faces challenges due to lack of technical writers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maintaining a high quality in the Kubernetes documentation&lt;/strong&gt;:
There are over 750 pages of documentation. That&amp;rsquo;s &lt;em&gt;750 pages&lt;/em&gt; to check for
stale content on a regular basis. This involves more than running a link
checker against the &lt;code&gt;kubernetes/website&lt;/code&gt; repo. This involves people having a
technical understanding of Kubernetes, knowing which code release changes
impact documentation, and knowing where content is located in the
documentation so that &lt;em&gt;all&lt;/em&gt; impacted pages and example code files are updated
in a timely fashion. Other SIGs help with this, but based on the number of
issues created by readers, enough people aren&amp;rsquo;t working on keeping the content
fresh.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducing the time to review and merge a PR&lt;/strong&gt;:
The larger the size of the PR, the longer it takes to get the &lt;code&gt;lgtm&lt;/code&gt; label
and eventual approval. My &lt;code&gt;size/M&lt;/code&gt; and larger PRs took from five to thirty
days to approve. Sometimes I politely poked reviewers to review again after
I had pushed updates. Other times I asked on the #sig-docs channel for &lt;em&gt;any
approver&lt;/em&gt; to take a look and approve. People are busy. People go on
vacation. People also move on to new roles that don&amp;rsquo;t involve SIG Docs and
forget to remove themselves from the reviewer and approver assignment file.
A large part of the time-to-merge problem is not having enough reviewers and
approvers. The other part is the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/community-membership.md#reviewer&#34; target=&#34;_blank&#34;&gt;high
barrier&lt;/a&gt;
to becoming a reviewer or approver, much higher than what I&amp;rsquo;ve seen on other
open source projects. Experienced open source tech writers who want to
contribute to SIG Docs aren&amp;rsquo;t fast-tracked into approver and reviewer roles.
On one hand, that high barrier ensures that those roles are filled by folks
with a minimum level of Kubernetes documentation knowledge; on the other
hand, it might deter experienced tech writers from contributing at all, or
from a company allocating a tech writer to SIG Docs. Maybe SIG Docs should
consider deviating from the Kubernetes community requirements by lowering
the barrier to becoming a reviewer or approver, on a case-by-case basis, of
course.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ensuring consistent naming across all pages&lt;/strong&gt;:
Terms should be identical to what is used in the &lt;strong&gt;Standardized Glossary&lt;/strong&gt;. Being consistent reduces confusion.
Tracking down and fixing these occurrences is time-consuming but worthwhile for readers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Working with the Steering Committee to create project documentation guidelines&lt;/strong&gt;:
The &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Repository Guidelines&lt;/a&gt; don&amp;rsquo;t mention documentation at all. Between a
project&amp;rsquo;s GitHub docs and the Kubernetes docs, some projects have almost
duplicate content, whereas others have conflicting content. Create clear
guidelines so projects know to put roadmaps, milestones, and comprehensive
feature details in the &lt;code&gt;kubernetes/&amp;lt;project&amp;gt;&lt;/code&gt; repo and to put installation,
configuration, usage details, and tutorials in the Kubernetes docs.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removing duplicate content&lt;/strong&gt;:
Kubernetes users install Docker, so a good example of duplicate content is
Docker installation instructions. Rather than repeat what&amp;rsquo;s in the Docker
docs, state which version of Docker works with which version of Kubernetes
and link to the Docker docs for installation. Then detail any
Kubernetes-specific configuration. That idea is the same for the container
runtimes that Kubernetes supports.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removing third-party vendor content&lt;/strong&gt;:
This is tightly coupled to removing duplicate content. Some third-party
content consists of lists or tables detailing external products. Other
third-party content is found in the &lt;strong&gt;Tasks&lt;/strong&gt; and &lt;strong&gt;Tutorials&lt;/strong&gt; sections.
SIG Docs should not be responsible for verifying that third-party products
work with the latest version of Kubernetes. Nor should SIG Docs be
responsible for maintaining lists of training courses or cloud providers.
Additionally, the Kubernetes documentation isn&amp;rsquo;t the place to pitch vendor
products. If SIG Docs is forced to reverse its policy on not allowing
third-party content, there could be a tidal wave of
vendor-or-commercially-oriented pull requests. Maintaining that content
places an undue burden on SIG Docs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indicating which version of Kubernetes works with each task and tutorial&lt;/strong&gt;:
This means reviewing each task and tutorial for every release. Readers
assume if a task or tutorial is in the latest version of the docs, it works
with the latest version of Kubernetes.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Addressing issues&lt;/strong&gt;:
There are 470 open issues in the &lt;code&gt;kubernetes/website&lt;/code&gt; repo. It&amp;rsquo;s hard to keep up with all the issues that are created. We encourage
those creating simpler issues to submit PRs: some do; most do not.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creating more detailed content&lt;/strong&gt;:
Readers
&lt;a href=&#34;https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey&#34; target=&#34;_blank&#34;&gt;indicated&lt;/a&gt;
they would like to see more detailed content across all sections of the
documentation, including tutorials.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes has seen unparalleled growth since its first release in 2015. Like
any fast-growing project, it has growing pains. Providing consistently
high-quality documentation is one of those pains, and one incredibly important
to an open source project. SIG Docs needs a larger core team of tech writers who
are allocated at least 50%. SIG Docs can then better achieve goals, move forward
with new content, update existing content, and address open issues in a timely fashion.&lt;/p&gt;

&lt;h2 id=&#34;observation-9&#34;&gt;Observation 09: Contributing to technical documentation projects requires, on average, more skills than developing software&lt;/h2&gt;

&lt;p&gt;When I said that to my former colleagues, the response was a healthy dose of
skepticism and lots of laughter. It seems that many developers, as well as
managers, don&amp;rsquo;t fully know what tech writers contributing to open source
projects actually do. Having done both development and technical writing for the
better part of 22 years, I&amp;rsquo;ve noticed that tech writers are valued far less than
software developers of comparative standing.&lt;/p&gt;

&lt;p&gt;SIG Docs core team members do far more than write content based on requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We use some of the same processes and tools as developers, such as the
terminal, git workflow, GitHub, and IDEs like Atom, Golang, and Visual Studio Code; we
also use documentation-specific plugins and tools.&lt;/li&gt;
&lt;li&gt;We possess a good eye for detail as well as design and organization: the big picture &lt;em&gt;and&lt;/em&gt; the little picture.&lt;/li&gt;
&lt;li&gt;We provide documentation which has a logical flow; it is not merely content on a page
but the way pages fit into sections and sections fit into the overall structure.&lt;/li&gt;
&lt;li&gt;We write content that is comprehensive and uses language that readers not fluent in English can understand.&lt;/li&gt;
&lt;li&gt;We have a firm grasp of English composition using various markup languages.&lt;/li&gt;
&lt;li&gt;We are technical, sometimes to the level of a Kubernetes admin.&lt;/li&gt;
&lt;li&gt;We read, understand, and occasionally write code.&lt;/li&gt;
&lt;li&gt;We are project managers, able to plan new work as well as assign issues to releases.&lt;/li&gt;
&lt;li&gt;We are educators and diplomats with every review we do and with every comment we leave on an issue.&lt;/li&gt;
&lt;li&gt;We use site analytics to plan work based on which pages readers access most often as well as which pages readers say are unhelpful.&lt;/li&gt;
&lt;li&gt;We are surveyors, soliciting feedback from the community on a regular basis.&lt;/li&gt;
&lt;li&gt;We analyze the documentation as a whole, deciding what content should stay and
what content should be removed based on available resources and reader needs.&lt;/li&gt;
&lt;li&gt;We have a working knowledge of Hugo and other frameworks used for
online documentation; we know how to create, use, and debug Hugo shortcodes that
enable content to be more robust than pure Markdown.&lt;/li&gt;
&lt;li&gt;We troubleshoot performance issues not only with Hugo but with Netlify.&lt;/li&gt;
&lt;li&gt;We grapple with the complex problem of API documentation.&lt;/li&gt;
&lt;li&gt;We are dedicated to providing the highest quality documentation that we can.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have any doubts about the complexity of the Kubernetes documentation
project, watch presentations given by SIG Docs Chair Zach Corleissen:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.fosdem.org/2019/schedule/event/multikuber/&#34; target=&#34;_blank&#34;&gt;Multilingual Kubernetes&lt;/a&gt; - the kubernetes.io stack, how we got there, and what it took to get there&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/GXkpHAruNV8&#34; target=&#34;_blank&#34;&gt;Found in Translation: Lessons from a Year of Open Source Localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, &lt;a href=&#34;https://youtu.be/JvRd7MmAxPw&#34; target=&#34;_blank&#34;&gt;Docs as Code: The Missing Manual&lt;/a&gt;
(Jennifer Rondeau, Margaret Eker; 2016) is an excellent presentation on the
complexity of documentation projects in general.&lt;/p&gt;

&lt;p&gt;The Write the Docs &lt;a href=&#34;http://www.writethedocs.org/&#34; target=&#34;_blank&#34;&gt;website&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/channel/UCr019846MitZUEhc6apDdcQ&#34; target=&#34;_blank&#34;&gt;YouTube
channel&lt;/a&gt; are
fantastic places to delve into the good, the bad, and the ugly of technical writing.&lt;/p&gt;

&lt;p&gt;Think what an open source project would be without talented, dedicated tech writers!&lt;/p&gt;

&lt;h2 id=&#34;observation-10&#34;&gt;Observation 10: Community is everything&lt;/h2&gt;

&lt;p&gt;The SIG Docs community, and the larger Kubernetes community, is dedicated,
intelligent, friendly, talented, fun, helpful, and a whole bunch of other
positive adjectives! People welcomed me with open arms, and not only because SIG
Docs needs more technical writers. I have never felt that my ideas and contributions were
dismissed because I was the newbie. Humility and respect go a long way.
Community members have a wealth of knowledge to share. Attend meetings, ask
questions, propose improvements, thank people, and contribute in
every way that you can!&lt;/p&gt;

&lt;p&gt;Big shout out to those who helped me, and put up with me (LOL), during my
break-in period: @zacharaysarah, @sftim, @kbhawkey, @jaypipes,  @jrondeau,
@jmangel, @bradtopol, @cody_clark, @thecrudge, @jaredb, @tengqm, @steveperry-53,
@mrbobbytables, @cblecker, and @kbarnard10.&lt;/p&gt;

&lt;h2 id=&#34;outro&#34;&gt;Outro&lt;/h2&gt;

&lt;p&gt;Do I grok SIG Docs? Not quite yet, but I do understand that SIG Docs needs more
dedicated resources to continue to be successful.&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; @linuxfoundation. &amp;ldquo;Megan Byrd-Sanicki, Open Source Strategist, Google @megansanicki - documentation is the #1 driver of software adoption. #ossummit.&amp;rdquo; &lt;em&gt;Twitter&lt;/em&gt;, Oct 29, 2019, 3:54 a.m., twitter.com/linuxfoundation/status/1189103201439637510.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Documentation Survey</title>
      <link>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/aimee-ukasick/&#34; target=&#34;_blank&#34;&gt;Aimee Ukasick&lt;/a&gt; and SIG Docs&lt;/p&gt;

&lt;p&gt;In September, SIG Docs conducted its first survey about the &lt;a href=&#34;https://kubernetes.io/docs/&#34; target=&#34;_blank&#34;&gt;Kubernetes
documentation&lt;/a&gt;. We&amp;rsquo;d like to thank the CNCF&amp;rsquo;s Kim
McMahon for helping us create the survey and access the results.&lt;/p&gt;

&lt;h1 id=&#34;key-takeaways&#34;&gt;Key takeaways&lt;/h1&gt;

&lt;p&gt;Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.&lt;/p&gt;

&lt;p&gt;74% of respondents would like the Tutorials section to contain advanced content.&lt;/p&gt;

&lt;p&gt;69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;survey-methodology-and-respondents&#34;&gt;Survey methodology and respondents&lt;/h1&gt;

&lt;p&gt;We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.&lt;/p&gt;

&lt;h2 id=&#34;quick-facts-about-respondents&#34;&gt;Quick facts about respondents:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner&lt;/li&gt;
&lt;li&gt;57.58% use Kubernetes in both administrator and developer roles&lt;/li&gt;
&lt;li&gt;64.65% have been using the Kubernetes documentation for more than 12 months&lt;/li&gt;
&lt;li&gt;95.96% read the documentation in English&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;question-and-response-highlights&#34;&gt;Question and response highlights&lt;/h1&gt;

&lt;h2 id=&#34;why-people-access-the-kubernetes-documentation&#34;&gt;Why people access the Kubernetes documentation&lt;/h2&gt;

&lt;p&gt;The majority of respondents stated that they access the documentation for the Concepts.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png&#34;
         alt=&#34;Why respondents access the Kubernetes documentation&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.&lt;/p&gt;

&lt;h2 id=&#34;satisfaction-with-the-documentation&#34;&gt;Satisfaction with the documentation&lt;/h2&gt;

&lt;p&gt;We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Concepts: 47.96% Moderately Satisfied&lt;/li&gt;
&lt;li&gt;Tasks: 50.54% Moderately Satisfied&lt;/li&gt;
&lt;li&gt;Reference: 40.86% Very Satisfied&lt;/li&gt;
&lt;li&gt;Tutorial: 47.25% Moderately Satisfied&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-sig-docs-can-improve-each-documentation-section&#34;&gt;How SIG Docs can improve each documentation section&lt;/h2&gt;

&lt;p&gt;We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they&amp;#39;re a bucket of separate eels moving in different directions right now
- More diagrams, and more example code&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Respondents used the &amp;ldquo;Other&amp;rdquo; text box to record areas causing frustration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I&amp;#39;ve never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;how-sig-docs-can-improve-the-documentation-overall&#34;&gt;How SIG Docs can improve the documentation overall&lt;/h2&gt;

&lt;p&gt;We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Other respondents provided feedback on the content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;-  ...But since we&amp;#39;re talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each
configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
- More in-depth examples and use cases would be great. I often feel that the Kubernetes
documentation scratches the surface of a topic, which might be great for new users, but it leaves
more experienced users without much &amp;#34;official&amp;#34; guidance on how to implement certain things.
- More production like examples in the resource sections (notably secrets) or links to production like
examples
- It would be great to see a very clear &amp;#34;Quick Start&amp;#34; A-&amp;gt;Z up and running like many other tech
projects. There are a handful of almost-quick-starts, but no single guidance. The result is
information overkill.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few respondents provided technical suggestions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Make table columns sortable and filterable using a ReactJS or Angular component.
- For most, I think creating documentation with Hugo - a system for static site generation - is not
appropriate. There are better systems for documenting large software project. Specifically, I would
like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy to
learn if you know markdown, it is widely adopted by other projects (e.g. every software project in
readthedocs.io, linux kernel, docs.python.org etc).&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.&lt;/p&gt;

&lt;h1 id=&#34;where-to-see-more&#34;&gt;Where to see more&lt;/h1&gt;

&lt;p&gt;Survey results summary, charts, and raw data are available in &lt;code&gt;kubernetes/community&lt;/code&gt; sig-docs &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs/survey&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; directory.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit San Diego Schedule Announced!</title>
      <link>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid>
      <description>
        
        
        &lt;p&gt;Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p&gt;

&lt;p&gt;tl;dr A week ago we announced that &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;registration is open&lt;/a&gt; for the contributor
summit , and we&amp;rsquo;re now live with &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/&#34; target=&#34;_blank&#34;&gt;the full Contributor Summit schedule!&lt;/a&gt;
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop.  (&lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;Register here!&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don&amp;rsquo;t often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations.  We want folks to learn and have a
good time meeting their OSS teammates.&lt;/p&gt;

&lt;p&gt;Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we&amp;rsquo;ve covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg&#34; alt=&#34;Unconference&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While the schedule contains difficult decisions in every timeslot, we&amp;rsquo;ve picked
a few below to give you a taste of what you&amp;rsquo;ll hear, see, and participate in, at
the summit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvMc&#34; target=&#34;_blank&#34;&gt;Vision&lt;/a&gt;&lt;/strong&gt;: SIG-Architecture will be sharing their vision of where we&amp;rsquo;re going
with Kubernetes development for the next year and beyond.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvMj&#34; target=&#34;_blank&#34;&gt;Security&lt;/a&gt;&lt;/strong&gt;: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/Vv6Z&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt;&lt;/strong&gt;: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start?  Rob Keilty will help you get a Prow test environment
running on your laptop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvNa&#34; target=&#34;_blank&#34;&gt;Git&lt;/a&gt;&lt;/strong&gt;: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VutA&#34; target=&#34;_blank&#34;&gt;Reviewing&lt;/a&gt;&lt;/strong&gt;: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvNJ&#34; target=&#34;_blank&#34;&gt;End Users&lt;/a&gt;&lt;/strong&gt;: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;amp;A with contributors to strengthen our feedback loop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/Vux2&#34; target=&#34;_blank&#34;&gt;Docs&lt;/a&gt;&lt;/strong&gt;: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;re also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp;amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.&lt;/p&gt;

&lt;p&gt;Hope to see you all there, and &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;make sure you register!&lt;/a&gt;
&lt;a href=&#34;http://git.k8s.io/community/events/events-team&#34; target=&#34;_blank&#34;&gt;San Diego team&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2019 Steering Committee Election Results</title>
      <link>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://git.k8s.io/community/events/elections/2019&#34; target=&#34;_blank&#34;&gt;2019 Steering Committee Election&lt;/a&gt; is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.&lt;/p&gt;

&lt;p&gt;Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Christoph Blecker (&lt;a href=&#34;https://github.com/cblecker&#34; target=&#34;_blank&#34;&gt;@cblecker&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Derek Carr (&lt;a href=&#34;https://github.com/derekwaynecarr&#34; target=&#34;_blank&#34;&gt;@derekwaynecarr&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nikhita Raghunath (&lt;a href=&#34;https://github.com/nikhita&#34; target=&#34;_blank&#34;&gt;@nikhita&lt;/a&gt;), Loodse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paris Pittman (&lt;a href=&#34;https://github.com/parispittman&#34; target=&#34;_blank&#34;&gt;@parispittman&lt;/a&gt;)&lt;/strong&gt;, &lt;strong&gt;Google&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They join Aaron Crickenberger (&lt;a href=&#34;https://github.com/spiffxp&#34; target=&#34;_blank&#34;&gt;@spiffxp&lt;/a&gt;), Google; Davanum Srinivas (&lt;a href=&#34;https://github.com/dims&#34; target=&#34;_blank&#34;&gt;@dims&lt;/a&gt;),
VMware; and Timothy St. Clair (&lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.&lt;/p&gt;

&lt;h2 id=&#34;big-thanks&#34;&gt;Big Thanks!&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:

&lt;ul&gt;
&lt;li&gt;Joe Beda (&lt;a href=&#34;https://github.com/jbeda&#34; target=&#34;_blank&#34;&gt;@jbeda&lt;/a&gt;), VMware&lt;/li&gt;
&lt;li&gt;Brendan Burns (&lt;a href=&#34;https://github.com/brendandburns&#34; target=&#34;_blank&#34;&gt;@brendandburns&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Clayton Coleman (&lt;a href=&#34;https://github.com/smarterclayton&#34; target=&#34;_blank&#34;&gt;@smarterclayton&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;li&gt;Brian Grant (&lt;a href=&#34;https://github.com/bgrant0607&#34; target=&#34;_blank&#34;&gt;@bgrant0607&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;@thockin&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Sarah Novotny (&lt;a href=&#34;https://github.com/sarahnovotny&#34; target=&#34;_blank&#34;&gt;@sarahnovotny&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Brandon Philips (&lt;a href=&#34;https://github.com/philips&#34; target=&#34;_blank&#34;&gt;@philips&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:

&lt;ul&gt;
&lt;li&gt;Quinton Hoole (&lt;a href=&#34;https://github.com/quinton-hoole&#34; target=&#34;_blank&#34;&gt;@quinton-hoole&lt;/a&gt;), Huawei&lt;/li&gt;
&lt;li&gt;Michelle Noorali (&lt;a href=&#34;https://github.com/michelleN&#34; target=&#34;_blank&#34;&gt;@michelleN&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Phillip Wittrock (&lt;a href=&#34;https://github.com/pwittrock&#34; target=&#34;_blank&#34;&gt;@pwittrock&lt;/a&gt;), Google&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.&lt;/li&gt;
&lt;li&gt;Thanks to all 377 voters who cast a ballot.&lt;/li&gt;
&lt;li&gt;And last but not least…Thanks to Cornell University for hosting &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;get-involved-with-the-steering-committee&#34;&gt;Get Involved with the Steering Committee&lt;/h2&gt;

&lt;p&gt;You can follow along with Steering Committee &lt;a href=&#34;https://github.com/kubernetes/steering/projects/1&#34; target=&#34;_blank&#34;&gt;backlog items&lt;/a&gt; and weigh in by
filing an issue or creating a PR against their &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt;. They meet bi-weekly on
&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Wednesdays at 8pm UTC&lt;/a&gt; and regularly attend Meet Our Contributors.  They can
also be contacted at their public mailing list &lt;a href=&#34;mailto:steering@kubernetes.io&#34; target=&#34;_blank&#34;&gt;steering@kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Steering Committee Meetings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34; target=&#34;_blank&#34;&gt;YouTube Playlist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit San Diego Registration Open!</title>
      <link>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;Contributor Summit San Diego 2019 Event Page&lt;/a&gt;&lt;br /&gt;
Registration is now open and in record time, we’ve hit capacity for the
&lt;em&gt;new contributor workshop&lt;/em&gt; session of the event! Waitlist is now available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sunday, November 17&lt;/strong&gt;&lt;br /&gt;
Evening Contributor Celebration:&lt;br /&gt;
&lt;a href=&#34;https://quartyardsd.com/&#34; target=&#34;_blank&#34;&gt;QuartYard&lt;/a&gt;*&lt;br /&gt;
Address: 1301 Market Street, San Diego, CA 92101&lt;br /&gt;
Time: 6:00PM - 9:00PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monday, November 18&lt;/strong&gt;&lt;br /&gt;
All Day Contributor Summit:&lt;br /&gt;
&lt;a href=&#34;https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2&#34; target=&#34;_blank&#34;&gt;Marriott Marquis San Diego Marina&lt;/a&gt;&lt;br /&gt;
Address: 333 W Harbor Dr, San Diego, CA 92101&lt;br /&gt;
Time: 9:00AM - 5:00PM&lt;/p&gt;

&lt;p&gt;While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.&lt;/p&gt;

&lt;p&gt;Kubernetes has a large distributed remote contributing team, from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All&#34; target=&#34;_blank&#34;&gt;individuals and
organizations&lt;/a&gt; all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We&amp;rsquo;re going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We&amp;rsquo;ve heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.&lt;/p&gt;

&lt;p&gt;This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/events-team&#34; target=&#34;_blank&#34;&gt;rolebooks, guidelines,
best practices&lt;/a&gt; and opened up our &lt;a href=&#34;https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;meetings&lt;/a&gt; and &lt;a href=&#34;https://github.com/orgs/kubernetes/projects/21&#34; target=&#34;_blank&#34;&gt;project board&lt;/a&gt;. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.&lt;/p&gt;

&lt;p&gt;Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the &lt;a href=&#34;https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes&#34; target=&#34;_blank&#34;&gt;SIG Intro and Deep Dive sessions&lt;/a&gt; during KubeCon + CloudNativeCon to
participate in Q&amp;amp;A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.&lt;/p&gt;

&lt;p&gt;We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;register right now&lt;/a&gt;! This event will sell out - here’s your warning.
:smiley:&lt;/p&gt;

&lt;p&gt;Check out past blogs on &lt;a href=&#34;https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/&#34; target=&#34;_blank&#34;&gt;persona building around our events&lt;/a&gt; and the &lt;a href=&#34;https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona summit story&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG&#34; alt=&#34;Group Picture in 2018&#34; /&gt;&lt;/p&gt;

&lt;p&gt;*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.16: Custom Resources, Overhauled Metrics, and Volume Extensions</title>
      <link>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.16/release_team.md&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.16 Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.16, our third release of 2019! Kubernetes 1.16 consists of 31 enhancements: 8 enhancements moving to stable, 8 enhancements in beta, and 15 enhancements in alpha.&lt;/p&gt;

&lt;h1 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h1&gt;

&lt;h2 id=&#34;custom-resources&#34;&gt;Custom resources&lt;/h2&gt;

&lt;p&gt;CRDs are in widespread use as a Kubernetes extensibility mechanism and have been available in beta since the 1.7 release. The 1.16 release marks the graduation of CRDs to general availability (GA).&lt;/p&gt;

&lt;h2 id=&#34;overhauled-metrics&#34;&gt;Overhauled metrics&lt;/h2&gt;

&lt;p&gt;Kubernetes has previously made extensive use of a global metrics registry to register metrics to be exposed. By implementing a metrics registry, metrics are registered in more transparent means. Previously, Kubernetes metrics have been excluded from any kind of stability requirements.&lt;/p&gt;

&lt;h2 id=&#34;volume-extension&#34;&gt;Volume Extension&lt;/h2&gt;

&lt;p&gt;There are quite a few enhancements in this release that pertain to volumes and volume modifications. Volume resizing support in CSI specs is moving to beta which allows for any CSI spec volume plugin to be resizable.&lt;/p&gt;

&lt;h1 id=&#34;significant-changes-to-the-kubernetes-api&#34;&gt;Significant Changes to the Kubernetes API&lt;/h1&gt;

&lt;p&gt;As the Kubernetes API has evolved, we have promoted some API resources to &lt;em&gt;stable&lt;/em&gt;, others have been reorganized to different groups. We deprecate older versions of a resource and make newer versions available in accordance with the &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning&#34; target=&#34;_blank&#34;&gt;API versioning policy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An example of this is the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; resource. This was introduced under the &lt;code&gt;extensions/v1beta1&lt;/code&gt; group in 1.6 and as the project changed has been promoted to &lt;code&gt;extensions/v1beta2&lt;/code&gt;, &lt;code&gt;apps/v1beta2&lt;/code&gt; and finally promoted to &lt;code&gt;stable&lt;/code&gt; and moved to &lt;code&gt;apps/v1&lt;/code&gt; in 1.9.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s important to note that until this release the project has not stopped serving any of the previous versions of the any of the deprecated resources.&lt;/p&gt;

&lt;p&gt;This means that folks interacting with the Kubernetes API have not been &lt;em&gt;required&lt;/em&gt; to move to the new version of any of the deprecated API objects.&lt;/p&gt;

&lt;p&gt;In 1.16 if you submit a &lt;code&gt;Deployment&lt;/code&gt; to the API server and specify &lt;code&gt;extensions/v1beta1&lt;/code&gt; as the API group it will be rejected with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: unable to recognize &amp;quot;deployment&amp;quot;: no matches for kind &amp;quot;Deployment&amp;quot; in version &amp;quot;extensions/v1beta1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this release we are taking a very important step in the maturity of the Kubernetes API, and are no longer serving the deprecated APIs. Our earlier post &lt;a href=&#34;https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/&#34; target=&#34;_blank&#34;&gt;Deprecated APIs Removed In 1.16: Here’s What You Need To Know&lt;/a&gt; tells you more, including which resources are affected.&lt;/p&gt;

&lt;h1 id=&#34;additional-enhancements&#34;&gt;Additional Enhancements&lt;/h1&gt;

&lt;h2 id=&#34;custom-resources-reach-general-availability&#34;&gt;Custom Resources Reach General Availability&lt;/h2&gt;

&lt;p&gt;CRDs have become the basis for extensions in the Kubernetes ecosystem. Started as a ground-up redesign of the ThirdPartyResources prototype, they have finally reached GA in 1.16 with apiextensions.k8s.io/v1, as the hard-won lessons of API evolution in Kubernetes have been integrated. As we transition to GA, the focus is on data consistency for API clients.&lt;/p&gt;

&lt;p&gt;As you upgrade to the GA API, you’ll notice that several of the previously optional guard rails have become required and/or default behavior. Things like structural schemas, pruning unknown fields, validation, and protecting the *.k8s.io group are important for ensuring the longevity of your APIs and are now much harder to accidentally miss. Defaulting is another important part of API evolution and that support will be on by default for CRD.v1. The combination of these, along with CRD conversion mechanisms are enough to build stable APIs that evolve over time, the same way that native Kubernetes resources have changed without breaking backward-compatibility.&lt;/p&gt;

&lt;p&gt;Updates to the CRD API won’t end here. We have ideas for features like arbitrary subresources, API group migration, and maybe a more efficient serialization protocol, but the changes from here are expected to be optional and complementary in nature to what’s already here in the GA API. Happy operator writing!&lt;/p&gt;

&lt;p&gt;Details on how to work with custom resources can be found &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;in the Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;opening-doors-with-windows-enhancements&#34;&gt;Opening Doors With Windows Enhancements&lt;/h2&gt;

&lt;h3 id=&#34;beta-enhancing-the-workload-identity-options-for-windows-containers&#34;&gt;Beta: Enhancing the workload identity options for Windows containers&lt;/h3&gt;

&lt;p&gt;Active Directory Group Managed Service Account (GMSA) support is graduating to beta and certain annotations that were introduced with the alpha support are being deprecated. GMSA is a specific type of Active Directory account that enables Windows containers to carry an identity across the network and communicate with other resources. Windows containers can now gain authenticated access to external resources. In addition, GMSA provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.&lt;/p&gt;

&lt;p&gt;Adding support for RunAsUserName as an alpha release. The RunAsUserName is a string specifying the windows identity (or username) in Windows to run the entrypoint of the container and is a part of the newly introduced windowsOptions component of the securityContext (WindowsSecurityContextOptions).&lt;/p&gt;

&lt;h3 id=&#34;alpha-improvements-to-setup-node-join-experience-with-kubeadm&#34;&gt;Alpha: Improvements to setup &amp;amp; node join experience with kubeadm&lt;/h3&gt;

&lt;p&gt;Introducing alpha support for kubeadm, enabling Kubernetes users to easily join (and reset) Windows worker nodes to an existing cluster the same way they do for Linux nodes. Users can utilize kubeadm to prepare and add a Windows node to cluster. When the operations are complete, the node will be in a Ready state and able to run Windows containers. In addition, we will also provide a set of Windows-specific scripts to enable the installation of prerequisites and CNIs ahead of joining the node to the cluster.&lt;/p&gt;

&lt;h3 id=&#34;alpha-introducing-support-for-container-storage-interface-csi&#34;&gt;Alpha: Introducing support for Container Storage Interface (CSI)&lt;/h3&gt;

&lt;p&gt;Introducing CSI plugin support for out-of-tree providers, enabling Windows nodes in a Kubernetes cluster to leverage persistent storage capabilities for Windows-based workloads. This significantly expands the storage options of Windows workloads, adding onto a list that included FlexVolume and in-tree storage plugins. This capability is achieved through a host OS proxy that enables the execution of privileged operations on the Windows node on behalf of containers.&lt;/p&gt;

&lt;h2 id=&#34;introducing-endpoint-slices&#34;&gt;Introducing Endpoint Slices&lt;/h2&gt;

&lt;p&gt;The release of Kubernetes 1.16 includes an exciting new alpha feature: the EndpointSlice API. This API provides a scalable and extensible alternative to the &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#endpoints-v1-core&#34; target=&#34;_blank&#34;&gt;Endpoints&lt;/a&gt; resource, which dates back to the very first versions of Kubernetes. Behind the scenes, Endpoints play a big role in network routing within Kubernetes. Each Service endpoint is tracked within these resources - kube-proxy uses them for generating proxy rules that allow pods to communicate with each other so easily in Kubernetes, and many ingress controllers use them to route HTTP traffic directly to pods.&lt;/p&gt;

&lt;h3 id=&#34;providing-greater-scalability&#34;&gt;Providing Greater Scalability&lt;/h3&gt;

&lt;p&gt;A key goal for EndpointSlices is to enable greater scalability for Kubernetes Services. With the existing Endpoints API, a single instance must include network endpoints representing all pods matching a Service. As Services start to scale to thousands of pods, the corresponding Endpoints resources become quite large. Simply adding or removing one endpoint from a Service at this scale can be quite costly. As the Endpoints instance is updated, every piece of code watching Endpoints will need to be sent a full copy of the resource. With kube-proxy running on every node in a cluster, a copy needs to be sent to every single node. At a small scale, this is not an issue, but it becomes increasingly noticeable as clusters get larger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/endpoint-slices.png&#34; alt=&#34;Endpoints to Endpoint Slice&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With EndpointSlices, network endpoints for a Service are split into multiple instances, significantly decreasing the amount of data required for updates at scale. By default, EndpointSlices are limited to 100 endpoints each.&lt;/p&gt;

&lt;p&gt;For example, let’s take a cluster with 10,000 Service endpoints spread over 5,000 nodes. A single Pod update would result in approximately 5GB transmitted with the Endpoints API (that’s enough to fill a DVD). This becomes increasingly significant given how frequently Endpoints can change during events like rolling updates on Deployments. The same update will be much more efficient with EndpointSlices since each one includes only a tiny portion of the total number of Service endpoints. Instead of transferring a big Endpoints object to each node, only the small EndpointSlice that’s been changed has to be transferred. In this example, EndpointSlices would decrease data transferred by approximately 100x.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Endpoints &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Endpoint Slices&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;# of resources
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;1&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;20k / 100 = 200&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;# of network endpoints stored
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;1 * 20k = 20k&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;200 * 100 = 20k&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;size of each resource
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;20k * const = ~2.0 MB&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt; 100 * const = ~10 kB&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;watch event data transferred
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;~2.0MB * 5k = 10GB&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;~10kB * 5k = 50MB&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&#34;providing-greater-extensibility&#34;&gt;Providing Greater Extensibility&lt;/h3&gt;

&lt;p&gt;A second goal for EndpointSlices was to provide a resource that would be highly extensible and useful across a wide variety of use cases. One of the key additions with EndpointSlices involves a new topology attribute. By default, this will be populated with the existing topology labels used throughout Kubernetes indicating attributes such as region and zone. Of course, this field can be populated with custom labels as well for more specialized use cases.&lt;/p&gt;

&lt;p&gt;EndpointSlices also include greater flexibility for address types. Each contains a list of addresses. An initial use case for multiple addresses would be to support dual-stack endpoints with both IPv4 and IPv6 addresses. As an example, here’s a simple EndpointSlice showing how one could be represented:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: discovery.k8s.io/v1alpha
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IP
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
    - &amp;quot;10.1.2.3&amp;quot;
    - &amp;quot;2001:db8::1234:5678&amp;quot;
    topology:
      kubernetes.io/hostname: node-1
      topology.kubernetes.io/zone: us-west2-a
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;more-about-endpoint-slices&#34;&gt;More About Endpoint Slices&lt;/h3&gt;

&lt;p&gt;EndpointSlices are an alpha feature in Kubernetes 1.16 and not enabled by default. The Endpoints API will continue to be enabled by default, but we’re working to move the largest Endpoints consumers to the new EndpointSlice API. Notably, kube-proxy in Kubernetes 1.16 includes alpha support for EndpointSlices.&lt;/p&gt;

&lt;p&gt;The official Kubernetes documentation contains more information about EndpointSlices as well as how to enable them in your cluster. There’s also a &lt;a href=&#34;https://www.youtube.com/watch?v=Y5JOCCbJ_Fg&#34; target=&#34;_blank&#34;&gt;great KubeCon talk&lt;/a&gt; that provides more background on the initial rationale for developing this API.&lt;/p&gt;

&lt;h4 id=&#34;notable-feature-updates&#34;&gt;Notable Feature Updates&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/693&#34; target=&#34;_blank&#34;&gt;Topology Manager&lt;/a&gt;, a new Kubelet component, aims to co-ordinate resource assignment decisions to provide optimized resource allocations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs//concepts/services-networking/dual-stack/&#34; target=&#34;_blank&#34;&gt;IPv4/IPv6 dual-stack&lt;/a&gt; enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/20190422-cloud-controller-manager-migration.md&#34; target=&#34;_blank&#34;&gt;Extensions&lt;/a&gt; for Cloud Controller Manager Migration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.16 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.16.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.16 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release Team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;http://bit.ly/k8s116-team&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Lachlan Evenson, Principal Program Manager at Microsoft. The 32 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;32,000 individual contributors&lt;/a&gt; to date and an active community of more than 66,000 people.&lt;/p&gt;

&lt;h2 id=&#34;release-mascot&#34;&gt;Release Mascot&lt;/h2&gt;

&lt;p&gt;The Kubernetes 1.16 release crest was loosely inspired by the Apollo 16 mission crest. It represents the hard work of the release-team and the community alike and is an ode to the challenges and fun times we shared as a team throughout the release cycle. Many thanks to Ronan Flynn-Curran of Microsoft for creating this magnificent piece.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/mascot.png&#34; alt=&#34;Kubernetes 1.16 Release Mascot&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-updates&#34;&gt;Kubernetes Updates&lt;/h1&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://k8s.devstats.cncf.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. This past year, &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&#34; target=&#34;_blank&#34;&gt;1,147 different companies and over 3,149 individuals&lt;/a&gt; contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The Kubernetes project leadership created the Security Audit Working Group to oversee the very first third-part &lt;a href=&#34;https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/&#34; target=&#34;_blank&#34;&gt;Kubernetes security audit&lt;/a&gt;, in an effort to improve the overall security of the ecosystem.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.cncf.io/announcement/2019/07/09/the-cloud-native-computing-foundation-announces-the-kubernetes-certified-service-providers-program-has-reached-100-participants/&#34; target=&#34;_blank&#34;&gt;Kubernetes Certified Service Providers&lt;/a&gt; program (KCSP) reached 100 member companies, ranging from the largest multinational cloud, enterprise software, and consulting companies to tiny startups.&lt;/li&gt;
&lt;li&gt;The first &lt;a href=&#34;https://www.cncf.io/blog/2019/08/29/announcing-the-cncf-kubernetes-project-journey-report/&#34; target=&#34;_blank&#34;&gt;Kubernetes Project Journey Report&lt;/a&gt; was released, showcasing the massive growth of the project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon-cloudnativecon&#34;&gt;KubeCon + CloudNativeCon&lt;/h2&gt;

&lt;p&gt;The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in San Diego, California from November 18-21, 2019. Join Kubernetes, Prometheus, Envoy, CoreDNS, containerd, Fluentd, OpenTracing, gRPC, CNI, Jaeger, Notary, TUF, Vitess, NATS, Linkerd, Helm, Rook, Harbor, etcd, Open Policy Agent, CRI-O, and TiKV as the community gathers for four days to further the education and advancement of cloud native computing. &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.16 release team on Oct 22, 2019 to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/9015681469655/WN_JTLYA0DMRD6Mnm2f64KYMg&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below. Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing etcd 3.4</title>
      <link>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gyuho Lee (Amazon Web Services, @&lt;a href=&#34;https://github.com/gyuho&#34; target=&#34;_blank&#34;&gt;gyuho&lt;/a&gt;), Jingyi Hu (Google, @&lt;a href=&#34;https://github.com/jingyih&#34; target=&#34;_blank&#34;&gt;jingyih&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;etcd 3.4 focuses on stability, performance and ease of operation, with features like pre-vote and non-voting member and improvements to storage backend and client balancer.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.4.md&#34; target=&#34;_blank&#34;&gt;CHANGELOG&lt;/a&gt; for full lists of changes.&lt;/p&gt;

&lt;h2 id=&#34;better-storage-backend&#34;&gt;Better Storage Backend&lt;/h2&gt;

&lt;p&gt;etcd v3.4 includes a number of performance improvements for large scale Kubernetes workloads.&lt;/p&gt;

&lt;p&gt;In particular, etcd experienced performance issues with a large number of concurrent read transactions even when there is no write (e.g. &lt;code&gt;“read-only range request ... took too long to execute”&lt;/code&gt;). Previously, the storage backend commit operation on pending writes blocks incoming read transactions, even when there was no pending write. Now, the commit &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9296&#34; target=&#34;_blank&#34;&gt;does not block reads&lt;/a&gt; which improve long-running read transaction performance.&lt;/p&gt;

&lt;p&gt;We further made &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10523&#34; target=&#34;_blank&#34;&gt;backend read transactions fully concurrent&lt;/a&gt;. Previously, ongoing long-running read transactions block writes and upcoming reads. With this change, write throughput is increased by 70% and P99 write latency is reduced by 90% in the presence of long-running reads. We also ran &lt;a href=&#34;https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-scale-performance/1130745634945503235&#34; target=&#34;_blank&#34;&gt;Kubernetes 5000-node scalability test on GCE&lt;/a&gt; with this change and observed similar improvements. For example, in the very beginning of the test where there are a lot of long-running “LIST pods”, the P99 latency of “POST clusterrolebindings” is &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10523#issuecomment-499262001&#34; target=&#34;_blank&#34;&gt;reduced by 97.4%&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More improvements have been made to lease storage. We enhanced &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9418&#34; target=&#34;_blank&#34;&gt;lease expire/revoke performance&lt;/a&gt; by storing lease objects more efficiently, and made &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9229&#34; target=&#34;_blank&#34;&gt;lease look-up operation non-blocking&lt;/a&gt; with current lease grant/revoke operation. And etcd v3.4 introduces &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9924&#34; target=&#34;_blank&#34;&gt;lease checkpoint&lt;/a&gt; as an experimental feature to persist remaining time-to-live values through consensus. This ensures short-lived lease objects are not auto-renewed after leadership election. This also prevents lease object pile-up when the time-to-live value is relatively large (e.g. &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/65497&#34; target=&#34;_blank&#34;&gt;1-hour TTL never expired in Kubernetes use case&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;improved-raft-voting-process&#34;&gt;Improved Raft Voting Process&lt;/h2&gt;

&lt;p&gt;etcd server implements &lt;a href=&#34;https://raft.github.io&#34; target=&#34;_blank&#34;&gt;Raft consensus algorithm&lt;/a&gt; for data replication. Raft is a leader-based protocol. Data is replicated from leader to follower; a follower forwards proposals to a leader, and the leader decides what to commit or not. Leader persists and replicates an entry, once it has been agreed by the quorum of cluster. The cluster members elect a single leader, and all other members become followers. The elected leader periodically sends heartbeats to its followers to maintain its leadership, and expects responses from each follower to keep track of its progress.&lt;/p&gt;

&lt;p&gt;In its simplest form, a Raft leader steps down to a follower when it receives a message with higher terms without any further cluster-wide health checks. This behavior can affect the overall cluster availability.&lt;/p&gt;

&lt;p&gt;For instance, a flaky (or rejoining) member drops in and out, and starts campaign. This member ends up with higher terms, ignores all incoming messages with lower terms, and sends out messages with higher terms. When the leader receives this message of a higher term, it reverts back to follower.&lt;/p&gt;

&lt;p&gt;This becomes more disruptive when there’s a network partition. Whenever the partitioned node regains its connectivity, it can possibly trigger the leader re-election. To address this issue, etcd Raft introduces a new node state pre-candidate with the &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9352&#34; target=&#34;_blank&#34;&gt;pre-vote feature&lt;/a&gt;. The pre-candidate first asks other servers whether it&amp;rsquo;s up-to-date enough to get votes. Only if it can get votes from the majority, it increments its term and starts an election. This extra phase improves the robustness of leader election in general. And helps the leader remain stable as long as it maintains its connectivity with the quorum of its peers.&lt;/p&gt;

&lt;p&gt;Similarly, etcd availability can be affected when a restarting node has not received the leader heartbeats in time (e.g. due to slow network), which triggers the leader election. Previously, etcd fast-forwards election ticks on server start, with only one tick left for leader election. For example, when the election timeout is 1-second, the follower only waits 100ms for leader contacts before starting an election. This speeds up initial server start, by not having to wait for the election timeouts (e.g. election is triggered in 100ms instead of 1-second). Advancing election ticks is also useful for cross datacenter deployments with larger election timeouts. However, in many cases, the availability is more critical than the speed of initial leader election. To ensure better availability with rejoining nodes, etcd now &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9415&#34; target=&#34;_blank&#34;&gt;adjusts election ticks&lt;/a&gt; with more than one tick left, thus more time for the leader to prevent a disruptive restart.&lt;/p&gt;

&lt;h2 id=&#34;raft-non-voting-member-learner&#34;&gt;Raft Non-Voting Member, Learner&lt;/h2&gt;

&lt;p&gt;The challenge with membership reconfiguration is that it often leads to quorum size changes, which are prone to cluster unavailabilities. Even if it does not alter the quorum, clusters with membership change are more likely to experience other underlying problems. To improve the reliability and confidence of reconfiguration, a new role - learner is introduced in etcd 3.4 release.&lt;/p&gt;

&lt;p&gt;A new etcd member joins the cluster with no initial data, requesting all historical updates from the leader until it catches up to the leader’s logs. This means the leader’s network is more likely to be overloaded, blocking or dropping leader heartbeats to followers. In such cases, a follower may experience election-timeout and start a new leader election. That is, a cluster with a new member is more vulnerable to leader election. Both leader election and the subsequent update propagation to the new member are prone to causing periods of cluster unavailability (see &lt;em&gt;Figure 1&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-1.png&#34; alt=&#34;learner-figure-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The worst case is a misconfigured membership add. Membership reconfiguration in etcd is a two-step process: &lt;code&gt;etcdctl member add&lt;/code&gt; with peer URLs, and starting a new etcd to join the cluster. That is, &lt;code&gt;member add&lt;/code&gt; command is applied whether the peer URL value is invalid or not. If the first step is to apply the invalid URLs and change the quorum size, it is possible that the cluster already loses the quorum until the new node connects. Since the node with invalid URLs will never become online and there’s no leader, it is impossible to revert the membership change (see &lt;em&gt;Figure 2&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-2.png&#34; alt=&#34;learner-figure-2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This becomes more complicated when there are partitioned nodes (see the &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt; for more).&lt;/p&gt;

&lt;p&gt;In order to address such failure modes, etcd introduces a &lt;a href=&#34;https://github.com/etcd-io/etcd/issues/10537&#34; target=&#34;_blank&#34;&gt;new node state “Learner”&lt;/a&gt;, which joins the cluster as a non-voting member until it catches up to leader’s logs. This means the learner still receives all updates from leader, while it does not count towards the quorum, which is used by the leader to evaluate peer activeness. The learner only serves as a standby node until promoted. This relaxed requirements for quorum provides the better availability during membership reconfiguration and operational safety (see &lt;em&gt;Figure 3&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-3.png&#34; alt=&#34;learner-figure-3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We will further improve learner robustness, and explore auto-promote mechanisms for easier and more reliable operation. Please read our &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md&#34; target=&#34;_blank&#34;&gt;learner design documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#add-a-new-member-as-learner&#34; target=&#34;_blank&#34;&gt;runtime-configuration document&lt;/a&gt; for user guides.&lt;/p&gt;

&lt;h2 id=&#34;new-client-balancer&#34;&gt;New Client Balancer&lt;/h2&gt;

&lt;p&gt;etcd is designed to tolerate various system and network faults. By design, even if one node goes down, the cluster “appears” to be working normally, by providing one logical cluster view of multiple servers. But, this does not guarantee the liveness of the client. Thus, etcd client has implemented a different set of intricate protocols to guarantee its correctness and high availability under faulty conditions.&lt;/p&gt;

&lt;p&gt;Historically, etcd client balancer heavily relied on old gRPC interface: every gRPC dependency upgrade broke client behavior. A majority of development and debugging efforts were devoted to fixing those client behavior changes. As a result, its implementation has become overly complicated with bad assumptions on server connectivity. The primary goal was to &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9860&#34; target=&#34;_blank&#34;&gt;simplify balancer failover logic in etcd v3.4 client&lt;/a&gt;; instead of maintaining a list of unhealthy endpoints, which may be stale, simply roundrobin to the next endpoint whenever client gets disconnected from the current endpoint. It does not assume endpoint status. Thus, no more complicated status tracking is needed.&lt;/p&gt;

&lt;p&gt;Furthermore, the new client now creates its own credential bundle to &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10911&#34; target=&#34;_blank&#34;&gt;fix balancer failover against secure endpoints&lt;/a&gt;. This resolves the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/72102&#34; target=&#34;_blank&#34;&gt;year-long bug&lt;/a&gt;, where kube-apiserver loses its connectivity to etcd cluster when the first etcd server becomes unavailable.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md&#34; target=&#34;_blank&#34;&gt;client balancer design documentation&lt;/a&gt; for more.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: OPA Gatekeeper: Policy and Governance for Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Open Policy Agent Gatekeeper&lt;/a&gt; project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.&lt;/p&gt;

&lt;p&gt;The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/Yup1FUc2Qn0&#34; target=&#34;_blank&#34;&gt;Intro: Open Policy Agent Gatekeeper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/n94_FNhuzy4&#34; target=&#34;_blank&#34;&gt;Deep Dive: Open Policy Agent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;

&lt;p&gt;If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?&lt;/p&gt;

&lt;p&gt;For example, you can enforce policies like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All images must be from approved repositories&lt;/li&gt;
&lt;li&gt;All ingress hostnames must be globally unique&lt;/li&gt;
&lt;li&gt;All pods must have resource limits&lt;/li&gt;
&lt;li&gt;All namespaces must have a label that lists a point-of-contact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes allows decoupling policy decisions from the API server by means of &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&#34; target=&#34;_blank&#34;&gt;admission controller webhooks&lt;/a&gt; to intercept admission requests before they are persisted as objects in Kubernetes. &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; was created to enable users to customize admission control via configuration, not code and to bring awareness of the cluster’s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the &lt;a href=&#34;https://www.openpolicyagent.org&#34; target=&#34;_blank&#34;&gt;Open Policy Agent (OPA)&lt;/a&gt;, a policy engine for Cloud Native environments hosted by CNCF.&lt;/p&gt;

&lt;h2 id=&#34;evolution&#34;&gt;Evolution&lt;/h2&gt;

&lt;p&gt;Before we dive into the current state of Gatekeeper, let’s take a look at how the Gatekeeper project has evolved.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.&lt;/li&gt;
&lt;li&gt;Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.

&lt;ul&gt;
&lt;li&gt;Gatekeeper v3.0 - The admission controller is integrated with the &lt;a href=&#34;https://github.com/open-policy-agent/frameworks/tree/master/constraint&#34; target=&#34;_blank&#34;&gt;OPA Constraint Framework&lt;/a&gt; to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for &lt;a href=&#34;https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/&#34; target=&#34;_blank&#34;&gt;Rego&lt;/a&gt; policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;gatekeeper-v3-0-features&#34;&gt;Gatekeeper v3.0 Features&lt;/h2&gt;

&lt;p&gt;Now let’s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the object’s labels. How can you do this with Gatekeeper?&lt;/p&gt;

&lt;h3 id=&#34;validating-admission-control&#34;&gt;Validating Admission Control&lt;/h3&gt;

&lt;p&gt;Once all the Gatekeeper components have been &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;installed&lt;/a&gt; in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.&lt;/p&gt;

&lt;p&gt;During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.&lt;/p&gt;

&lt;h3 id=&#34;policies-and-constraints&#34;&gt;Policies and Constraints&lt;/h3&gt;

&lt;p&gt;With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.&lt;/p&gt;

&lt;p&gt;Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.&lt;/p&gt;

&lt;p&gt;For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;templates.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ConstraintTemplate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;crd:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;names:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;listKind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabelsList&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;plural:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;singular:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;validation:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Schema for the `parameters` field&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;openAPIV3Schema:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;targets:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;target:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;admission.k8s.gatekeeper.sh&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;rego:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;        package k8srequiredlabels&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;deny[{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;msg&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;msg,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;details&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;missing_labels&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;missing}}]&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;provided&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;|&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;input.review.object.metadata.labels[label]}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;required&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;|&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;input.parameters.labels[_]}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;missing&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;provided&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;count(missing)&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;msg&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;sprintf(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;you must provide labels: %v&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[missing])&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label &lt;code&gt;hr&lt;/code&gt; to be present on all namespaces.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-hr&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;hr&amp;#34;&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Similarly, another Constraint CRD that requires the label &lt;code&gt;finance&lt;/code&gt; to be present on all namespaces can easily be created from the same Constraint template.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-finance&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;finance&amp;#34;&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.&lt;/p&gt;

&lt;h3 id=&#34;audit&#34;&gt;Audit&lt;/h3&gt;

&lt;p&gt;The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as &lt;code&gt;violations&lt;/code&gt; listed in the &lt;code&gt;status&lt;/code&gt; field of the relevant Constraint.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-hr&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;hr&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;status:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;auditTimestamp:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;byPod:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforced:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gatekeeper-controller-manager&lt;span style=&#34;color:#666&#34;&gt;-0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;violations:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gatekeeper-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-public&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;data-replication&#34;&gt;Data Replication&lt;/h3&gt;

&lt;p&gt;Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.&lt;/p&gt;

&lt;p&gt;To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config.gatekeeper.sh/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;gatekeeper-system&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;sync:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;syncOnly:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;group:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;version:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;group:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;version:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Pod&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;planned-for-future&#34;&gt;Planned for Future&lt;/h2&gt;

&lt;p&gt;The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.&lt;/p&gt;

&lt;p&gt;If you are interested in learning more about the project, check out the &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; repo. If you are interested in helping define the direction of Gatekeeper, join the &lt;a href=&#34;https://openpolicyagent.slack.com/messages/CDTN970AX&#34; target=&#34;_blank&#34;&gt;#kubernetes-policy&lt;/a&gt; channel on OPA Slack, and join our &lt;a href=&#34;https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit&#34; target=&#34;_blank&#34;&gt;weekly meetings&lt;/a&gt; to discuss development, issues, use cases, etc.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Get started with Kubernetes (using Python)</title>
      <link>https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jason Haley (Independent Consultant)&lt;/p&gt;

&lt;p&gt;So, you know you want to run your application in Kubernetes but don’t know where to start. Or maybe you’re getting started but still don’t know what you don’t know. In this blog you’ll walk through how to containerize an application and get it running in Kubernetes.&lt;/p&gt;

&lt;p&gt;This walk-through assumes you are a developer or at least comfortable with the command line (preferably bash shell).&lt;/p&gt;

&lt;h2 id=&#34;what-we-ll-do&#34;&gt;What we’ll do&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Get the code and run the application locally&lt;/li&gt;
&lt;li&gt;Create an image and run the application in Docker&lt;/li&gt;
&lt;li&gt;Create a deployment and run the application in Kubernetes&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A Kubernetes service - I&amp;rsquo;m using &lt;a href=&#34;https://www.docker.com/products/kubernetes&#34; target=&#34;_blank&#34;&gt;Docker Desktop with Kubernetes&lt;/a&gt; in this walkthrough, but you can use one of the others. See &lt;a href=&#34;https://kubernetes.io/docs/setup/&#34; target=&#34;_blank&#34;&gt;Getting Started&lt;/a&gt; for a full listing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34; target=&#34;_blank&#34;&gt;Python 3.7&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34; target=&#34;_blank&#34;&gt;Git&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;containerizing-an-application&#34;&gt;Containerizing an application&lt;/h2&gt;

&lt;p&gt;In this section you’ll take some source code, verify it runs locally, and then create a Docker image of the application. The sample application used is a very simple Flask web application; if you want to test it locally, you’ll need Python installed. Otherwise, you can skip to the &amp;ldquo;Create a Dockerfile&amp;rdquo; section.&lt;/p&gt;

&lt;h3 id=&#34;get-the-application-code&#34;&gt;Get the application code&lt;/h3&gt;

&lt;p&gt;Use git to clone the repository to your local machine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/JasonHaley/hello-python.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change to the app directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd hello-python/app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are only two files in this directory. If you look at the main.py file, you’ll see the application prints out a hello message. You can learn more about Flask on the &lt;a href=&#34;http://flask.pocoo.org/&#34; target=&#34;_blank&#34;&gt;Flask website&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)

@app.route(&amp;quot;/&amp;quot;)
def hello():
    return &amp;quot;Hello from Python!&amp;quot;

if __name__ == &amp;quot;__main__&amp;quot;:
    app.run(host=&#39;0.0.0.0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The requirements.txt file contains the list of packages needed by the main.py and will be used by &lt;a href=&#34;https://pip.pypa.io/en/stable/&#34; target=&#34;_blank&#34;&gt;pip&lt;/a&gt; to install the Flask library.&lt;/p&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; When you start writing more advanced Python, you&amp;rsquo;ll find it&amp;rsquo;s not always recommended to use &lt;code&gt;pip install&lt;/code&gt; and may want to use &lt;code&gt;virtualenv&lt;/code&gt; (or &lt;code&gt;pyenv&lt;/code&gt;) to install your dependencies in a virtual environment.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;run-locally&#34;&gt;Run locally&lt;/h3&gt;

&lt;p&gt;Manually run the installer and application using the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
python main.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will start a development web server hosting your application, which you will be able to see by navigating to &lt;a href=&#34;http://localhost:5000&#34; target=&#34;_blank&#34;&gt;http://localhost:5000&lt;/a&gt;. Because port 5000 is the default port for the development server, we didn’t need to specify it.&lt;/p&gt;

&lt;h3 id=&#34;create-a-dockerfile&#34;&gt;Create a Dockerfile&lt;/h3&gt;

&lt;p&gt;Now that you have verified the source code works, the first step in containerizing the application is to create a Dockerfile.&lt;/p&gt;

&lt;p&gt;In the hello-python/app directory, create a file named Dockerfile with the following contents and save it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM python:3.7

RUN mkdir /app
WORKDIR /app
ADD . /app/
RUN pip install -r requirements.txt

EXPOSE 5000
CMD [&amp;quot;python&amp;quot;, &amp;quot;/app/main.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This file is a set of instructions Docker will use to build the image. For this simple application, Docker is going to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get the official &lt;a href=&#34;https://hub.docker.com/_/python/&#34; target=&#34;_blank&#34;&gt;Python Base Image&lt;/a&gt; for version 3.7 from Docker Hub.&lt;/li&gt;
&lt;li&gt;In the image, create a directory named app.&lt;/li&gt;
&lt;li&gt;Set the working directory to that new app directory.&lt;/li&gt;
&lt;li&gt;Copy the local directory’s contents to that new folder into the image.&lt;/li&gt;
&lt;li&gt;Run the pip installer (just like we did earlier) to pull the requirements into the image.&lt;/li&gt;
&lt;li&gt;Inform Docker the container listens on port 5000.&lt;/li&gt;
&lt;li&gt;Configure the starting command to use when the container starts.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;create-an-image&#34;&gt;Create an image&lt;/h3&gt;

&lt;p&gt;At your command line or shell, in the hello-python/app directory, build the image with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build -f Dockerfile -t hello-python:latest .
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&amp;rsquo;m using the :latest tag in this example, if you are not familiar with what it is you may want to read &lt;a href=&#34;https://container-solutions.com/docker-latest-confusion/&#34; target=&#34;_blank&#34;&gt;Docker: The latest Confusion&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will perform those seven steps listed above and create the image. To verify the image was created, run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker image ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/docker-image-ls.png&#34; alt=&#34;Docker image listing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The application is now containerized, which means it can now run in Docker and Kubernetes!&lt;/p&gt;

&lt;h2 id=&#34;running-in-docker&#34;&gt;Running in Docker&lt;/h2&gt;

&lt;p&gt;Before jumping into Kubernetes, let’s verify it works in Docker.
Run the following command to have Docker run the application in a container and map it to port 5001:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 5001:5000 hello-python
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now navigate to &lt;a href=&#34;http://localhost:5001&#34; target=&#34;_blank&#34;&gt;http://localhost:5001&lt;/a&gt;, and you should see the “Hello form Python!” message.&lt;/p&gt;

&lt;h3 id=&#34;more-info&#34;&gt;More info&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-started/&#34; target=&#34;_blank&#34;&gt;Get started with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34;&gt;Best practices for writing Dockerfiles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.docker.com/sites/default/files/Docker_CheatSheet_08.09.2016_0.pdf&#34; target=&#34;_blank&#34;&gt;Docker Cheat Sheet&lt;/a&gt; (pdf)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;running-in-kubernetes&#34;&gt;Running in Kubernetes&lt;/h2&gt;

&lt;p&gt;You are finally ready to get the application running in Kubernetes. Because you have a web application, you will create a service and a deployment.&lt;/p&gt;

&lt;p&gt;First verify your kubectl is configured. At the command line, type the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don’t see a reply with a Client and Server version, you’ll need to &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;install&lt;/a&gt; and configure it.&lt;/p&gt;

&lt;p&gt;If you are running on Windows or Mac, make sure it is using the Docker for Desktop context by running the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context docker-for-desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you are working with Kubernetes! You can see the node by typing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s have it run the application. Create a file named deployment.yaml and add the following contents to it and then save it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python-service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;protocol:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;TCP&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;port:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;6000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;targetPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;LoadBalancer&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apps/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Deployment&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;replicas:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;containerPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This YAML file is the instructions to Kubernetes for what you want running. It is telling Kubernetes the following:
* You want a load-balanced service exposing port 6000
* You want four instances of the hello-python container running&lt;/p&gt;

&lt;p&gt;Use kubectl to send the YAML file to Kubernetes by running the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see the pods are running if you execute the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/kubectl-get-pods.png&#34; alt=&#34;Pod listing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now navigate to &lt;a href=&#34;http://localhost:6000&#34; target=&#34;_blank&#34;&gt;http://localhost:6000&lt;/a&gt;, and you should see the “Hello form Python!” message.&lt;/p&gt;

&lt;p&gt;That’s it! The application is now running in Kubernetes!&lt;/p&gt;

&lt;h3 id=&#34;more-info-1&#34;&gt;More Info&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/topic/kubernetes/&#34; target=&#34;_blank&#34;&gt;Learn Kubernetes Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/cheatsheet/&#34; target=&#34;_blank&#34;&gt;kubectl Cheat Sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/&#34; target=&#34;_blank&#34;&gt;kubectl for Docker Users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this walk-through, we containerized an application, and got it running in Docker and in Kubernetes. This simple application only scratches the surface of what’s possible (and what you’ll need to learn).&lt;/p&gt;

&lt;h3 id=&#34;next-steps&#34;&gt;Next steps&lt;/h3&gt;

&lt;p&gt;If you are just getting started and this walk-through was useful to you, then the following resources should be good next steps for you to further expand your Kubernetes knowledge:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=1xo-0gCVhTU&#34; target=&#34;_blank&#34;&gt;Introduction to Microservices, Docker, and Kubernetes&lt;/a&gt; - 55-minute video by James Quigley

&lt;ul&gt;
&lt;li&gt;This is a great place to start because it provides more information than I could here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Containerize-your-Apps-with-Docker-and-Kubernetes&#34; target=&#34;_blank&#34;&gt;Containerize your Apps with Docker and Kubernetes&lt;/a&gt; - free e-book by Dr Gabriel N Schenker

&lt;ul&gt;
&lt;li&gt;This is my favorite book on Docker and Kubernetes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aka.ms/LearnKubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes Learning Path: 50 days from zero to hero with Kubernetes&lt;/a&gt; - on Microsoft’s site

&lt;ul&gt;
&lt;li&gt;This is a 10-page pdf that has tons of links to videos (with Brendan Burns), documentation sites, and a really good workshop for Azure Kubernetes Service.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-enable-kubernetes-in-docker-desktop&#34;&gt;How to enable Kubernetes in Docker Desktop&lt;/h2&gt;

&lt;p&gt;Once you have Docker Desktop installed, open the Settings:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/docker-settings-menu.png&#34; alt=&#34;Docker settings menu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Select the &lt;strong&gt;Kubernetes&lt;/strong&gt; menu item on the left and verify that the &lt;strong&gt;Enable Kubernetes&lt;/strong&gt; is checked. If it isn’t, &lt;strong&gt;check it&lt;/strong&gt; and click the &lt;strong&gt;Apply&lt;/strong&gt; button at the bottom right:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/kubernetes-tab.png&#34; alt=&#34;Kubernetes tab&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Deprecated APIs Removed In 1.16: Here’s What You Need To Know</title>
      <link>https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Vallery Lancey (Lyft)&lt;/p&gt;

&lt;p&gt;As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old API is deprecated and eventually removed.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;v1.16&lt;/strong&gt; release will stop serving the following deprecated API versions in favor of newer and more stable API versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NetworkPolicy (in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API group)

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;networking.k8s.io/v1&lt;/strong&gt; API, available since v1.8.
Existing persisted data can be retrieved/updated via the &lt;strong&gt;networking.k8s.io/v1&lt;/strong&gt; API.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PodSecurityPolicy (in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API group)

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;policy/v1beta1&lt;/strong&gt; API, available since v1.10.
Existing persisted data can be retrieved/updated via the &lt;strong&gt;policy/v1beta1&lt;/strong&gt; API.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DaemonSet, Deployment, StatefulSet, and ReplicaSet (in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; and &lt;strong&gt;apps/v1beta2&lt;/strong&gt; API groups)

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;apps/v1&lt;/strong&gt; API, available since v1.9.
Existing persisted data can be retrieved/updated via the &lt;strong&gt;apps/v1&lt;/strong&gt; API.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;v1.20&lt;/strong&gt; release will stop serving the following deprecated API versions in favor of newer and more stable API versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ingress (in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API group)

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;networking.k8s.io/v1beta1&lt;/strong&gt; API, serving Ingress since v1.14.
Existing persisted data can be retrieved/updated via the &lt;strong&gt;networking.k8s.io/v1beta1&lt;/strong&gt; API.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;what-to-do&#34;&gt;What To Do&lt;/h1&gt;

&lt;p&gt;Kubernetes 1.16 is due to be released in September 2019, so be sure to audit
your configuration and integrations now!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Change YAML files to reference the newer APIs&lt;/li&gt;
&lt;li&gt;Update custom integrations and controllers to call the newer APIs&lt;/li&gt;
&lt;li&gt;Update third party tools (ingress controllers, continuous delivery systems)
to call the newer APIs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Migrating to the new Ingress API will only require changing the API path - the
API fields remain the same. However, migrating other resources (EG Deployments)
will require some updates based on changed fields. You can use the
&lt;code&gt;kubectl convert&lt;/code&gt; command to automatically convert an existing object:
&lt;code&gt;kubectl convert -f &amp;lt;file&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, to convert
an older Deployment to apps/v1, you can run:
&lt;code&gt;kubectl convert -f ./my-deployment.yaml --output-version apps/v1&lt;/code&gt;
Note that this may use non-ideal default values. To learn more about a specific
resource, check the Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/reference/#api-reference&#34; target=&#34;_blank&#34;&gt;api reference&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can test your clusters by starting an apiserver with the above resources
disabled, to simulate the upcoming removal. Add the following flag to the
apiserver startup arguments:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;want-to-know-more&#34;&gt;Want To Know More?&lt;/h1&gt;

&lt;p&gt;Deprecations are announced in the Kubernetes release notes. You can see these
announcements in
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.14.md#deprecations&#34; target=&#34;_blank&#34;&gt;1.14&lt;/a&gt;
and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#deprecations-and-removals&#34; target=&#34;_blank&#34;&gt;1.15&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can read more &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api&#34; target=&#34;_blank&#34;&gt;in our deprecation policy document&lt;/a&gt;
about the deprecation policies for Kubernetes APIs, and other Kubernetes components.
Deprecation policies vary by component (for example, the primary APIs vs.
admin CLIs) and by maturity (alpha, beta, or GA).&lt;/p&gt;

&lt;p&gt;These details were also &lt;a href=&#34;https://groups.google.com/forum/#!topic/kubernetes-dev/je0rjyfTVyc&#34; target=&#34;_blank&#34;&gt;previously announced&lt;/a&gt;
on the kubernetes-dev mailing list, along with the releases of Kubernetes 1.14
and 1.15. From Jordan Liggitt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In case you missed it in the 1.15.0 release notes, the timelines for deprecated resources in the extensions/v1beta1, apps/v1beta1, and apps/v1beta2 API groups to no longer be served by default have been updated:

* NetworkPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the networking.k8s.io/v1 API, available since v1.8. Existing persisted data can be retrieved/updated via the networking.k8s.io/v1 API.
* PodSecurityPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the policy/v1beta1 API, available since v1.10. Existing persisted data can be retrieved/updated via the policy/v1beta1 API.
* DaemonSet, Deployment, StatefulSet, and ReplicaSet resources will no longer be served from extensions/v1beta1, apps/v1beta1, or apps/v1beta2 by default in v1.16. Migrate to the apps/v1 API, available since v1.9. Existing persisted data can be retrieved/updated via the apps/v1 API.

To start a v1.15.0 API server with these resources disabled to flush out dependencies on these deprecated APIs, and ensure your application/manifests will work properly against the v1.16 release, use the following --runtime-config argument:

--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Recap of Kubernetes Contributor Summit Barcelona 2019</title>
      <link>https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/</link>
      <pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jonas Rosland (VMware)&lt;/p&gt;

&lt;p&gt;First of all, &lt;strong&gt;THANK YOU&lt;/strong&gt; to everyone who made the Kubernetes Contributor Summit in Barcelona possible. We had an amazing team of volunteers tasked with planning and executing the event, and it was so much fun meeting and talking to all new and current contributors during the main event and the pre-event celebration.&lt;/p&gt;

&lt;p&gt;Contributor Summit in Barcelona kicked off KubeCon + CloudNativeCon in a big way as it was the &lt;strong&gt;largest contributor summit&lt;/strong&gt; to date with 331 people signed up, and only 9 didn&amp;rsquo;t pick up their badges!&lt;/p&gt;

&lt;h2 id=&#34;contributor-celebration&#34;&gt;Contributor Celebration&lt;/h2&gt;

&lt;p&gt;Sunday evening before the main event we held a &lt;strong&gt;Contributor Celebration&lt;/strong&gt;, which was very well attended. We hope that all new and current contributors felt welcome and enjoyed the food, the music, and the company.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/46981485515_561bb324b2_z.jpg&#34; alt=&#34;contributor-celebration2&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/46981484655_8122564557_z.jpg&#34; alt=&#34;contributor-celebration&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;new-contributor-workshops&#34;&gt;New Contributor Workshops&lt;/h2&gt;

&lt;p&gt;We had over &lt;strong&gt;130 people registered&lt;/strong&gt; for the New Contributor Workshops. This year the workshops were divided into &lt;em&gt;101-level content&lt;/em&gt; for people who were not familiar with contributing to an open source project, and &lt;em&gt;201-level content&lt;/em&gt; for those who were.&lt;/p&gt;

&lt;p&gt;The workshops contained overviews of what SIGs are, deep-dives into the codebase, test builds of the Kubernetes project, and real contributions.&lt;/p&gt;

&lt;p&gt;Did you miss something during the workshops? We now have them &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP2WTJ6P8sQenhf0RY-JqF5L&#34; target=&#34;_blank&#34;&gt;published on YouTube&lt;/a&gt;, with added closed captioning!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47897543541_a57d3b9ac9_z.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47108247174_5d60b60846_z.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sig-face-to-face&#34;&gt;SIG Face-to-Face&lt;/h2&gt;

&lt;p&gt;We also tried a new thing for Barcelona, the SIG Face-to-Face meetings. We had &lt;strong&gt;over 170 people&lt;/strong&gt; registered to attend the 11 SIG and one subproject meetings throughout the day, going over what they&amp;rsquo;re working on and what they want to do in the near future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47108254124_248a80ef1a_z.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47108250434_88afdf930a_z.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sig-meet-and-greet&#34;&gt;SIG Meet and Greet&lt;/h2&gt;

&lt;p&gt;At the end of the summit, both new and current contributors had a chance to sit down with SIG chairs and members. The goal of this was to make sure that contributors got to know even more individuals in the project, hear what some of the SIGs actually do, and sign up to be a part of them and learn more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47108248464_97eb2bbbb6_k.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47845452032_a3d478beb9_k.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;join-us&#34;&gt;Join us!&lt;/h2&gt;

&lt;p&gt;Interested in attending the Contributor Summit in San Diego? &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;You can get more information on our event page&lt;/a&gt;, sign up and we will notify you when registration opens.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks!&lt;/h2&gt;

&lt;p&gt;Again, thank you to everyone for making this an amazing event, and we&amp;rsquo;re looking forward to seeing you next time!&lt;/p&gt;

&lt;p&gt;To our Barcelona crew, you ROCK! 🥁&lt;/p&gt;

&lt;p&gt;Paris Pittman, Bob Killen, Guinevere Saenger, Tim Pepper, Deb Giles, Ihor Dvoretskyi, Jorge Castro, Noah Kantrowitz, Dawn Foster, Ruben Orduz, Josh Berkus, Kiran Mova, Bart Smykla, Rostislav Georgiev, Jeffrey Sica, Rael Garcia, Silvia Moura Pina, Arnaud Meukam, Jason DeTiberius, Andy Goldstein, Suzanne Ambiel, Jonas Rosland&lt;/p&gt;

&lt;p&gt;You can see many more pictures from the event &lt;a href=&#34;https://www.flickr.com/photos/143247548@N03/sets/72157680323974628&#34; target=&#34;_blank&#34;&gt;over on CNCF&amp;rsquo;s Flickr&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Automated High Availability in kubeadm v1.15: Batteries Included But Swappable</title>
      <link>https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lucas Käldström, &lt;a href=&#34;https://github.com/luxas&#34; target=&#34;_blank&#34;&gt;@luxas&lt;/a&gt;, SIG Cluster Lifecycle co-chair &amp;amp; kubeadm subproject owner, Weaveworks&lt;/li&gt;
&lt;li&gt;Fabrizio Pandini, &lt;a href=&#34;https://github.com/fabriziopandini&#34; target=&#34;_blank&#34;&gt;@fabriziopandini&lt;/a&gt;, kubeadm subproject owner, Independent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt; is a tool that enables Kubernetes administrators
to quickly and easily bootstrap minimum viable clusters that are fully compliant with
&lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes&lt;/a&gt; guidelines.
It’s been under active development by &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle&lt;/a&gt;
since 2016 and graduated it from beta to
&lt;a href=&#34;https://kubernetes.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/&#34; target=&#34;_blank&#34;&gt;generally available (GA) at the end of 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After this important milestone, the kubeadm team is now focused on the stability of the core feature set and working on
maturing existing features.&lt;/p&gt;

&lt;p&gt;With this post, we are introducing the improvements made in the v1.15 release of kubeadm.&lt;/p&gt;

&lt;h2 id=&#34;the-scope-of-kubeadm&#34;&gt;The scope of kubeadm&lt;/h2&gt;

&lt;p&gt;kubeadm is focused on performing the actions necessary to get a minimum viable, secure cluster up and running in a
user-friendly way. kubeadm&amp;rsquo;s scope is limited to the local machine’s filesystem and the Kubernetes API, and it is
intended to be a &lt;em&gt;composable building block for higher-level tools&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The core of the kubeadm interface is quite simple: new control plane nodes are created by you running
&lt;strong&gt;&lt;code&gt;kubeadm init&lt;/code&gt;&lt;/strong&gt;, worker nodes are joined to the control plane by you running
&lt;strong&gt;&lt;code&gt;kubeadm join&lt;/code&gt;&lt;/strong&gt;. Also included are common utilities for managing already bootstrapped
clusters, such as control plane upgrades, token and certificate renewal.&lt;/p&gt;

&lt;p&gt;To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of scope:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Infrastructure provisioning&lt;/li&gt;
&lt;li&gt;Third-party networking&lt;/li&gt;
&lt;li&gt;Non-critical add-ons, e.g. monitoring, logging, and visualization&lt;/li&gt;
&lt;li&gt;Specific cloud provider integrations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those tasks are addressed by other SIG Cluster Lifecycle projects, such as the
&lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API&lt;/a&gt; for infrastructure provisioning and management.&lt;/p&gt;

&lt;p&gt;Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/#kubernetes-control-plane&#34; target=&#34;_blank&#34;&gt;control plane&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/overview.png&#34; alt=&#34;Cluster Lifecycle Layers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-in-kubeadm-v1-15&#34;&gt;What’s new in kubeadm v1.15?&lt;/h2&gt;

&lt;h3 id=&#34;high-availability-to-beta&#34;&gt;High Availability to Beta&lt;/h3&gt;

&lt;p&gt;We are delighted to announce that automated support for High Availability clusters is graduating to &lt;strong&gt;Beta&lt;/strong&gt; in kubeadm v1.15. Let’s give a great shout out to all the contributors that helped in this effort and to the early adopter users for the great feedback received so far!&lt;/p&gt;

&lt;p&gt;But how does automated High Availability work in kubeadm?&lt;/p&gt;

&lt;p&gt;The great news is that you can use the familiar &lt;code&gt;kubeadm init&lt;/code&gt; or &lt;code&gt;kubeadm join&lt;/code&gt; workflow for creating high availability cluster as well, with the only difference that you have to pass the &lt;code&gt;--control-plane&lt;/code&gt; flag to &lt;code&gt;kubeadm join&lt;/code&gt; when adding more control plane nodes.&lt;/p&gt;

&lt;p&gt;A 3-minute screencast of this feature is here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/252343&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/252343.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In a nutshell:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Set up a Load Balancer.&lt;/strong&gt; You need an &lt;em&gt;external load balancer&lt;/em&gt;; providing this however, is out of scope of kubeadm.

&lt;ul&gt;
&lt;li&gt;The community will provide a set of reference implementations for this task though&lt;/li&gt;
&lt;li&gt;HAproxy, Envoy, or a similar Load Balancer from a cloud provider work well&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Run kubeadm init&lt;/strong&gt; on the first control plane node, with small modifications:

&lt;ul&gt;
&lt;li&gt;Create a &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file&#34; target=&#34;_blank&#34;&gt;kubeadm Config File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In the config file, set the &lt;code&gt;controlPlaneEndpoint&lt;/code&gt; field to where your Load Balancer can be reached at.&lt;/li&gt;
&lt;li&gt;Run init, with the &lt;code&gt;--upload-certs&lt;/code&gt; flag like this: &lt;code&gt;sudo kubeadm init --config=kubeadm-config.yaml --upload-certs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Run kubeadm join &amp;ndash;control-plane&lt;/strong&gt; at any time when you want to expand the set of control plane nodes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Both control-plane- and normal nodes can be joined in any order, at any time&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The command to run will be given by &lt;code&gt;kubeadm init&lt;/code&gt; above, and is of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join [LB endpoint] \
--token ... \                                                                                               
--discovery-token-ca-cert-hash sha256:... \                                                             
--control-plane --certificate-key ...  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For those interested in the details, there are many things that make this functionality possible. Most notably:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Automated certificate transfer&lt;/strong&gt;. kubeadm implements an automatic certificate copy feature to automate the distribution of all the certificate authorities/keys that must be shared across all the control-planes nodes in order to get your cluster to work. This feature can be activated by passing  &lt;code&gt;--upload-certs&lt;/code&gt; to  &lt;code&gt;kubeadm init&lt;/code&gt;; see &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/&#34; target=&#34;_blank&#34;&gt;configure and deploy an HA control plane&lt;/a&gt; for more details. This is an explicit opt-in feature, you can also distribute the certificates manually in your preferred way. &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dynamically-growing etcd cluster&lt;/strong&gt;. When you&amp;rsquo;re not providing an external etcd cluster, kubeadm automatically adds a new etcd member, running as a static pod. All the etcd members are joined in a “stacked” etcd cluster that grows together with your high availability control-plane &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Concurrent joining&lt;/strong&gt;. Similarly to what already implemented for worker nodes, you join control-plane nodes whenever, in any order, or even in parallel. &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upgradable&lt;/strong&gt;. The kubeadm upgrade workflow was improved in order to properly handle the HA scenario, and, after starting the upgrade with &lt;code&gt;kubeadm upgrade apply&lt;/code&gt; as usual, users can now complete the upgrade process by using &lt;code&gt;kubeadm upgrade node&lt;/code&gt; both on the remaining control-plane nodes and worker nodes&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, it is also worthy to notice that an entirely new test suite has been created specifically for ensuring High Availability in kubeadm will stay stable over time.&lt;/p&gt;

&lt;h3 id=&#34;certificate-management&#34;&gt;Certificate Management&lt;/h3&gt;

&lt;p&gt;Certificate management has become more simple and robust in kubeadm v1.15.&lt;/p&gt;

&lt;p&gt;If you perform Kubernetes version upgrades regularly, kubeadm will now take care of keeping your cluster up to date and reasonably secure by &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal&#34; target=&#34;_blank&#34;&gt;automatically rotating all your certificates&lt;/a&gt; at &lt;code&gt;kubeadm upgrade&lt;/code&gt; time.&lt;/p&gt;

&lt;p&gt;If instead, you prefer to renew your certificates manually, you can opt out from the automatic certificate renewal by passing &lt;code&gt;--certificate-renewal=false&lt;/code&gt; to &lt;code&gt;kubeadm upgrade&lt;/code&gt; commands. Then you can perform &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal&#34; target=&#34;_blank&#34;&gt;manual certificate renewal&lt;/a&gt; with the &lt;code&gt;kubeadm alpha certs renew&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;But there is more.&lt;/p&gt;

&lt;p&gt;A new command &lt;code&gt;kubeadm alpha certs check-expiration&lt;/code&gt; was introduced to allow users to
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#check-certificate-expiration&#34; target=&#34;_blank&#34;&gt;check certificate expiration&lt;/a&gt;. The output is similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 May 15, 2020 13:03 UTC   364d            false
apiserver                  May 15, 2020 13:00 UTC   364d            false
apiserver-etcd-client      May 15, 2020 13:00 UTC   364d            false
apiserver-kubelet-client   May 15, 2020 13:00 UTC   364d            false
controller-manager.conf    May 15, 2020 13:03 UTC   364d            false
etcd-healthcheck-client    May 15, 2020 13:00 UTC   364d            false
etcd-peer                  May 15, 2020 13:00 UTC   364d            false
etcd-server                May 15, 2020 13:00 UTC   364d            false
front-proxy-client         May 15, 2020 13:00 UTC   364d            false
scheduler.conf             May 15, 2020 13:03 UTC   364d            false&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should expect also more work around certificate management in kubeadm in the next releases, with the introduction of ECDSA keys and with improved support for CA key rotation. Additionally, the commands staged under &lt;code&gt;kubeadm alpha&lt;/code&gt; are expected to move top-level soon.&lt;/p&gt;

&lt;h3 id=&#34;improved-configuration-file-format&#34;&gt;Improved Configuration File Format&lt;/h3&gt;

&lt;p&gt;You can argue that there are hardly two Kubernetes clusters that are configured equally, and hence there is a need to customize how the cluster is set up depending on the environment. One way of configuring a component is via flags. However, this has some scalability limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hard to maintain.&lt;/strong&gt; When a component’s flag set grows over 30+ flags, configuring it becomes really painful.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complex upgrades&lt;/strong&gt;. When flags are removed, deprecated or changed, you need to upgrade of the binary at the same time as the arguments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key-value limited&lt;/strong&gt;. There are simply many types of configuration you can’t express with the  &lt;code&gt;--key=value&lt;/code&gt; syntax.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Imperative&lt;/strong&gt;. In contrast to Kubernetes API objects themselves that are declaratively specified, flag arguments are imperative by design.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a key problem for Kubernetes components in general, as some components have 150+ flags. With kubeadm we’re pioneering the ComponentConfig effort, and providing users with a small set of flags, but most importantly, a &lt;strong&gt;declarative and versioned configuration file&lt;/strong&gt; for advance use-cases. We call this &lt;em&gt;ComponentConfig&lt;/em&gt;. It has the following characteristics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Upgradable&lt;/strong&gt;: You can upgrade the binary, and still use the existing, older schema. Automatic migrations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Programmable&lt;/strong&gt;. Configuration expressed in JSON/YAML allows for consistent, and programmable manipulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expressible&lt;/strong&gt;. Advanced patterns of configuration can be used and applied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Declarative&lt;/strong&gt;. OpenAPI information can easily be exposed / used for doc generation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In kubeadm v1.15, we have improved the structure and are releasing the new &lt;strong&gt;v1beta2&lt;/strong&gt; format. Important to note that the existing &lt;strong&gt;v1beta1&lt;/strong&gt; format released in v1.13 will still continue to work for several releases. This means you can upgrade kubeadm to v1.15, and still use your existing v1beta1 configuration files. When you’re ready to take advantage of the improvements made in v1beta2, you can perform an automatic schema migration using the &lt;code&gt;kubeadm config migrate&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;During the course of the year, we’re looking forward to graduate the schema to General Availability &lt;code&gt;v1&lt;/code&gt;.` If you’re interested in this effort, you can also join &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-component-standard&#34; target=&#34;_blank&#34;&gt;WG Component Standard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&#34;2019-plans&#34;&gt;2019 plans&lt;/h3&gt;

&lt;p&gt;We are focusing our efforts around graduating the configuration file format to GA (&lt;code&gt;kubeadm.k8s.io/v1&lt;/code&gt;)`, graduating this super-easy High Availability flow to stable, and providing better tools around rotating certificates needed for running the cluster automatically.&lt;/p&gt;

&lt;p&gt;In addition to these three key milestones of our charter, we want to improve the following areas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support joining Windows nodes to a kubeadm cluster (with end-to-end tests)&lt;/li&gt;
&lt;li&gt;Improve the upstream CI signal, mainly for HA and upgrades&lt;/li&gt;
&lt;li&gt;Consolidate how Kubernetes artifacts are built and installed&lt;/li&gt;
&lt;li&gt;Utilize Kustomize to allow for advanced, layered and declarative configuration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We make no guarantees that these deliverables will ship this year though, as this is a community effort. If you want to see these things happen, please join our SIG and start contributing! The ComponentConfig issues in particular need more attention.&lt;/p&gt;

&lt;h3 id=&#34;kubeadm-now-has-a-logo&#34;&gt;kubeadm now has a logo!&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dankohn&#34; target=&#34;_blank&#34;&gt;Dan Kohn&lt;/a&gt; offered CNCF’s help with creating a logo for kubeadm in this cycle.
&lt;a href=&#34;https://github.com/alexcontini&#34; target=&#34;_blank&#34;&gt;Alex Contini&lt;/a&gt; created 19 (!) different logo options for the community to vote on. The public poll
was active for around a week, and we got 386 answers. The winning option got 17.4% of the votes. In other words, now we have an
official logo!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/logo.png&#34; alt=&#34;kubeadm&#39;s logo&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;contributing&#34;&gt;Contributing&lt;/h2&gt;

&lt;p&gt;If this all sounds exciting, &lt;strong&gt;join us&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle&lt;/a&gt; has many different
subprojects, where kubeadm is one of them. In the following picture you can see that there are many pieces in the
puzzle, and we have a lot still to do.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/projects.png&#34; alt=&#34;SIG Cluster Lifecycle Projects&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Some handy links if you want to start contribute:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can watch the SIG Cluster Lifecycle &lt;a href=&#34;https://www.youtube.com/watch?v=Bof9aveB3rA&#34; target=&#34;_blank&#34;&gt;New Contributor Onboarding&lt;/a&gt; session on YouTube.&lt;/li&gt;
&lt;li&gt;Look out for “good first issue”, “help wanted” and “sig/cluster-lifecycle” labeled issues in our repositories
(e.g. &lt;a href=&#34;https://github.com/kubernetes/kubeadm&#34; target=&#34;_blank&#34;&gt;kubernetes/kubeadm&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Join &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;#sig-cluster-lifecycle&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/kubeadm&#34; target=&#34;_blank&#34;&gt;#kubeadm&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/cluster-api&#34; target=&#34;_blank&#34;&gt;#cluster-api&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/minikube&#34; target=&#34;_blank&#34;&gt;#minikube&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/kind&#34; target=&#34;_blank&#34;&gt;#kind&lt;/a&gt;, etc. in Slack&lt;/li&gt;
&lt;li&gt;Join our public, bi-weekly SIG Cluster Lifecycle Zoom meeting at Tuesdays 9am PT

&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://docs.google.com/document/d/1Gmc7LyCIL_148a9Tft7pdhdee0NBHdOfHS1SAF0duI4/edit&#34; target=&#34;_blank&#34;&gt;Meeting Notes&lt;/a&gt; to join&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Join our public, weekly kubeadm Office Hours Zoom meeting at Wednesdays 9am PT

&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://docs.google.com/document/d/130_kiXjG7graFNSnIAgtMS1G8zPDwpkshgfRYS0nggo/edit&#34; target=&#34;_blank&#34;&gt;Meeting Notes&lt;/a&gt; to join&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://youtu.be/bA2M41J4wvg&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle Intro&lt;/a&gt; or the
&lt;a href=&#34;https://youtu.be/spXSSIbZTqM&#34; target=&#34;_blank&#34;&gt;kubeadm Deep Dive&lt;/a&gt; sessions from KubeCon Barcelona&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;thank-you&#34;&gt;Thank You&lt;/h3&gt;

&lt;p&gt;This release wouldn’t have been possible without the help of the great people that have been contributing to SIG Cluster Lifecycle
and kubeadm. We would like to thank all the kubeadm contributors and companies making it possible for their developers to work
on Kubernetes!&lt;/p&gt;

&lt;p&gt;In particular, we would like to thank the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/OWNERS&#34; target=&#34;_blank&#34;&gt;kubeadm subproject owners&lt;/a&gt; that made this possible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tim St. Clair , &lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;, SIG Cluster Lifecycle co-chair, VMware&lt;/li&gt;
&lt;li&gt;Lucas Käldström, &lt;a href=&#34;https://github.com/luxas&#34; target=&#34;_blank&#34;&gt;@luxas&lt;/a&gt;, SIG Cluster Lifecycle co-chair, Weaveworks&lt;/li&gt;
&lt;li&gt;Fabrizio Pandini, &lt;a href=&#34;https://github.com/fabriziopandini&#34; target=&#34;_blank&#34;&gt;@fabriziopandini&lt;/a&gt;, Independent&lt;/li&gt;
&lt;li&gt;Lubomir I. Ivanov, &lt;a href=&#34;https://github.com/neolit123&#34; target=&#34;_blank&#34;&gt;@neolit123&lt;/a&gt;, VMware&lt;/li&gt;
&lt;li&gt;Rostislav M. Georgiev, &lt;a href=&#34;https://github.com/rosti&#34; target=&#34;_blank&#34;&gt;@rosti&lt;/a&gt;, VMware&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Volume Cloning Alpha for Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/06/21/introducing-volume-cloning-alpha-for-kubernetes/</link>
      <pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/21/introducing-volume-cloning-alpha-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: John Griffith (Red Hat)&lt;/p&gt;

&lt;p&gt;Kubernetes v1.15 introduces alpha support for volume cloning. This feature allows you to create new volumes using the contents of existing volumes in the user&amp;rsquo;s namespace using the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-clone&#34;&gt;What is a Clone?&lt;/h2&gt;

&lt;p&gt;Many storage systems provide the ability to create a &amp;ldquo;clone&amp;rdquo; of a volume.  A clone is a duplicate of an existing volume that is its own unique volume on the system, but the data on the source is duplicated to the destination (clone).  A clone is similar to a snapshot in that it&amp;rsquo;s a point in time copy of a volume, however rather than creating a new snapshot object from a volume, we&amp;rsquo;re instead creating a new independent volume, sometimes thought of as pre-populating the newly created volume.&lt;/p&gt;

&lt;h2 id=&#34;why-add-cloning-to-kubernetes&#34;&gt;Why add cloning to Kubernetes&lt;/h2&gt;

&lt;p&gt;The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p&gt;

&lt;p&gt;Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no specific storage device knowledge.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage SIG&lt;/a&gt; identified clone operations as critical functionality for many stateful workloads. For example, a database administrator may want to duplicate a database volume and create another instance of an existing database.&lt;/p&gt;

&lt;p&gt;By providing a standard way to trigger clone operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).  While cloning is similar in behavior to creating a snapshot of a volume, then creating a volume from the snapshot, a clone operation is more streamlined and is more efficient for many backend devices.&lt;/p&gt;

&lt;p&gt;Kubernetes users are now empowered to incorporate clone operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-api-and-cloning&#34;&gt;Kubernetes API and Cloning&lt;/h2&gt;

&lt;p&gt;The cloning feature in Kubernetes is enabled via the &lt;code&gt;PersistentVolumeClaim.DataSource&lt;/code&gt; field.  Prior to v1.15 the only valid object type permitted for use as a dataSource was a &lt;code&gt;VolumeSnapshot&lt;/code&gt;.  The cloning feature extends the allowed &lt;code&gt;PersistentVolumeclaim.DataSource.Kind&lt;/code&gt; field to not only allow &lt;code&gt;VolumeSnapshot&lt;/code&gt; but also &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;.  The existing behavior is not changed.&lt;/p&gt;

&lt;p&gt;There are no new objects introduced to enable cloning. Instead, the existing dataSource field in the PersistentVolumeClaim object is expanded to be able to accept the name of an existing PersistentVolumeClaim in the same namespace.  It is important to note that from a users perspective a clone is just another PersistentVolume and PersistentVolumeClaim, the only difference being that that PersistentVolume is being populated with the contents of another PersistentVolume at creation time.  After creation it behaves exactly like any other Kubernetes PersistentVolume and adheres to the same behaviors and rules.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-kubernetes-cloning&#34;&gt;Which volume plugins support Kubernetes Cloning?&lt;/h2&gt;

&lt;p&gt;Kubernetes supports three types of volume plugins: in-tree, Flex, and &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;Container Storage Interface&lt;/a&gt; (CSI). See &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Volume Plugin FAQ&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;Cloning is only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes cloning feature, ensure that a CSI Driver that implements cloning is deployed on your cluster.
For a list of CSI drivers that currently support cloning see the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;CSI Drivers doc&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-cloning-requirements&#34;&gt;Kubernetes Cloning Requirements&lt;/h2&gt;

&lt;p&gt;Before using Kubernetes Volume Cloning, you must:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensure a CSI driver implementing Cloning is deployed and running on your Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Enable the Kubernetes Volume Cloning feature via new Kubernetes feature gate (disabled by default for alpha):

&lt;ul&gt;
&lt;li&gt;Set the following flag on the API server binary: &lt;code&gt;--feature-gates=VolumePVCDataSource=true&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The source and destination claims must be in the same namespace.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;creating-a-clone-with-kubernetes&#34;&gt;Creating a clone with Kubernetes&lt;/h2&gt;

&lt;p&gt;To provision a new volume pre-populated with data from an existing Kubernetes Volume, use the dataSource field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;.  There are three parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name - name of the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object to use as source&lt;/li&gt;
&lt;li&gt;kind - must be &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;apiGroup - must be &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: pvc-clone
Namespace: demo-namespace
spec:
storageClassName: csi-storageclass
dataSource:
name: src-pvc
kind: PersistentVolumeClaim 
apiGroup: &amp;quot;&amp;quot;
accessModes:
- ReadWriteOnce
resources:
requests:
  storage: 1Gi # NOTE this capacity must be specified and must be &amp;gt;= the capacity of the source volume
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified &lt;code&gt;dataSource&lt;/code&gt; volume.  It is the sole responsbility of the CSI Plugin to implement the cloning of volumes.&lt;/p&gt;

&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-cloning-to-my-csi-driver&#34;&gt;As a storage vendor, how do I add support for cloning to my CSI driver?&lt;/h2&gt;

&lt;p&gt;For more information on how to implement cloning in your CSI Plugin, reference the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/developing.html&#34; target=&#34;_blank&#34;&gt;developing a CSI driver for Kubernetes&lt;/a&gt; section of the CSI docs.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-limitations-of-alpha&#34;&gt;What are the limitations of alpha?&lt;/h2&gt;

&lt;p&gt;The alpha implementation of cloning for Kubernetes has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does not support cloning volumes across different namespaces&lt;/li&gt;
&lt;li&gt;Does not support cloning volumes across different storage classes (backends)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;future&#34;&gt;Future&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI cloning implementation to beta in 1.16.&lt;/p&gt;

&lt;p&gt;A common question that users have regarding cloning is &amp;ldquo;what about cross namespace clones&amp;rdquo;.  As we&amp;rsquo;ve mentioned, the current release requires that source and destination be in the same namespace.  There are however efforts underway to propose a namespace transfer API, future versions of Kubernetes may provide the ability to transfer volume resources from one namespace to another.  This feature is still under discussion and design, and may or may not be available in a future release.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;You can find additional documentation on the cloning feature in the &lt;a href=&#34;https://k8s.io/docs/concepts/storage/volume-pvc-datasource.md&#34; target=&#34;_blank&#34;&gt;storage concept docs&lt;/a&gt; and also the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/volume-cloning.html&#34; target=&#34;_blank&#34;&gt;CSI docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;We offer a huge thank you to all the contributors in Kubernetes Storage SIG and CSI community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saadali&#34; target=&#34;_blank&#34;&gt;saadali&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Future of CRDs: Structural Schemas</title>
      <link>https://kubernetes.io/blog/2019/06/20/crd-structural-schema/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/20/crd-structural-schema/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Stefan Schimanski (Red Hat)&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions were introduced roughly two years ago as the primary way to extend the Kubernetes API with custom resources. From the beginning they stored arbitrary JSON data, with the exception that &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;apiVersion&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt; had to follow the Kubernetes API conventions. In Kubernetes 1.8 CRDs gained the ability to define an optional OpenAPI v3 based validation schema.&lt;/p&gt;

&lt;p&gt;By the nature of OpenAPI specifications though—only describing what must be there, not what shouldn’t, and by being potentially incomplete specifications—the Kubernetes API server never knew the complete structure of CustomResource instances. As a consequence, kube-apiserver—until today—stores all JSON data received in an API request (if it validates against the OpenAPI spec). This especially includes anything that is not specified in the OpenAPI schema.&lt;/p&gt;

&lt;h2 id=&#34;the-story-of-malicious-unspecified-data&#34;&gt;The story of malicious, unspecified data&lt;/h2&gt;

&lt;p&gt;To understand this, we assume a CRD for maintenance jobs by the operations team, running each night as a service user:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;operations/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;MaintenanceNightlyJob&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;&amp;gt;
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;    grep backdoor /etc/passwd || &lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“backdoor:76asdfh76:/bin/bash”&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/passwd&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;||&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“az1-master1”,”az1-master2”,”az2-master3”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The privileged field is not specified by the operations team. Their controller does not know it, and their validating admission webhook does not know about it either. Nevertheless, kube-apiserver persists this suspicious, but unknown field without ever validating it.&lt;/p&gt;

&lt;p&gt;When run in the night, this job never fails, but because the service user is not able to write &lt;code&gt;/etc/passwd&lt;/code&gt;, it will also not cause any harm.&lt;/p&gt;

&lt;p&gt;The maintenance team needs support for privileged jobs. It adds the &lt;code&gt;privileged&lt;/code&gt; support, but is super careful to implement authorization for privileged jobs by only allowing those to be created by very few people in the company. That malicious job though has long been persisted to etcd. The next night arrives and the malicious job is executed.&lt;/p&gt;

&lt;h2 id=&#34;towards-complete-knowledge-of-the-data-structure&#34;&gt;Towards complete knowledge of the data structure&lt;/h2&gt;

&lt;p&gt;This example shows that we cannot trust CustomResource data in etcd. Without having complete knowledge about the JSON structure, the kube-apsierver cannot do anything to prevent persistence of unknown data.&lt;/p&gt;

&lt;p&gt;Kubernetes 1.15 introduces the concept of a (complete) structural OpenAPI schema—an OpenAPI schema with a certain shape, more in a second—which will fill this knowledge gap.&lt;/p&gt;

&lt;p&gt;If the provided OpenAPI validation schema provided by the CRD author is not structural, violations are reported in a &lt;code&gt;NonStructural&lt;/code&gt; condition in the CRD.&lt;/p&gt;

&lt;p&gt;A structural schema for CRDs in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt; will not be required. But we plan to require structural schemas for every CRD created in &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt;, targeted for 1.16.&lt;/p&gt;

&lt;p&gt;But now let us see what a structural schema looks like.&lt;/p&gt;

&lt;h2 id=&#34;structural-schema&#34;&gt;Structural Schema&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;core of a structural schema&lt;/strong&gt; is an OpenAPI v3 schema made out of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;properties&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;items&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;additionalProperties&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nullable&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;descriptions&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, all types must be non-empty, and in each sub-schema only one of &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt; or &lt;code&gt;items&lt;/code&gt; may be used.&lt;/p&gt;

&lt;p&gt;Here is an example of our &lt;code&gt;MaintenanceNightlyJob&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This schema is structural because we only use the permitted OpenAPI constructs, and we specify each type.&lt;/p&gt;

&lt;p&gt;Note that we leave out &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;. These are implicitly defined for each object.&lt;/p&gt;

&lt;p&gt;Starting from this structural core of our schema, we might enhance it for value validation purposes with nearly all other OpenAPI constructs, with only a few restrictions, for example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                          &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                          &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;pattern:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“^[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+(-[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+)&lt;span style=&#34;color:#080&#34;&gt;*$”&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;                                    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“command”]&lt;span style=&#34;color:#bbb&#34;&gt;                   &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“shell”]&lt;span style=&#34;color:#bbb&#34;&gt;                     &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“spec”]&lt;span style=&#34;color:#bbb&#34;&gt;                            &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notable restrictions for these additional value validations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the last 5 of the core constructs are not allowed: &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;every properties field mentioned, must also show up in the core (without the blue value validations).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see also logical constraints using &lt;code&gt;oneOf&lt;/code&gt;, &lt;code&gt;allOf&lt;/code&gt;, &lt;code&gt;anyOf&lt;/code&gt;, &lt;code&gt;not&lt;/code&gt; are allowed.&lt;/p&gt;

&lt;p&gt;To sum up, an OpenAPI schema is structural if&lt;br/&gt;&lt;br/&gt;
1. it has the core as defined above out of &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;items&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;,&lt;br/&gt;
2. all types are defined,&lt;br/&gt;
3. the core is extended with value validation following the constraints:&lt;br/&gt;
   (i) inside of value validations no &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;&lt;br/&gt;
   (ii) all fields mentioned in value validation are specified in the core.&lt;/p&gt;

&lt;p&gt;Let us modify our example spec slightly, to make it non-structural:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;pattern:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“^[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+(-[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+)&lt;span style=&#34;color:#080&#34;&gt;*$”&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“command”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“shell”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;not:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“spec”]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This spec is non-structural for many reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;type: object&lt;/code&gt; at the root is missing (rule 2).&lt;/li&gt;
&lt;li&gt;inside of &lt;code&gt;oneOf&lt;/code&gt; it is not allowed to use &lt;code&gt;type&lt;/code&gt; (rule 3-i).&lt;/li&gt;
&lt;li&gt;inside of &lt;code&gt;not&lt;/code&gt; the property &lt;code&gt;privileged&lt;/code&gt; is mentioned, but it is not specified in the core (rule 3-ii).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we know what a structural schema is, and what is not, let us take a look at our attempt above to forbid &lt;code&gt;privileged&lt;/code&gt; as a field. While we have seen that this is not possible in a structural schema, the good news is that we don’t have to explicitly attempt to forbid unwanted fields in advance.&lt;/p&gt;

&lt;h2 id=&#34;pruning-don-t-preserve-unknown-fields&#34;&gt;Pruning – don’t preserve unknown fields&lt;/h2&gt;

&lt;p&gt;In &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt; pruning will be the default, with ways to opt-out of it. Pruning in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt; is enabled via&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiextensions/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CustomResourceDefinition&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;…&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;preserveUnknownFields:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pruning can only be enabled if the global schema or the schemas of all versions are structural.&lt;/p&gt;

&lt;p&gt;If pruning is enabled, the pruning algorithm&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;assumes that the schema is complete, i.e. every field is mentioned and not-mentioned fields can be dropped&lt;/li&gt;
&lt;li&gt;is run on&lt;br/&gt;
(i) data received via an API request&lt;br/&gt;
(ii) after conversion and admission requests&lt;br/&gt;
(iii) when reading from etcd (using the schema version of the data in etcd).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we don’t specify &lt;code&gt;privileged&lt;/code&gt; in our structural example schema, the malicious field is pruned from before persisting to etcd:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;operations/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;MaintenanceNightlyJob&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;&amp;gt;
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;    grep backdoor /etc/passwd || &lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“backdoor:76asdfh76:/bin/bash”&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/passwd&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;||&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“az1-master1”,”az1-master2”,”az2-master3”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# pruned: privileged: true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;

&lt;p&gt;While most Kubernetes-like APIs can be expressed with a structural schema, there are a few exceptions, notably &lt;code&gt;intstr.IntOrString&lt;/code&gt;, &lt;code&gt;runtime.RawExtension&lt;/code&gt;s and pure JSON fields.&lt;/p&gt;

&lt;p&gt;Because we want CRDs to make use of these types as well, we introduce the following OpenAPI vendor extensions to the permitted core constructs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;x-kubernetes-embedded-resource: true&lt;/code&gt; — specifies that this is an &lt;code&gt;runtime.RawExtension&lt;/code&gt;-like field, with a Kubernetes resource with apiVersion, kind and metadata. The consequence is that those 3 fields are not pruned and are automatically validated.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;x-kubernetes-int-or-string: true&lt;/code&gt; — specifies that this is either an integer or a string. No types must be specified, but&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;integer&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;is permitted, though optional.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x-kubernetes-preserve-unknown-fields: true&lt;/code&gt; — specifies that the pruning algorithm should not prune any field. This can be combined with &lt;code&gt;x-kubernetes-embedded-resource&lt;/code&gt;. Note that within a nested &lt;code&gt;properties&lt;/code&gt; or &lt;code&gt;additionalProperties&lt;/code&gt; OpenAPI schema the pruning starts again.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One can use &lt;code&gt;x-kubernetes-preserve-unknown-fields: true&lt;/code&gt; at the root of the schema (and inside any &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt;) to get the traditional CRD behaviour that nothing is pruned, despite setting &lt;code&gt;spec.preserveUnknownProperties: false&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With this we conclude the discussion of the structural schema in Kubernetes 1.15 and beyond. To sum up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;structural schemas are optional in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt;. Non-structural CRDs will keep working as before.&lt;/li&gt;
&lt;li&gt;pruning (enabled via &lt;code&gt;spec.preserveUnknownProperties: false&lt;/code&gt;) requires a structural schema.&lt;/li&gt;
&lt;li&gt;structural schema violations are signalled via the &lt;code&gt;NonStructural&lt;/code&gt; condition in the CRD.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Structural schemas are the future of CRDs. &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt; will require them. But&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;x-kubernetes-preserve-unknown-fields:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;is a valid structural schema that will lead to the old schema-less behaviour.&lt;/p&gt;

&lt;p&gt;Any new feature for CRDs starting from Kubernetes 1.15 will require to have a structural schema:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;publishing of OpenAPI validation schemas and therefore support for kubectl client-side validation, and &lt;code&gt;kubectl explain&lt;/code&gt; support (beta in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;CRD conversion (beta in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;CRD defaulting (alpha in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;Server-side apply (alpha in Kubernetes 1.15, CRD support pending).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema&#34; target=&#34;_blank&#34;&gt;structural schemas&lt;/a&gt; are also described in the Kubernetes documentation for the 1.15 release.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.15: Extensibility and Continuous Improvement</title>
      <link>https://kubernetes.io/blog/2019/06/19/kubernetes-1-15-release-announcement/</link>
      <pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/19/kubernetes-1-15-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; The 1.15 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.15/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.15, our second release of 2019! Kubernetes 1.15 consists of 25 enhancements: 2 moving to stable, 13 in beta, and 10 in alpha. The main themes of this release are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Continuous Improvement

&lt;ul&gt;
&lt;li&gt;Project sustainability is not just about features. Many SIGs have been working on improving test coverage, ensuring the basics stay reliable, and stability of the core feature set and working on maturing existing features and cleaning up the backlog.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Extensibility

&lt;ul&gt;
&lt;li&gt;The community has been asking for continuing support of extensibility, so this cycle features more work around CRDs and API Machinery. Most of the enhancements in this cycle were from SIG API Machinery and related areas.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;extensibility-around-core-kubernetes-apis&#34;&gt;Extensibility around core Kubernetes APIs&lt;/h2&gt;

&lt;p&gt;The theme of the new developments around CustomResourceDefinitions is data consistency and native behaviour. A user should not notice whether the interaction is with a CustomResource or with a Golang-native resource. With big steps we are working towards a GA release of CRDs and GA of admission webhooks in one of the next releases.&lt;/p&gt;

&lt;p&gt;In this direction, we have rethought our OpenAPI based validation schemas in CRDs and from 1.15 on we check each schema against a restriction called “structural schema”. This basically enforces non-polymorphic and complete typing of each field in a CustomResource. We are going to require structural schemas in the future, especially for all new features including those listed below, and list violations in a &lt;code&gt;NonStructural&lt;/code&gt; condition. Non-structural schemas keep working for the time being in the v1beta1 API group. But any serious CRD application is urged to migrate to structural schemas in the foreseeable future.&lt;/p&gt;

&lt;p&gt;Details about what makes a schema structural will be published in a blog post on kubernetes.io later this week, and it is of course &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema&#34;&gt;documented in the Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinition Webhook Conversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions support multiple versions as beta since 1.14. With Kubernetes 1.15, they gain the ability to convert between different versions on-the-fly, just like users are used to from native resources for long term. Conversions for CRDs are implemented via webhooks, deployed inside the cluster by the cluster admin. This feature is promoted to beta in Kubernetes 1.15, lifting CRDs to a completely new level for serious CRD applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinition OpenAPI Publishing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OpenAPI specs for native types have been served at &lt;code&gt;/openapi/v2&lt;/code&gt; by kube-apiserver for a long time, and they are consumed by a number of components, notably kubectl client-side validation, kubectl explain and OpenAPI based client generators.&lt;/p&gt;

&lt;p&gt;OpenAPI publishing for CRDs will be available with Kubernetes 1.15 as beta, yet again only for structural schemas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinitions Pruning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pruning is the automatic removal of unknown fields in objects sent to a Kubernetes API. A field is unknown if it is not specified in the OpenAPI validation schema. This is both a data consistency and security relevant feature. It enforces that only data structures specified by the CRD developer are persisted to etcd. This is the behaviour of native resources, and will be available for CRDs as well, starting as beta in Kubernetes 1.15.&lt;/p&gt;

&lt;p&gt;Pruning is activated via &lt;code&gt;spec.preserveUnknownFields: false&lt;/code&gt; in the CustomResourceDefinition. A future apiextensions.k8s.io/v1 variant of CRDs will enforce pruning (with a possible, but explicitly necessary opt-out).&lt;/p&gt;

&lt;p&gt;Pruning requires that CRD developer provides complete, structural validation schemas, either top-level or for all versions of the CRD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;alpha: CustomResourceDefinition Defaulting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions get support for defaulting. Defaults are specified using the &lt;code&gt;default&lt;/code&gt; keyword in the OpenAPI validation schema. Defaults are set for unspecified field in an object sent to the API, and when reading from etcd.&lt;/p&gt;

&lt;p&gt;Defaulting will be available as alpha in Kubernetes 1.15 for structural schemas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: Admission Webhook Reinvocation &amp;amp; Improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mutating and validating admission webhooks become more and more mainstream for projects extending the Kubernetes API. Until now mutating webhooks were only called once, in alphabetical order. An earlier run webhook cannot react on the output of webhooks called later in the chain. With Kubernetes 1.15 this will change:&lt;/p&gt;

&lt;p&gt;Mutating webhooks can opt-in into at least one re-invocation by specifying &lt;code&gt;reinvocationPolicy: IfNeeded&lt;/code&gt;. If a later mutating webhook modifies the object, the earlier webhook will get a second chance.&lt;/p&gt;

&lt;p&gt;This requires that webhooks have an idem-potent-like behaviour which can cope with this second invocation.&lt;/p&gt;

&lt;p&gt;It is not planned to add another round of invocations such that webhook authors still have to be careful about the changes to admitted objects they implement. Finally the validating webhooks are called to verify that promised invariants are fulfilled.&lt;/p&gt;

&lt;p&gt;There are more smaller changes to admission webhook, notably &lt;code&gt;objectSelector&lt;/code&gt; to exclude objects with certain labels from admission, arbitrary port (not only 443) for the webhook server.&lt;/p&gt;

&lt;h2 id=&#34;cluster-lifecycle-stability-and-usability-improvements&#34;&gt;Cluster Lifecycle Stability and Usability Improvements&lt;/h2&gt;

&lt;p&gt;Work on making Kubernetes installation, upgrade and configuration even more robust has been a major focus for this cycle for SIG Cluster Lifecycle (see our last &lt;a href=&#34;https://docs.google.com/presentation/d/1QUOsQxfEfHlMq4lPjlK2ewQHsr9peEKymDw5_XwZm8Q/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Community Update&lt;/a&gt;). Bug fixes across bare metal tooling and production-ready user stories, such as the high availability use cases have been given priority for 1.15.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kubeadm&lt;/strong&gt;, the cluster lifecycle building block, continues to receive features and stability work required for bootstrapping production clusters efficiently. kubeadm has promoted high availability (HA) capability to beta, allowing users to use the familiar &lt;code&gt;kubeadm init&lt;/code&gt; and &lt;code&gt;kubeadm join&lt;/code&gt; commands to &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/&#34;&gt;configure and deploy an HA control plane&lt;/a&gt;. An entire new test suite has been created specifically for ensuring these features will stay stable over time.&lt;/p&gt;

&lt;p&gt;Certificate management has become more robust in 1.15, with kubeadm now seamlessly rotating all your certificates (on upgrades) before they expire. Check the &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/&#34;&gt;kubeadm documentation&lt;/a&gt; for information on how to manage your certificates.&lt;/p&gt;

&lt;p&gt;The kubeadm configuration file API is moving from v1beta1 to v1beta2 in 1.15.&lt;/p&gt;

&lt;p&gt;Finally, let’s celebrate that kubeadm now &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/1588&#34; target=&#34;_blank&#34;&gt;has its own logo&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-19-kubernetes-1-15-release-announcement/kubeadm-logo.png&#34; alt=&#34;kubeadm official logo&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;continued-improvement-of-csi&#34;&gt;Continued improvement of CSI&lt;/h2&gt;

&lt;p&gt;In Kubernetes v1.15, SIG Storage continued work to &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/625&#34; target=&#34;_blank&#34;&gt;enable migration of in-tree volume plugins&lt;/a&gt; to Container Storage Interface (CSI). SIG Storage worked on bringing CSI to feature parity with in-tree functionality, including functionality like resizing, inline volumes, and more. SIG Storage introduces new alpha functionality in CSI that doesn&amp;rsquo;t exist in the Kubernetes Storage subsystem yet, like volume cloning.&lt;/p&gt;

&lt;p&gt;Volume cloning enables users to specify another PVC as a &amp;ldquo;DataSource&amp;rdquo; when provisioning a new volume. If the underlying storage system supports this functionality and implements the &amp;ldquo;CLONE_VOLUME&amp;rdquo; capability in its CSI driver, then the new volume becomes a clone of the source volume.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional Notable Feature Updates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support for go modules in Kubernetes Core&lt;/li&gt;
&lt;li&gt;Continued preparation on cloud provider extraction and code organization. The cloud provider code has been moved to &lt;a href=&#34;https://github.com/kubernetes/legacy-cloud-providers&#34; target=&#34;_blank&#34;&gt;kubernetes/legacy-cloud-providers&lt;/a&gt; for easier removal later and external consumption.&lt;/li&gt;
&lt;li&gt;Kubectl &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/515&#34; target=&#34;_blank&#34;&gt;get and describe&lt;/a&gt; now work with extensions&lt;/li&gt;
&lt;li&gt;Nodes now support &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/606&#34; target=&#34;_blank&#34;&gt;third party monitoring plugins&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A new &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/624&#34; target=&#34;_blank&#34;&gt;Scheduling Framework&lt;/a&gt; for schedule plugins is now Alpha&lt;/li&gt;
&lt;li&gt;ExecutionHook API &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/962&#34; target=&#34;_blank&#34;&gt;designed to trigger hook commands&lt;/a&gt; in the containers for different use cases is now Alpha.&lt;/li&gt;
&lt;li&gt;Continued deprecation of extensions/v1beta1, apps/v1beta1, and apps/v1beta2 APIs; these extensions will be retired in 1.16!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#kubernetes-v115-release-notes&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt; for a complete list of notable features and fixes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 1.15 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.15.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.15 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features Blog Series&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back this week and the next for our Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Future of CRDs: Structural Schemas&lt;/li&gt;
&lt;li&gt;Introducing Volume Cloning Alpha for Kubernetes&lt;/li&gt;
&lt;li&gt;Automated High Availability in Kubeadm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Release team&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://git.k8s.io/sig-release/releases/release-1.15/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Claire Laurence, Senior Technical Program Manager at Pivotal Software. The 38 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;32,000 individual contributors&lt;/a&gt; to date and an active community of more than 66,000 people.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project Velocity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;from=now-1y&amp;amp;to=now&#34; target=&#34;_blank&#34;&gt;379 different companies and over 2,715 individuals&lt;/a&gt; contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;User Highlights&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;China Unicom&lt;/strong&gt; is using Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/chinaunicom/&#34; target=&#34;_blank&#34;&gt;increase their resource utilization 20-50%&lt;/a&gt;, lowering IT infrastructure costs, and cutting deployment time from hours to 10-15 minutes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The City of Montreal&lt;/strong&gt; is using Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/city-of-montreal/&#34; target=&#34;_blank&#34;&gt;decrease deployments from months to hours&lt;/a&gt; and run 200 application components on 8 machines with 5 people operating Kubernetes clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SLAMTEC&lt;/strong&gt; is using Kubernetes along with other CNCF projects to achiever &lt;a href=&#34;https://kubernetes.io/case-studies/slamtec/&#34; target=&#34;_blank&#34;&gt;18+ months of 100% uptime&lt;/a&gt; saving 50% time spent on troubleshooting and debugging and 30% time savings on CI/CD efforts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ThredUP&lt;/strong&gt; has decreased deployment time by &lt;a href=&#34;https://kubernetes.io/case-studies/thredup/&#34; target=&#34;_blank&#34;&gt;about 50% on average for key services&lt;/a&gt; and has shrunk lead time for deployment to under 20 minutes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ecosystem Updates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes recently celebrated its &lt;a href=&#34;https://www.cncf.io/blog/2019/06/06/reflections-on-the-fifth-anniversary-of-kubernetes/&#34; target=&#34;_blank&#34;&gt;five-year anniversary&lt;/a&gt; at KubeCon + CloudNativeCon Barcelona&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.cncf.io/certification/expert/cka/&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes Administrator (CKA) exam&lt;/a&gt; has become one of the most popular Linux Foundation certifications to date with over 9,000 registrations and over 1,700 individuals that passed and received the certification.&lt;/li&gt;
&lt;li&gt;Coming off the heels of a successful &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon Europe 2019&lt;/a&gt;, the CNCF announced it has over 400 members with a 130 percent year-over-year growth rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;KubeCon&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; (co-located with Open Source Summit) from June 24-26, 2019 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/&#34; target=&#34;_blank&#34;&gt;San Diego&lt;/a&gt; from November 18-21. These conferences will feature technical sessions, case studies, developer deep dives, salons, and more! &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;&lt;strong&gt;Webinar&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.15 release team on July 23 at 10am PDT to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/8415609575308/WN_AtjsGjz5TRqOsLrEFTWlJQ&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get Involved&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below. Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Join us at the Contributor Summit in Shanghai</title>
      <link>https://kubernetes.io/blog/2019/06/12/join-us-at-the-contributor-summit-in-shanghai/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/12/join-us-at-the-contributor-summit-in-shanghai/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Josh Berkus (Red Hat)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png&#34; alt=&#34;Picture of contributor panel at 2018 Shanghai contributor summit.  Photo by Josh Berkus, licensed CC-BY 4.0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the second year, we will have &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/&#34; target=&#34;_blank&#34;&gt;a Contributor Summit event&lt;/a&gt; the day before &lt;a href=&#34;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;KubeCon China&lt;/a&gt; in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt;. The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints.&lt;/p&gt;

&lt;h3 id=&#34;current-contributor-day&#34;&gt;Current Contributor Day&lt;/h3&gt;

&lt;p&gt;After last year&amp;rsquo;s Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule.&lt;/p&gt;

&lt;p&gt;While we do not yet have a full schedule up, the topics covered in the current contributor track will include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to write a KEP (Kubernetes Enhancement Proposal)&lt;/li&gt;
&lt;li&gt;Codebase and repository review&lt;/li&gt;
&lt;li&gt;Local Build &amp;amp; Test troubleshooting session&lt;/li&gt;
&lt;li&gt;Guide to Non-Code Contribution opportunities&lt;/li&gt;
&lt;li&gt;SIG-Azure face-to-face meeting&lt;/li&gt;
&lt;li&gt;SIG-Scheduling face-to-face meeting&lt;/li&gt;
&lt;li&gt;Other SIG face-to-face meetings as we confirm them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The schedule will be on &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit&#34; target=&#34;_blank&#34;&gt;the Community page&lt;/a&gt; once it is complete.&lt;/p&gt;

&lt;p&gt;If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact &lt;a href=&#34;mailto:jberkus@redhat.com&#34; target=&#34;_blank&#34;&gt;Josh Berkus&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;new-contributor-workshop&#34;&gt;New Contributor Workshop&lt;/h3&gt;

&lt;p&gt;Students at &lt;a href=&#34;https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/&#34;&gt;last year&amp;rsquo;s New Contributor Workshop&lt;/a&gt; (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a one-stop-shop for becoming familiar with the community.&amp;rdquo; said one participant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you have not contributed to Kubernetes before, or have only done one or two things, please consider &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;enrolling&lt;/a&gt; in the NCW.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Got to know the process from signing CLA to PR and made friends with other contributors.&amp;rdquo; said another.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;documentation-sprints&#34;&gt;Documentation Sprints&lt;/h3&gt;

&lt;p&gt;Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;sign up&lt;/a&gt; to help with the Doc Sprints.&lt;/p&gt;

&lt;h3 id=&#34;before-you-attend&#34;&gt;Before you attend&lt;/h3&gt;

&lt;p&gt;Regardless of where you participate, everyone at the Contributor Summit should &lt;a href=&#34;https://git.k8s.io/community/CLA.md#the-contributor-license-agreement&#34; target=&#34;_blank&#34;&gt;sign the Kubernetes Contributor License Agreement&lt;/a&gt; (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kyma - extend and build on Kubernetes with ease</title>
      <link>https://kubernetes.io/blog/2019/05/23/kyma-extend-and-build-on-kubernetes-with-ease/</link>
      <pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/23/kyma-extend-and-build-on-kubernetes-with-ease/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lukasz Gornicki (SAP)&lt;/p&gt;

&lt;p&gt;According to this recently completed &lt;a href=&#34;https://www.cncf.io/blog/2018/08/29/cncf-survey-use-of-cloud-native-technologies-in-production-has-grown-over-200-percent/&#34; target=&#34;_blank&#34;&gt;CNCF Survey&lt;/a&gt;, the adoption rate of Cloud Native technologies in production is growing rapidly. Kubernetes is at the heart of this technological revolution. Naturally, the growth of cloud native technologies has been accompanied by the growth of the ecosystem that surrounds it. Of course, the complexity of cloud native technologies have increased as well. Just google for the phrase “Kubernetes is hard”, and you’ll get plenty of articles that explain this complexity problem. The best thing about the CNCF community is that problems like this can be solved by smart people building new tools to enable Kubernetes users: Projects like Knative and its &lt;a href=&#34;https://github.com/knative/build&#34; target=&#34;_blank&#34;&gt;Build resource&lt;/a&gt; extension, for example, serve to reduce complexity across a range of scenarios. Even though increasing complexity might seem like the most important issue to tackle, it is not the only challenge you face when transitioning to Cloud Native.&lt;/p&gt;

&lt;h2 id=&#34;problems-to-solve&#34;&gt;Problems to solve&lt;/h2&gt;

&lt;h3 id=&#34;picking-the-right-technology-is-hard&#34;&gt;Picking the right technology is hard&lt;/h3&gt;

&lt;p&gt;Now that you understand Kubernetes, your teams are trained and you’ve started building applications on top, it’s time to face a new layer of challenges. Cloud native doesn’t just mean deploying a platform for developers to build on top of. Developers also need storage, backup, monitoring, logging and a service mesh to enforce policies upon data in transit. Each of these individual systems must be properly configured and deployed, as well as logged, monitored and backed up on its own. The CNCF is here to help. We provide a &lt;a href=&#34;https://landscape.cncf.io/&#34; target=&#34;_blank&#34;&gt;landscape&lt;/a&gt; overview of all cloud-native technologies, but the list is huge and can be overwhelming.&lt;/p&gt;

&lt;p&gt;This is where &lt;a href=&#34;http://kyma-project.io&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; will make your life easier. Its mission statement is to enable a flexible and easy way of extending applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/kyma-center.png&#34; width=&#34;40%&#34; alt=&#34;Kyma in center&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This project is designed to give you the tools you need to be able to write an end-to-end, production-grade cloud native application. &lt;a href=&#34;https://github.com/kyma-project/kyma/&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; was donated to the open-source community by &lt;a href=&#34;https://www.sap.com&#34; target=&#34;_blank&#34;&gt;SAP&lt;/a&gt;; a company with great experience in writing production-grade cloud native applications. That’s why we’re so excited to &amp;ndash; &lt;a href=&#34;https://twitter.com/kymaproject/status/1121426458243678209&#34; target=&#34;_blank&#34;&gt;announce&lt;/a&gt; the first major release of &lt;a href=&#34;https://github.com/kyma-project/kyma/releases/tag/1.0.0&#34; target=&#34;_blank&#34;&gt;Kyma 1.0&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&#34;deciding-on-the-path-from-monolith-to-cloud-native-is-hard&#34;&gt;Deciding on the path from monolith to cloud-native is hard&lt;/h3&gt;

&lt;p&gt;Try Googling &lt;code&gt;monolith to cloud native&lt;/code&gt; or &lt;code&gt;monolith to microservices&lt;/code&gt; and you’ll get a list of plenty of talks and papers that tackle this challenge. There are many different paths available for migrating a monolith to the cloud, and our experience has taught us to be quite opinionated in this area. First, let&amp;rsquo;s answer the question of why you’d want to move from monolith to cloud native. The goals driving this move are typically:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Increased scalability.&lt;/li&gt;
&lt;li&gt;Faster implementation of new features.&lt;/li&gt;
&lt;li&gt;More flexible approach to extensibility.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You do not have to rewrite your monolith to achieve these goals. Why spend all that time rewriting functionality that you already have? Just focus on enabling your monolith to support &lt;a href=&#34;https://en.wikipedia.org/wiki/Event-driven_architecture&#34; target=&#34;_blank&#34;&gt;event-driven architecture&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-does-kyma-solve-your-challenges&#34;&gt;How does Kyma solve your challenges?&lt;/h2&gt;

&lt;h3 id=&#34;what-is-kyma&#34;&gt;What is Kyma?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kyma-project.io/docs/root/kyma/#overview-overview&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; runs on Kubernetes and consists of a number of different components, three of which are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/application-connector/&#34; target=&#34;_blank&#34;&gt;Application connector&lt;/a&gt; that you can use to connect any application with a Kubernetes cluster and expose its APIs and Events through the &lt;a href=&#34;https://github.com/kubernetes-incubator/service-catalog&#34; target=&#34;_blank&#34;&gt;Kubernetes Service Catalog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/serverless/&#34; target=&#34;_blank&#34;&gt;Serverless&lt;/a&gt; which enables you to easily write extensions for your application. Your function code can be triggered by API calls and also by events coming from external system. You can also securely call back the integrated system from your function.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/&#34; target=&#34;_blank&#34;&gt;Service Catalog&lt;/a&gt; is here to expose integrated systems. This integration also enables you to use services from hyperscalers like Azure, AWS or Google Cloud. &lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/#service-brokers-service-brokers&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; allows for easy integration of official service brokers maintained by Microsoft and Google.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/ac-s-sc.svg&#34; alt=&#34;core components&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can watch &lt;a href=&#34;https://www.youtube.com/watch?v=wJzVWFGkiKk&#34; target=&#34;_blank&#34;&gt;this video&lt;/a&gt; for a short overview of Kyma key features that is based on a real demo scenario.&lt;/p&gt;

&lt;h3 id=&#34;we-picked-the-right-technologies-for-you&#34;&gt;We picked the right technologies for you&lt;/h3&gt;

&lt;p&gt;You can provide reliable extensibility in a project like Kyma only if it is properly monitored and configured. We decided not to reinvent the wheel. There are many great projects in the CNCF landscape, most with huge communities behind them. We decided to pick the best ones and glue them all together in Kyma. You can see the same architecture diagram that is above but with a focus on the projects we put together to create Kyma:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/arch.png&#34; width=&#34;70%&#34; alt=&#34;Kyma architecture&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Monitoring and alerting is based on &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; and &lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34;&gt;Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Logging is based on &lt;a href=&#34;https://grafana.com/loki&#34; target=&#34;_blank&#34;&gt;Loki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Eventing uses &lt;a href=&#34;https://github.com/knative/eventing/&#34; target=&#34;_blank&#34;&gt;Knative&lt;/a&gt; and &lt;a href=&#34;https://nats.io/&#34; target=&#34;_blank&#34;&gt;NATS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Asset management uses &lt;a href=&#34;https://min.io/&#34; target=&#34;_blank&#34;&gt;Minio&lt;/a&gt; as a storage&lt;/li&gt;
&lt;li&gt;Service Mesh is based on &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tracing is done with &lt;a href=&#34;https://www.jaegertracing.io/&#34; target=&#34;_blank&#34;&gt;Jaeger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authentication is supported by &lt;a href=&#34;https://github.com/dexidp/dex&#34; target=&#34;_blank&#34;&gt;dex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You don&amp;rsquo;t have to integrate these tools: We made sure they all play together well, and are always up to date ( Kyma is already using Istio 1.1). With our custom &lt;a href=&#34;https://github.com/kyma-project/kyma/blob/master/docs/kyma/04-02-local-installation.md&#34; target=&#34;_blank&#34;&gt;Installer&lt;/a&gt; and &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; charts, we enabled easy installation and easy upgrades to new versions of Kyma.&lt;/p&gt;

&lt;h3 id=&#34;do-not-rewrite-your-monoliths&#34;&gt;Do not rewrite your monoliths&lt;/h3&gt;

&lt;p&gt;Rewriting is hard, costs a fortune, and in most cases is not needed. At the end of the day, what you need is to be able to write and put new features into production quicker. You can do it by connecting your monolith to Kyma using the &lt;a href=&#34;https://kyma-project.io/docs/components/application-connector&#34; target=&#34;_blank&#34;&gt;Application Connector&lt;/a&gt;. In short, this component makes sure that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can securely call back the registered monolith without the need to take care of authorization, as the Application Connector handles this.&lt;/li&gt;
&lt;li&gt;Events sent from your monolith get securely to the Kyma Event Bus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the moment, your monolith can consume three different types of services: REST (with &lt;a href=&#34;https://www.openapis.org/&#34; target=&#34;_blank&#34;&gt;OpenAPI&lt;/a&gt; specification)  and OData (with Entity Data Model specification) for synchronous communication, and for asynchronous communication you can register a catalog of events based on &lt;a href=&#34;https://www.asyncapi.com/&#34; target=&#34;_blank&#34;&gt;AsyncAPI&lt;/a&gt; specification. Your events are later delivered internally using &lt;a href=&#34;https://nats.io/&#34; target=&#34;_blank&#34;&gt;NATS Streaming&lt;/a&gt; channel with &lt;a href=&#34;https://github.com/knative/eventing/&#34; target=&#34;_blank&#34;&gt;Knative eventing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once your monolith&amp;rsquo;s services are connected, you can provision them in selected Namespaces thanks to the previously mentioned &lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/&#34; target=&#34;_blank&#34;&gt;Service Catalog&lt;/a&gt; integration. You, as a developer, can go to the catalog and see a list of all the services you can consume. There are services from your monolith, and services from other 3rd party providers thanks to registered Service Brokers, like &lt;a href=&#34;https://github.com/Azure/open-service-broker-azure&#34; target=&#34;_blank&#34;&gt;Azure&amp;rsquo;s OSBA&lt;/a&gt;. It is the one single place with everything you need. If you want to stand up a new application, everything you need is already available in Kyma.&lt;/p&gt;

&lt;h3 id=&#34;finally-some-code&#34;&gt;Finally some code&lt;/h3&gt;

&lt;p&gt;Check out some code I had to write to integrate a monolith with Azure services. I wanted to understand the sentiments shared by customers under the product&amp;rsquo;s review section. On every event with a review comment, I wanted to use machine learning to call a sentiments analysis service, and in the case of a negative comment, I wanted to store it in a database for later review. This is the code of a function created thanks to our &lt;a href=&#34;https://kyma-project.io/docs/components/serverless&#34; target=&#34;_blank&#34;&gt;Serverless&lt;/a&gt; component. Pay attention to my code comments:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can watch &lt;a href=&#34;https://www.youtube.com/watch?v=wJzVWFGkiKk&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; short video for a full demo of sentiment analysis function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-js&#34; data-lang=&#34;js&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* It is a function powered by NodeJS runtime so I have to import some necessary dependencies. I choosed Azure&amp;#39;s CosmoDB that is a Mongo-like database, so I could use a MongoClient */&lt;/span&gt;
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; axios &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; require(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;axios&amp;#34;&lt;/span&gt;);
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; MongoClient &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; require(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;mongodb&amp;#39;&lt;/span&gt;).MongoClient;

module.exports &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; { main&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; (event, context) {
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* My function was triggered because it was subscribed to customer review event. I have access to the payload of the event. */&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;let&lt;/span&gt; negative &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await isNegative(event.data.comment)
    
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; (negative) {
      console.log(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Customer sentiment is negative:&amp;#34;&lt;/span&gt;, event.data)
      await mongoInsert(event.data)
    } &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;else&lt;/span&gt; {
      console.log(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;This positive comment was not saved:&amp;#34;&lt;/span&gt;, event.data) 
    }
}}

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* Like in case of isNegative function, I focus of usage of the MongoClient API. The necessary information about the database location and an authorization needed to call it is injected into my function and I just need to pick a proper environment variable. */&lt;/span&gt;
async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; mongoInsert(data) {

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;try&lt;/span&gt; {
          client &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await MongoClient.connect(process.env.connectionString, { useNewUrlParser&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt; });
          db &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; client.db(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;mycommerce&amp;#39;&lt;/span&gt;);
          &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; collection &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; db.collection(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;);
          &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; await collection.insertOne(data);
    } &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;finally&lt;/span&gt; {
      client.close();
    }
}
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* This function calls Azure&amp;#39;s Text Analytics service to get information about the sentiment. Notice process.env.textAnalyticsEndpoint and process.env.textAnalyticsKey part. When I wrote this function I didn&amp;#39;t have to go to Azure&amp;#39;s console to get these details. I had these variables automatically injected into my function thanks to our integration with Service Catalog and our Service Binding Usage controller that pairs the binding with a function. */&lt;/span&gt;
async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; isNegative(comment) {
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;let&lt;/span&gt; response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await axios.post(&lt;span style=&#34;color:#b44&#34;&gt;`&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;process.env.textAnalyticsEndpoint&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;/sentiment`&lt;/span&gt;,
      { documents&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; [{ id&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt;, text&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; comment }] }, {headers&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;{ &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;Ocp-Apim-Subscription-Key&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; process.env.textAnalyticsKey }})
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; response.data.documents[&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;].score &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.5&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thanks to Kyma, I don&amp;rsquo;t have to worry about the infrastructure around my function. As I mentioned, I have all the tools needed in Kyma, and they are integrated together. I can quickly get access to my logs through &lt;a href=&#34;https://grafana.com/loki&#34; target=&#34;_blank&#34;&gt;Loki&lt;/a&gt;, and I can quickly get access to a preconfigured Grafana dashboard to see the metrics of my Lambda delivered thanks to &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; and &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/grafana-lambda.png&#34; width=&#34;70%&#34; alt=&#34;Grafana with preconfigured lambda dashboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Such an approach gives you a lot of flexibility in adding new functionality. It also gives you time to rethink the need to rewrite old functions.&lt;/p&gt;

&lt;h2 id=&#34;contribute-and-give-feedback&#34;&gt;Contribute and give feedback&lt;/h2&gt;

&lt;p&gt;Kyma is an open source project, and we would love help it grow. The way that happens is with your help. After reading this post, you already know that we don&amp;rsquo;t want to reinvent the wheel. We stay true to this approach in our work model, which enables community contributors. We work in &lt;a href=&#34;https://github.com/kyma-project/community/tree/master/contributing&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; and have publicly recorded meeting that you can join any time, so we have a setup similar to what you know from Kubernetes itself.
Feel free to share also your feedback with us, through &lt;a href=&#34;https://twitter.com/kymaproject&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; or &lt;a href=&#34;http://slack.kyma-project.io&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes, Cloud Native, and the Future of Software</title>
      <link>https://kubernetes.io/blog/2019/05/17/kubernetes-cloud-native-and-the-future-of-software/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/17/kubernetes-cloud-native-and-the-future-of-software/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brian Grant (Google), Jaice Singer DuMars (Google)&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-cloud-native-and-the-future-of-software&#34;&gt;Kubernetes, Cloud Native, and the Future of Software&lt;/h1&gt;

&lt;p&gt;Five years ago this June, Google Cloud announced a new application management technology called Kubernetes. It began with a &lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56&#34; target=&#34;_blank&#34;&gt;simple open source commit&lt;/a&gt;, followed the next day by a &lt;a href=&#34;https://cloudplatform.googleblog.com/2014/06/an-update-on-container-support-on-google-cloud-platform.html&#34; target=&#34;_blank&#34;&gt;one-paragraph blog mention&lt;/a&gt; around container support. Later in the week, Eric Brewer &lt;a href=&#34;https://www.youtube.com/watch?v=YrxnVKZeqK8&#34; target=&#34;_blank&#34;&gt;talked about Kubernetes for the first time&lt;/a&gt; at DockerCon. And soon the world was watching.&lt;/p&gt;

&lt;p&gt;We’re delighted to see Kubernetes become core to the creation and operation of modern software, and thereby a key part of the global economy. To us, the success of Kubernetes represents even more: A business transition with truly worldwide implications, thanks to the unprecedented cooperation afforded by the open source software movement.&lt;/p&gt;

&lt;p&gt;Like any important technology, Kubernetes has become about more than just itself; it has positively affected the environment in which it arose, changing how software is deployed at scale, how work is done, and how corporations engage with big open-source projects.&lt;/p&gt;

&lt;p&gt;Let’s take a look at how this happened, since it tells us a lot about where we are today, and what might be happening next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beginnings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The most important precursor to Kubernetes was the rise of application containers. Docker, the first tool to really make containers usable by a broad audience, began as an open source project in 2013. By containerizing an application, developers could achieve easier language runtime management, deployment, and scalability. This triggered a sea change in the application ecosystem. Containers made stateless applications easily scalable and provided an immutable deployment artifact that drastically reduced the number of variables previously encountered between test and production systems.&lt;/p&gt;

&lt;p&gt;While containers presented strong stand-alone value for developers, the next challenge was how to deliver and manage services, applications, and architectures that spanned multiple containers and multiple hosts.&lt;/p&gt;

&lt;p&gt;Google had already encountered similar issues within its own IT infrastructure. Running the world’s most popular search engine (and several other products with millions of users) lead to early innovation around, and adoption of, containers. Kubernetes was inspired by Borg, Google’s internal platform for scheduling and managing the hundreds of millions, and eventually billions, of containers that implement all of our services.&lt;/p&gt;

&lt;p&gt;Kubernetes is more than just “Borg, for everyone” It distills the most successful architectural and API patterns of prior systems and couples them with load balancing, authorization policies, and other features needed to run and manage applications at scale. This in turn provides the groundwork for cluster-wide abstractions that allow true portability across clouds.&lt;/p&gt;

&lt;p&gt;The November 2014 &lt;a href=&#34;https://cloudplatform.googleblog.com/2014/11/google-cloud-platform-live-introducing-container-engine-cloud-networking-and-much-more.html&#34; target=&#34;_blank&#34;&gt;alpha launch&lt;/a&gt; of Google Cloud’s &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/&#34; target=&#34;_blank&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt; introduced managed Kubernetes. There was an explosion of innovation around Kubernetes, and companies from the enterprise down to the startup saw barriers to adoption fall away. Google, Red Hat, and others in the community increased their investment of people, experience, and architectural know-how to ensure it was ready for increasingly mission-critical workloads. The response was a wave of adoption that swept it to the forefront of the crowded container management space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Rise of Cloud Native&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every enterprise, regardless of its core business, is embracing more digital technology. The ability to rapidly adapt is fundamental to continued growth and competitiveness. Cloud-native technologies, and especially Kubernetes, arose to meet this need, providing the automation and observability necessary to manage applications at scale and with high velocity. Organizations previously constrained to quarterly deployments of critical applications can now deploy safely multiple times a day.&lt;/p&gt;

&lt;p&gt;Kubernetes’s declarative, API-driven infrastructure empowers teams to operate independently, and enables them to focus on their business objectives. An inevitable cultural shift in the workplace has come from enabling greater autonomy and productivity and reducing the toil of development teams.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Increased engagement with open source&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ability for teams to rapidly develop and deploy new software creates a virtuous cycle of success for companies and technical practitioners alike. Companies have started to recognize that contributing back to the software projects they use not only improves the performance of the software for their use cases, but also builds critical skills and creates challenging opportunities that help them attract and retain new developers.&lt;/p&gt;

&lt;p&gt;The Kubernetes project in particular curates a collaborative culture that encourages contribution and sharing of learning and development with the community. This fosters a positive-sum ecosystem that benefits both contributors and end-users equally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s Next?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Where Kubernetes is concerned, five years seems like an eternity. That says much about the collective innovation we’ve seen in the community, and the rapid adoption of the technology.&lt;/p&gt;

&lt;p&gt;In other ways, it is just the start. New applications such as machine learning, edge computing, and the Internet of Things are finding their way into the cloud native ecosystem via projects like Kubeflow. Kubernetes is almost certain to be at the heart of their success.&lt;/p&gt;

&lt;p&gt;Kubernetes may be most successful if it becomes an invisible essential of daily life, like urban plumbing or electrical grids. True standards are dramatic, but they are also taken for granted. As Googler and KubeCon co-chair Janet Kuo said in a &lt;a href=&#34;https://www.youtube.com/watch?v=LAO7RuWwfzA&#34; target=&#34;_blank&#34;&gt;recent keynote&lt;/a&gt;, Kubernetes is going to become boring, and that’s a good thing, at least for the majority of people who don’t have to care about container management.&lt;/p&gt;

&lt;p&gt;At Google Cloud, we’re still excited about the project, and we go to work on it every day. Yet it’s all of the solutions and extensions that expand from Kubernetes that will dramatically change the world as we know it.&lt;/p&gt;

&lt;p&gt;So, as we all celebrate the continued success of Kubernetes, remember to take the time and thank someone you see helping make the community better. It’s up to all of us to foster a cloud-native ecosystem that prizes the efforts of everyone who helps maintain and nurture the work we do together.&lt;/p&gt;

&lt;p&gt;And, to everyone who has been a part of the global success of Kubernetes, thank you. You have changed the world.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Expanding our Contributor Workshops</title>
      <link>https://kubernetes.io/blog/2019/05/14/expanding-our-contributor-workshops/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/14/expanding-our-contributor-workshops/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guinevere Saenger (GitHub) and Paris Pittman (Google)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - learn about the contributor community with us and land your first
PR! We have spots available in &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona&lt;/a&gt; (registration &lt;strong&gt;closes&lt;/strong&gt; on
Wednesday May 15, so grab your spot!) and the upcoming &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; Summit.
The Barcelona event is poised to be our biggest one yet, with more registered
attendees than ever before!&lt;/p&gt;

&lt;p&gt;Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our community’s many code bases and seen places to improve? We
have a workshop for you!&lt;/p&gt;

&lt;p&gt;KubeCon + CloudNativeCon Barcelona’s &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;new contributor workshop&lt;/a&gt; will be the
fourth one of its kind, and we’re really looking forward to it! The workshop was
kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to
Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
We are constantly updating and improving the workshop content based on feedback
from past sessions. This time, we’re breaking up the participants by their
experience and comfort level with open source and Kubernetes. We’ll have
developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on. In the 201 track, we
will have a codebase walkthrough and local development and test demonstration
for folks who have a bit more experience in open source but may be unfamiliar
with our community’s development tools. For both tracks, you will have a chance
to get your hands dirty and have some fun. Because not every contributor works
with code, and not every contribution is technical, we will spend the beginning
of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.&lt;/p&gt;

&lt;h2 id=&#34;mentoring-opportunities&#34;&gt;Mentoring Opportunities&lt;/h2&gt;

&lt;p&gt;We will also bring back the SIG Meet-and-Greet where new contributors will have
a chance to mingle with current contributors, perhaps find their dream SIG,
learn what exciting areas they can help with, gain mentors, and make friends.&lt;/p&gt;

&lt;p&gt;PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on
Thursday, May 23. &lt;a href=&#34;http://bit.ly/mentor-bcn&#34; target=&#34;_blank&#34;&gt;Sign up here&lt;/a&gt;. 60% of the attendees during the
Seattle event asked contributor questions.&lt;/p&gt;

&lt;h2 id=&#34;past-attendee-story-vallery-lancy-engineer-at-lyft&#34;&gt;Past Attendee Story - Vallery Lancy, Engineer at Lyft&lt;/h2&gt;

&lt;p&gt;We talked to a few of our past participants in a series of interviews that we
will publish throughout the course of the year. In our first two clips, we meet
Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle
edition of the workshop. She was poking around in the community for a while to
see where she could jump in.&lt;/p&gt;

&lt;p&gt;Watch Vallery talk about her experience here:
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/uKg5WUcl6WU&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;What does Vallery say to folks curious about the workshops, or those attending
the Barcelona edition?&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/niHiem7JmPA&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Be like Vallery and hundreds of previous New Contributor Workshop attendees:
join us in Barcelona (or Shanghai - or San Diego!) for a unique experience
without digging into our documentation! Have the opportunity to meet with the
experts and go step by step into your journey with your peers around you. We’re
looking forward to seeing you there! &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;Register here&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Cat shirts and Groundhog Day: the Kubernetes 1.14 release interview</title>
      <link>https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/</guid>
      <description>
        
        
        &lt;p&gt;&lt;b&gt;Author&lt;/b&gt;: Craig Box (Google)&lt;/p&gt;

&lt;p&gt;Last week we celebrated one year of the &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt;. In this weekly show, my co-host Adam Glick and I focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.&lt;/p&gt;

&lt;p&gt;Every few cycles we check in on the release process for Kubernetes itself.  Last year we &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;interviewed the release managers for Kubernetes 1.11&lt;/a&gt;, and shared that transcript on the Kubernetes blog.  We got such great feedback that we wanted to share the transcript of our recent conversation with Aaron Crickenberger, the release manager for Kubernetes 1.14.&lt;/p&gt;

&lt;p&gt;As always, the canonical version can be enjoyed by listening to &lt;a href=&#34;https://kubernetespodcast.com/episode/046-kubernetes-1.14/&#34; target=&#34;_blank&#34;&gt;the podcast version&lt;/a&gt;. If you like what you hear, &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;we encourage you to subscribe&lt;/a&gt;!&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We like to start with our guests into digging into their backgrounds a little bit. Kubernetes is built from contributors from many different companies. You worked on Kubernetes at Samsung SDS before joining Google. Does anything change in your position in the community and the work you do, when you change companies?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Largely, no. I think the food&amp;rsquo;s a little bit better at the current company! But by and large, I have gotten to work with basically the same people doing basically the same thing. I cared about the community first and Google second before I joined Google, and I kind of still operate that way mostly because I believe that Google&amp;rsquo;s success depends upon the community&amp;rsquo;s success, as does everybody else who depends upon Kubernetes. A good and healthy upstream makes a good and healthy downstream.&lt;/p&gt;

&lt;p&gt;So that was largely why Samsung had me working on Kubernetes in the first place was because we thought the technology was legit. But we needed to make sure that the community and project as a whole was also legit. And so that&amp;rsquo;s why you&amp;rsquo;ve seen me continue to advocate for transparency and community empowerment throughout my tenure in Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You co-founded the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-testing&#34; target=&#34;_blank&#34;&gt;Testing SIG&lt;/a&gt;. How did you decide that that was needed, and at what stage in the process did you come to that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: This was very early on in the Kubernetes project. I&amp;rsquo;m actually a little hazy on specifically when it happened. But at the time, my boss, Bob Wise, worked with some folks within Google to co-found the Scalability SIG.&lt;/p&gt;

&lt;p&gt;If you remember way, way back when Kubernetes first started, there was concern over whether or not Kubernetes was performance enough. Like, I believe it officially supported something on the order of 100 nodes. And there were some who thought, that&amp;rsquo;s silly. I mean, come on, Google can do way more than that. And who in their right mind is going to use a container orchestrator that only supports 100 nodes?&lt;/p&gt;

&lt;p&gt;And of course the thing is we&amp;rsquo;re being super-conservative. We&amp;rsquo;re trying to iterate, ship early and often. And so we helped push the boundaries to make sure that Kubernetes could prove that it worked up to a thousand nodes before it was even officially supported to say, look, it already does this, we&amp;rsquo;re just trying to make sure we have all of the nuts and bolts tightened.&lt;/p&gt;

&lt;p&gt;OK, so great. We decided we needed to create a thing called a SIG in the very first place to talk about these things and make sure that we were moving in the right direction. I then turned my personal attention to testing as the next thing that I believe needed a SIG. So I believe that testing was the second SIG ever to be created for Kubernetes. It was co-founded initially with &lt;a href=&#34;https://github.com/ihmccreery&#34; target=&#34;_blank&#34;&gt;Ike McCreary&lt;/a&gt; who, at the time I believe, was an SRE for Google, and then eventually it was handed over to some folks who work in the engineering productivity part of Google where I think it aligned really well with testing&amp;rsquo;s interests.&lt;/p&gt;

&lt;p&gt;It is like &amp;ldquo;I don&amp;rsquo;t know what you people are trying to write here with Kubernetes, but I want to help you write it better, faster, and stronger&amp;rdquo;. And so I want to make sure we, as a community and as a project, are making it easier for you to write tests, easier for you to run tests, and most importantly, easier for you to act based on those test results.&lt;/p&gt;

&lt;p&gt;That came down to, let&amp;rsquo;s make sure that Kubernetes gets tested on more than just Google Cloud. That was super important to me, as somebody who operated not in Google Cloud but in other clouds. I think it really helped sell the story and build confidence in Kubernetes as something that worked effectively on multiple clouds. And I also thought it was really helpful to see SIG Testing in the community&amp;rsquo;s advocacy move us to a world today we can use test grids so that everybody see the same set of test results to understand what is allowed to prevent Kubernetes from going out the door.&lt;/p&gt;

&lt;p&gt;The process was basically just saying, let&amp;rsquo;s do it. The process was finding people who were motivated and suggesting that we meet on a recurring basis and we try to rally around a common set of work. This was sort of well before SIG governance was an official thing. And we gradually, after about a year, I think, settled on the pattern that most SIGs follow where you try to make sure you have a meeting agenda, you have a Slack channel, you have a mailing list, you discuss everything out in the open, you try to use sort a consistent set of milestones and move forward.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: A couple of things I wanted to ask about your life before Kubernetes. Why is there a &lt;a href=&#34;https://www.rockwellcollins.com/Products-and-Services/Defense/Simulation-and-Training/Training-Systems/Transportable-Black-Hawk-Operations-Simulator.aspx&#34; target=&#34;_blank&#34;&gt;Black Hawk flight simulator in a shipping container?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: As you may imagine, Black Hawk helicopters are flown in a variety of places around the world, not just next to a building that happens to have a parking lot next to it. And so in order to keep your pilots fresh, you may want to make sure they have good training hours and flight time, without spending fuel to fly an actual helicopter.&lt;/p&gt;

&lt;p&gt;I was involved in helping make what&amp;rsquo;s called a operation simulator, to train pilots on a bunch of the procedures using the same exact hardware that was deployed in Black Hawk helicopters, complete with motion seats that would shake to simulate movement and a full-fidelity visual system. This was all packed up in two shipping containers so that the simulator could be deployed wherever needed.&lt;/p&gt;

&lt;p&gt;I definitely had a really fun experience working on this simulator in the field at an Air Force base prior to a conference where I got to experience F-16s doing takeoff drills, which was amazing. They would get off the runway, and then just slam the afterburners to max and go straight up into the air. And I got to work on graphic simulation bugs. It was really cool.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: And for a lot of people, when you click on the web page they have listed in the GitHub link, you get their resume, or you get the list of open source projects they work on. In your case, there is &lt;a href=&#34;https://soundcloud.com/spiffxp&#34; target=&#34;_blank&#34;&gt;a SoundCloud page&lt;/a&gt;. What do people find on that page?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: They get to see me living my whole life. I find that music is a very important part of my life. It&amp;rsquo;s a non-verbal voice that I have developed over time. I needed some place to host that. And then it came down between SoundCloud and Bandcamp, and SoundCloud was a much easier place to host my recordings.&lt;/p&gt;

&lt;p&gt;So you get to hear the results of me having picked up a guitar and noodling with that about five years ago. You get to hear what I&amp;rsquo;ve learned messing around with Ableton Live. You get to hear some mixes that I&amp;rsquo;ve done of ambient music. And I haven&amp;rsquo;t posted anything in a while there because I&amp;rsquo;m trying to get my recording of drums just right.&lt;/p&gt;

&lt;p&gt;So if you go to &lt;a href=&#34;https://www.youtube.com/channel/UCfnUO-9Q_gMraUXjbk4p50g&#34; target=&#34;_blank&#34;&gt;my YouTube channel&lt;/a&gt;, mostly what you&amp;rsquo;ll see are recordings of the various SIG meetings that I&amp;rsquo;ve participated in. But if you go back a little bit earlier than that, you&amp;rsquo;ll see that I do, in fact, play the drums. I&amp;rsquo;m trying to get those folded into my next songs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Do you know who &lt;a href=&#34;https://en.wikipedia.org/wiki/Hugh_Padgham#The_%22gated_drum%22_sound&#34; target=&#34;_blank&#34;&gt;Hugh Padgham&lt;/a&gt; is?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I do not.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Hugh Padgham was the recording engineer who did the gated reverb drum sound that basically defined Phil Collins in the 1980s. I think you should call him up if you&amp;rsquo;re having problems with your drum sound.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That is awesome.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You mentioned you can also find videos of the work that you&amp;rsquo;re doing with the SIG. How did you become the release manager for 1.14?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I&amp;rsquo;ve been involved in the Kubernetes release process way back in the 1.4 days. I started out as somebody who tried to help figure out, how do you write release notes for this thing? How do you take this whole mess and try to describe it in a sane way that makes sense to end users and developers? And I gradually became involved in other aspects of the release over time.&lt;/p&gt;

&lt;p&gt;I helped out with CI Signal. I helped out with issue triage. When I helped out with CI Signal, I wrote the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/release-team/role-handbooks/ci-signal/README.md&#34; target=&#34;_blank&#34;&gt;very first playbook&lt;/a&gt; to describe what it is I do around here. That&amp;rsquo;s the model that has since been used for the rest of the release team, where every role describes what they do in a playbook that is used not just for their own benefit, but to help them train other people.&lt;/p&gt;

&lt;p&gt;Formally how I became release lead was I served as release shadow in 1.13. And when release leads are looking to figure out who&amp;rsquo;s going to lead the next release, they turn around and they look at their shadows, because those are who they have been helping out and training.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: If they don&amp;rsquo;t have a shadow, do they have to wait another three months and do a release again?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: They do not. The way it works is the release lead can look at their shadows, then they take a look at the rest of their release team leads to see if there is sufficient experience there. And then if not, they consult with the chairs of SIG release.&lt;/p&gt;

&lt;p&gt;So for example, for Kubernetes v1.15, I ended up in an unfortunate situation where neither of my shadows were available to step up and become the leads for 1.15. I consulted with &lt;a href=&#34;https://github.com/claurence&#34; target=&#34;_blank&#34;&gt;Claire Lawrence&lt;/a&gt;, who was my enhancements lead for 1.14 and who was on the release team for two quarters, and so met the requirements to become a release lead that way. So she will be the release lead for v1.15.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That was a fantastic answer to a throwaway &lt;a href=&#34;https://en.wikipedia.org/wiki/Groundhog_Day&#34; target=&#34;_blank&#34;&gt;Groundhog Day&lt;/a&gt; joke. I appreciate that.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: [LAUGHS]&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You can ask it again and see what the answer is, and then another time, and see how it evolves over time.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I&amp;rsquo;m short on my Groundhog Day riffs. I&amp;rsquo;ll come back to you.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What are your responsibilities as the release lead?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Don&amp;rsquo;t Panic. I mean, essentially, a release lead&amp;rsquo;s job is to make the final call, and then hold the line by making the final call. So what you shouldn&amp;rsquo;t be doing as a release lead is attempting to dive in and fix all of the things, or do all of the things, or second-guess anybody else&amp;rsquo;s work. You are there principally and primarily to listen to everybody else&amp;rsquo;s advice and help them make the best decision. And only in the situations where there&amp;rsquo;s not a clear consensus do you wade in and make the call yourself.&lt;/p&gt;

&lt;p&gt;I feel like I was helped out by a very capable team in this regard, this release cycle. So it was super helpful. But as somebody who has what I like to call an &amp;ldquo;accomplishment monkey&amp;rdquo; on my back, it can be very difficult to resist the urge to dive right in and help out, because I have been there before. I have the boots-on-the-ground experience.&lt;/p&gt;

&lt;p&gt;The release lead&amp;rsquo;s job is not to be the boots on the ground, but to help make sure that everybody who is boots on the ground is actually doing what they need to do and unblocked in doing what they need to do. It also involves doing songs and dances and making funny pictures. So I view it more as like it&amp;rsquo;s about effective communication. And doing a lot of songs and dances, and funny pictures, and memes is one way that I do that.&lt;/p&gt;

&lt;p&gt;So one way that I thought it would help people pay attention to the release updates that I gave every week at the Kubernetes community meeting was to make sure that I wore a different cat T-shirt each week. After people riffed and joked out my first cat T-shirt where I said, I really need coffee right &amp;ldquo;meow&amp;rdquo;, and somebody asked if I got that coffee from a &amp;ldquo;purr-colator&amp;rdquo;, I decided to up the ante.&lt;/p&gt;

&lt;p&gt;And I&amp;rsquo;ve heard that people will await those cat T-shirts. They want to know what the latest one is. I even got a special cat T-shirt just to signify that code freeze was coming.&lt;/p&gt;

&lt;p&gt;We also decided that instead of imposing this crazy process that involved a lot of milestones, and labels, and whatnot that would cause the machinery to impose a bunch of additional friction, I would just post a lot of memes to Twitter about code freeze coming. And that seems to have worked out really well. So by and large, the release lead&amp;rsquo;s job is communication, unblocking, and then doing nothing for as much as possible.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s really kind of difficult and terrifying because you always have this feeling that you may have missed something, or that you&amp;rsquo;re just not seeing something that&amp;rsquo;s out there. So I&amp;rsquo;m sitting in this position with a release that has been extremely stable, and I spent a lot of time thinking, OK, what am I missing? Like, this looks too good. This is too quiet. There&amp;rsquo;s usually something that blows up. Come on, what is it, what is it, what is it? And it&amp;rsquo;s an exercise in keeping that all in and not sharing it with everybody until the release is over.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: &lt;a href=&#34;https://twitter.com/KubernetesPod/status/1110611630180597760/photo/1&#34; target=&#34;_blank&#34;&gt;He is here in a cat T-shirt&lt;/a&gt;, as well.&lt;/p&gt;

&lt;p&gt;When a new US President takes over the office, it&amp;rsquo;s customary that the outgoing president leaves them a note with advice in it. Aside from the shadow team, is there something similar that exists with Kubernetes release management?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah, I would say there&amp;rsquo;s a very special&amp;ndash; I don&amp;rsquo;t know what the word is I&amp;rsquo;m looking for here&amp;ndash; bond, relationship, or something where people who have been release leads in the past are very empathetic and very supportive of those who step into the role as release lead.&lt;/p&gt;

&lt;p&gt;You know, I talked about release lead being a lot of uncertainty and second-guessing yourself, while on the outside you have to pretend like everything is OK. And having the support of people who have been there and who have gone through that experience is tremendously helpful.&lt;/p&gt;

&lt;p&gt;So I was able to reach out to a previous release lead. Not to pull the game with&amp;ndash; what is it, like two envelopes? The first envelope, you blame the outgoing president. The second envelope, you write two letters. It&amp;rsquo;s not quite like that.&lt;/p&gt;

&lt;p&gt;I am totally happy to be blamed for all of the changes we made to the release process that didn&amp;rsquo;t go well, but I&amp;rsquo;m also happy to help support my successor. I feel like my job as a release lead is, number one, make sure the release gets out the door, number two, make sure I set up my successor for success.&lt;/p&gt;

&lt;p&gt;So I&amp;rsquo;ve already been meeting with Claire to describe what I would do as the introductory steps. And I plan on continuing to consult with Claire throughout the release process to make sure that things are going well.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: If you want to hear the perspective from some previous release leads, check out &lt;a href=&#34;http://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;episode 10&lt;/a&gt;, where we interview Josh Berkus and Tim Pepper.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What do you plan to put into that set of notes for Claire?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That&amp;rsquo;s a really good question. I would tell Claire to trust her team first and trust her gut second. Like I said, I think it is super important to establish trust with your team, because the release is this superhuman effort that involves consuming, or otherwise fielding, or shepherding the work of hundreds of contributors.&lt;/p&gt;

&lt;p&gt;And your team is made up of at least 13 people. You could go all the way up to 40 or 50, if you include all of the people that are being trained by those people. There&amp;rsquo;s so much work out there. It&amp;rsquo;s just more work than any one person can possibly handle.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s honestly the same thing I will tell new contributors to Kubernetes is that there&amp;rsquo;s no way you can possibly understand all of it. You will not understand the shape of Kubernetes. You will never be the expert who knows literally all of the things, and that&amp;rsquo;s OK. The important part is to make sure that you have people who, when you don&amp;rsquo;t know the answer, you know who to ask for the answer. And it is really helpful if your team are those people.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: The specific version that you&amp;rsquo;ve been working on and the release that&amp;rsquo;s just come out is Kubernetes 1.14. What are some of the new things in this release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: This release of Kubernetes contains more stable enhancements than any other release of Kubernetes ever. And I&amp;rsquo;m pretty proud of that fact. I know in the past you may have heard other release leads talk about, like, this is the stability release, or this time we&amp;rsquo;re really making things a little more mature. But I feel a lot of confidence in saying that this time around.&lt;/p&gt;

&lt;p&gt;Like, I stood in a room, and it was a leadership summit, I think, back in 2017 where we said, look, we&amp;rsquo;re really going to try and make Kubernetes more stable. And we&amp;rsquo;re going to focus on sort of hardening the core of Kubernetes and defining what the core of Kubernetes is. And we&amp;rsquo;re not going to accept a bunch of new features. And then we kind of went and accepted a bunch of new features. And that was a while ago. And here we are today.&lt;/p&gt;

&lt;p&gt;But I think we are finally starting to see the results of work that was started back then. &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190103-windows-node-support.md&#34; target=&#34;_blank&#34;&gt;Windows Server Container Support&lt;/a&gt; is probably the biggest one. You can hear Michael Michael tell stories about how SIG Windows was started about three years ago. And today, they can finally announce that Windows Server containers have gone GA. That&amp;rsquo;s a huge accomplishment.&lt;/p&gt;

&lt;p&gt;A lot of the heavy lifting for this, I believe, came at the end. It started with a conversation in Kubernetes 1.13, and was really wrapped up this release where we define, what are Windows Server containers, exactly? How do they differ from Docker containers or other container runtimes that run on Linux?&lt;/p&gt;

&lt;p&gt;Because today so much of the assumptions people make about the functionality that Kubernetes offers are also baked in with the functionality that Linux-based containers offer. And so we wanted to enable people to use the awesome Kubernetes orchestration capabilities that they have come to love, but to also use that to orchestrate some applications or capabilities that are only available on Windows.&lt;/p&gt;

&lt;p&gt;So we put together what&amp;rsquo;s called a &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps&#34; target=&#34;_blank&#34;&gt;Kubernetes Enhancement Proposal&lt;/a&gt; process, or a KEP, for short. And we said that we&amp;rsquo;re going to use these KEPs to describe exactly what the criteria are to call something alpha, or beta, or stable. And so the Windows feature allowed us to use a KEP&amp;ndash; or in getting Windows in here, we used the KEP to describe everything that would and would not work for Windows Server containers. That was super huge. And that really, I think, helped us better understand or define what Kubernetes is in that context.&lt;/p&gt;

&lt;p&gt;But OK, I&amp;rsquo;ve spent most of the time answering your question with just one single stable feature.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Well, let&amp;rsquo;s dig a little bit in to the KEP process then, because this is the first release where there&amp;rsquo;s a new rule. It says, all proposed enhancements for this release must have an associated KEP. So that&amp;rsquo;s a Kubernetes Enhancement Proposal, a one-page document that describes it. What has the process been like of A, getting engineers on-board with using that, and then B, building something based on these documents?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: It is a process of continued improvement. So it is by no means done, but it honestly required a lot of talking, and saying the same thing over and over to the same people or to different people, as is often the case when it comes to things that involve communication and process changes. But by and large, everybody was pretty much on-board with this.&lt;/p&gt;

&lt;p&gt;There was a little bit of confusion, though, over how high the bar would be set and how rigorously or rigidly we would be enforcing these criteria. And that&amp;rsquo;s where I feel like we have room to iterate and improve on. But we have collectively agreed that, yeah, we do like having all of the information about a particular enhancement in one place. Right?&lt;/p&gt;

&lt;p&gt;The way the world used to operate before is we would throw around Google Docs, that were these design proposals, and then we&amp;rsquo;d comment on those a bunch. And then eventually, those were turned into markdown files. And those would end up in the community repo,&lt;/p&gt;

&lt;p&gt;And then we&amp;rsquo;d have a bunch of associated issues that talked about that. And then maybe somebody would open up another issue that they&amp;rsquo;d call an umbrella issue. And then a bunch of comments would be put there. And then there&amp;rsquo;s lots of discussion that goes on in the PRs. There&amp;rsquo;s like seven different things that I just rattled off there.&lt;/p&gt;

&lt;p&gt;So KEPs are about focusing all of the discussion about the design and implementation and reasoning behind enhancements in one single place. And I think there, we are fully on board. Do we have room to improve? Absolutely. Humans are involved, and it&amp;rsquo;s a messy process. We could definitely find places to automate this better, structure it better. And I look forward to seeing those improvements happen.&lt;/p&gt;

&lt;p&gt;You know, I think another one of the big things was a lot of these KEPs were mired across three different SIGs. There was sort of SIG architecture who had the technical vision for these. There was SIG PM, who&amp;ndash; you know, pick your P of choice&amp;ndash; product, project, process, program, people who are better about how to shepherd things forward, and then SIG release, who just wanted to figure out, what&amp;rsquo;s landing in the release, and why, and how, and why is it important? And so taking the responsibilities across all of those three SIGs and putting it in the right place, which is SIG PM, I think really will help us iterate properly, moving forward.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: The other change in this release is that there is no code slush. &lt;a href=&#34;https://github.com/kubernetes/sig-release/issues/269&#34; target=&#34;_blank&#34;&gt;What is a code slush, and why don&amp;rsquo;t we have one anymore?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That&amp;rsquo;s a really good question. I had 10 different people ask me that question over the past couple of months, quarters, years. Take your pick. And so I finally decided, if nobody knows what a code slush is, why do we even have it?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: It&amp;rsquo;s like a thawed freeze, but possibly with sugar?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: [LAUGHING] So code slush is about&amp;ndash; we want to slow the rate of change prior to code freeze. Like, let&amp;rsquo;s accept code freeze as this big deadline where nothing&amp;rsquo;s going to happen after a code freeze.&lt;/p&gt;

&lt;p&gt;So while I really want to assume and aspire to live in a world where developers are super productive, and start their changes early, and get them done when they&amp;rsquo;re done, today, I happen to live in a world where developers are driven by deadlines. And they get distracted. And there&amp;rsquo;s other stuff going on. And then suddenly, they realize there&amp;rsquo;s a code freeze ahead of them.&lt;/p&gt;

&lt;p&gt;And this wonderful feature that they&amp;rsquo;ve been thinking about implementing over the past two months, they now have to get done in two weeks. And so suddenly, all sorts of code starts to fly in super fast and super quickly. And OK, that&amp;rsquo;s great. I love empowering people to be productive.&lt;/p&gt;

&lt;p&gt;But what we don&amp;rsquo;t want to have happen is somebody decide to land some massive feature or enhancement that changes absolutely everything. Or maybe they decided they want to refactor the world. And if they do that, then they make everybody else&amp;rsquo;s life super difficult because of merge conflicts and rebases. Or maybe all of the test signal that we had kind-of grown accustomed to and gotten used to, completely changes.&lt;/p&gt;

&lt;p&gt;So code slush was about reminding people, hey, don&amp;rsquo;t be jerks. Be kind of responsible. Please try not to land anything super huge at the last minute. But the way that we enforced this was with, like, make sure your PR has a milestone. And make sure that it has priority critical/urgent. In times past, we were like, make sure there is a label called status approved for milestone.&lt;/p&gt;

&lt;p&gt;We were like, what do all these things even mean? People became obsessed with all the labels, and the milestones, and the process. And they never really paid attention to why we&amp;rsquo;re asking people to pay attention to the fact that code freeze was coming soon.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Process for process sake, they could start to build on top of each other. You mentioned that there is a number of other things in the release. Do you want to talk about some of the other pieces that are in there?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Sure. I think two of the other stable features that I believe other people will find to be exciting are &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0007-pod-ready%2B%2B.md&#34; target=&#34;_blank&#34;&gt;readiness gates&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20190131-pod-priority-preemption.md&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt;. Today, Pods have the concept of liveliness and readiness. A live Pod has an application running in it, but it might not be ready to do anything. And so when a Pod is ready, that means it&amp;rsquo;s ready to receive traffic.&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re thinking of some big application that&amp;rsquo;s scaled out everywhere, you want to make sure your Pods are only handling traffic when they&amp;rsquo;re good and ready to do so. But prior to 1.14, the only ways you could verify that were by using either TCP probes, HTTP probes, or exec probes. Either make sure that ports are open inside of the container, or run a command inside of the container and see what that command says.&lt;/p&gt;

&lt;p&gt;And then you can definitely customize a fair amount there, but that requires that you put all of that information inside of the Pod. And it might be really useful for some cluster operators to signify some more overarching concerns that they have before a Pod could be ready. So just&amp;ndash; I don&amp;rsquo;t know&amp;ndash; make sure a Pod has registered with some other system to make sure that it is authorized to serve traffic, or something of that nature. Pod readiness gates allow that sort of capability to happen&amp;ndash; to transparently extend the conditions that you use to figure out whether a Pod is ready for traffic. We believe this will enable more sophisticated orchestration and deployment mechanisms for people who are trying to manage their applications and services.&lt;/p&gt;

&lt;p&gt;I feel like Pod priority and preemption will be interesting to consumers who like to oversubscribe their Kubernetes clusters. Instead of assuming everything is the same size and is the same priority, and first Pods win, you can now say that certain Pods are more important than other Pods. They get scheduled before other Pods, and maybe even so that they kick out other Pods to make room for the really important Pods.&lt;/p&gt;

&lt;p&gt;You could think of it as if you have any super important agents or daemons that have to run on your cluster. Those should always be there. Now, you can describe them as high-priority to make sure that they are definitely always there and always scheduled before anything else is.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Are there any other new features that are in alpha or beta that you&amp;rsquo;re keeping your eye on?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah. So I feel like, on the beta side of things, a lot of what I am interested in&amp;ndash; if I go back to my theme of maturity, and stability, and defining the core of Kubernetes, I think that the storage SIG has been doing amazing work. They continue to ship out, quarter, after quarter, after quarter, after quarter, new and progressive enhancements to storage&amp;ndash; mostly these days through the CSI, Container Storage Interface project, which is fantastic. It allows you to plug in arbitrary pieces of storage functionality.&lt;/p&gt;

&lt;p&gt;They have a number of things related to that that are in beta this time around, such as topology support. So you&amp;rsquo;re going to be able to more accurately express how and where your CSI volumes need to live relative to your application. Block storage support is something I&amp;rsquo;ve heard a number of people asking for, as well as the ability to define &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190124-local-persistent-volumes.md&#34; target=&#34;_blank&#34;&gt;durable local volumes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you&amp;rsquo;re running a Pod on a node, and you want to make sure it&amp;rsquo;s writing directly to the node&amp;rsquo;s local volumes. And that way, it could be super performant. Cool. Give it an emptydir. It&amp;rsquo;ll be fine.&lt;/p&gt;

&lt;p&gt;But if you destroy the Pod, then you lose all the data that the Pod wrote. And so again, I go back to the example of maybe it&amp;rsquo;s an agent, and it&amp;rsquo;s writing a bunch of useful, stateful information to disk. And you&amp;rsquo;d love for the agent to be able to go away and something to replace it, and be able to get all of that information off of disk. Local durable volumes allow you to do that. And you get to do that in the same way that you&amp;rsquo;re used to specifying durable or persistent volumes that are given to you by a cloud provider, for example.&lt;/p&gt;

&lt;p&gt;Since I did co-found SIG testing, I think I have to call out a testing feature that I like. It&amp;rsquo;s really tiny and silly, but it has always bugged me that when you try to download the tests, you download something that&amp;rsquo;s over a gigabyte in size. That&amp;rsquo;s the way things used work for Kubernetes back in the old days for Kubernetes client and server stuff as well. And we have since broken that up into&amp;ndash; you only need to download the binaries that makes sense for your platform.&lt;/p&gt;

&lt;p&gt;So say I&amp;rsquo;m developing Kubernetes on my MacBook. I probably don&amp;rsquo;t need to download the Linux test binaries, or the Windows test binaries, or the ARM64 test binaries, or the s390x test binaries. Did I mention Kubernetes supports a lot of different architectures?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: I hadn&amp;rsquo;t noticed s390 was a supported platform until now.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: It is definitely something that we build binaries for. I&amp;rsquo;m not sure if we&amp;rsquo;ve actually seen a certified conformant Kubernetes that runs on s390, but it is definitely one of the things we build Kubernetes against.&lt;/p&gt;

&lt;p&gt;Not having to download an entire gigabyte plus of binaries just to run some tests is super great. I like to live in a world where I don&amp;rsquo;t have to build the tests from scratch. Can I please just run a program that has all the tests? Maybe I can use that to soak test or sanity test my cluster to make sure that everything is OK. And downloading just the thing that I need is super great.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: You&amp;rsquo;re talking about the idea of Kubernetes having a core and the idea of releases and stability. If you think back to Linux distributions maybe even 10 years ago, we didn&amp;rsquo;t care so much about the version number releases of the kernel anymore, but we cared when there was a new feature in a Red Hat release. Do you think we&amp;rsquo;re getting to that point with Kubernetes at the moment?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I think that is one model that people really hope to see Kubernetes move toward. I&amp;rsquo;m not sure if it is the model that we will move toward, but I think it is an ongoing discussion. So you know, we&amp;rsquo;ve created a working group called &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-lts&#34; target=&#34;_blank&#34;&gt;WG LTS&lt;/a&gt;. I like to call it by its longer name&amp;ndash; WG &amp;ldquo;to LTS, or not to LTS&amp;rdquo;. What does LTS even mean? What are we trying to release and support?&lt;/p&gt;

&lt;p&gt;Because I think that when people think about distributions, they do naturally gravitate towards some distributions have higher velocity release cadences, and others have slower release cadences. And that&amp;rsquo;s cool and great for people who want to live on a piece of software that never ever changes. But those of us who run software at scale find that you can&amp;rsquo;t actually prevent change from happening. There will always be pieces of your infrastructure, or your environment, or your software, that are not under your control.&lt;/p&gt;

&lt;p&gt;And so anything we can do to achieve what I like to call a dynamic stability is probably better for everybody involved. Make the cost of change as low as you possibly can. Make the pain of changing and upgrade as low as you possibly can, and accept that everything will always be changing all the time.&lt;/p&gt;

&lt;p&gt;So yeah. Maybe that&amp;rsquo;s where Linux lives, where the Kernel is always changing. And you can either care about that, or not. And you can go with a distribution that is super up-to-date with the Linux Kernel, or maybe has a slightly longer upgrade cadence. But I think it&amp;rsquo;s about enabling both of those options. Because I think if we try to live in a world where there are only distributions and nothing else, that&amp;rsquo;s going to actually harm everybody in the long term and maybe bring us away from all of these cloud-native ideals that we have, trying to accept change as a constant.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: We can&amp;rsquo;t let you go without talking about the Beard. What is SIG Beard, and how critical was it in you becoming the 1.14 release manager?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I feel like it&amp;rsquo;s a new requirement for all release leads to be a member of SIG Beard. SIG Beard happened because, one day, I realized I had gotten lazy, and I had this just ginormous and magnificent beard. It was really flattering to have Brendan Burns up on stage at KubeCon Seattle compliment my beard in front of an audience of thousands of people. I cannot tell you what that feels like.&lt;/p&gt;

&lt;p&gt;But to be serious for a moment, like OK, I&amp;rsquo;m a dude. I have a beard. There are a lot of dudes who work in tech, and many dudes are bearded. And this is by no means a way of being exclusionary, or calling that out, or anything like that. It was just noticing that while I was on camera, there seemed to be more beard than face at times. And what is that about?&lt;/p&gt;

&lt;p&gt;And I had somebody start referring to me as &amp;ldquo;The Beard&amp;rdquo; in my company. It turns out they read Neil Stevenson&amp;rsquo;s &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cryptonomicon&#34; target=&#34;_blank&#34;&gt;Cryptonomicon&lt;/a&gt;,&amp;rdquo; if you&amp;rsquo;re familiar with that book at all.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: It&amp;rsquo;s a great book.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah. It talks about how you have the beard, and you have the suit. The suit is the person who&amp;rsquo;s responsible for doing all the talking, and the beard is responsible for doing all the walking. And I guess I have gained a reputation for doing an awful lot of walking and showing up in an awful lot of places. And so I thought I would embrace that.&lt;/p&gt;

&lt;p&gt;When I showed up to Google my first day at work where I was looking for the name tag that shows what desk is mine, and my name tag was SIG Beard. And I don&amp;rsquo;t know who did it, but I was like, all right, I&amp;rsquo;m running with it. And so I referred to myself as &amp;ldquo;Aaron of SIG Beard&amp;rdquo; from then on.&lt;/p&gt;

&lt;p&gt;And so to me, the beard is not so much about being bearded on my face, but being bearded at heart&amp;ndash; being welcoming, being fun, embracing this community for all of the awesomeness that it has, and encouraging other people to do the same. So in that regard, I would like to see more people be members of SIG Beard. I&amp;rsquo;m trying to figure out ways to make that happen. And yeah, it&amp;rsquo;s great.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;i&gt;&lt;a href=&#34;http://twitter.com/spiffxp&#34; target=&#34;_blank&#34;&gt;Aaron Crickenberger&lt;/a&gt; is a senior test engineer with &lt;a href=&#34;https://cloud.google.com/&#34; target=&#34;_blank&#34;&gt;Google Cloud&lt;/a&gt;. He co-founded the Kubernetes Testing SIG, has participated in every Kubernetes release since version 1.4, has served on the &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Kubernetes steering committee&lt;/a&gt; since its inception in 2017, and most recently served as the Kubernetes 1.14 release lead.&lt;/p&gt;

&lt;p&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34; target=&#34;_blank&#34;&gt;@kubernetespod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.  Please come and say Hello to us at KubeCon EU!&lt;/i&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Join us for the 2019 KubeCon Diversity Lunch &amp; Hack</title>
      <link>https://kubernetes.io/blog/2019/05/02/kubecon-diversity-lunch-and-hack/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/02/kubecon-diversity-lunch-and-hack/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kiran Oliver, Podcast Producer, The New Stack&lt;/p&gt;

&lt;p&gt;Join us for the 2019 KubeCon Diversity Lunch &amp;amp; Hack: Building Tech Skills &amp;amp; An Inclusive Community - Sponsored by Google Cloud and VMware&lt;/p&gt;

&lt;p&gt;Registration for the Diversity Lunch opens today, May 2nd, 2019. To register, go to the main &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/schedule/&#34;&gt;KubeCon + CloudNativeCon EU schedule&lt;/a&gt;, then log in to your Sched account, and confirm your attendance to the Diversity Lunch. Please sign up ASAP once the link is live, as spaces will fill quickly. We filled the event in just a few days last year, and anticipate doing so again this year.&lt;/p&gt;

&lt;p&gt;The 2019 KubeCon Diversity Lunch &amp;amp; Hack will be held at the Fira Gran Via Barcelona Hall 8.0 Room F1 on May 22nd, 2019 from 12:30-14:00.&lt;/p&gt;

&lt;p&gt;If you’ve never attended a Diversity Lunch before, not to worry. All are welcome, and there’s a variety of things to experience and discuss.&lt;/p&gt;

&lt;p&gt;First things first, let’s establish some ground rules:&lt;/p&gt;

&lt;p&gt;This is a safe space. What does that mean? Simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Asking for and using people’s pronouns&lt;/li&gt;
&lt;li&gt;Absolutely no photography&lt;/li&gt;
&lt;li&gt;Awareness of your actions towards others. Do your best to ensure that you contribute towards making this environment welcoming, safe, and inclusive for all.&lt;/li&gt;
&lt;li&gt;Please avoid tech-heavy arbitrary community slang/jargon [keep in mind that not all of us are developers, many are tech-adjacent and/or new to the community]&lt;/li&gt;
&lt;li&gt;Act with care and empathy towards your fellow community members at all times.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This event also follows the &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/attend/code-of-conduct/&#34;&gt;Code of Conduct&lt;/a&gt; for all CNCF events.&lt;/p&gt;

&lt;p&gt;We have run a very successful diversity lunch event before. This isn’t a trial run, nor is it a proof of concept. We had a fun, productive, and educational conversation last year in Seattle, and hope to do so again this year. As 2018’s KubeCon + CloudNativeCon in Seattle marked our first Diversity Lunch with pair programming, we hammered out a lot of kinks post-mortem, using that feedback to inform and improve upon our decision making, planning, and organizational process moving forward, to bring you an improved experience at the 2019 KubeCon + CloudNativeCon Diversity Lunch.&lt;/p&gt;

&lt;p&gt;Tables not related to pair-programming or hands-on Kubernetes will be led by a moderator, where notes and feedback will then be taken and shared at the end of the lunch and in a post-mortem discussion after KubeCon+CloudNativeCon Barcelona ends, as part of our continuous improvement process. Some of last year’s tables were dedicated to topics that were submitted at registration, such as: security, D&amp;amp;I, service meshes, and more. You can suggest your own table topic on the registration form this year as well, and we highly encourage you to do so, particularly if you do not see your preferred topic or activity of choice listed. Your suggestions will then be used to determine the discussion table tracks that will be available at this year’s Diversity Lunch &amp;amp; Hack.&lt;/p&gt;

&lt;p&gt;We hope you are also excited to participate in the ‘Hack’ portion of this ‘Lunch and Hack.’ This breakout track will include a variety of peer-programming exercises led by your fellow Kubernetes community members, with discussion leads working together with attendees hands-on to solve their Kubernetes-related problems in a welcoming, safe environment.&lt;/p&gt;

&lt;p&gt;To make this all possible, we need you. Yes, you, to register. As much as we love having groups of diverse people all gather in the same room, we also need allies. If you’re a member of a privileged group or majority, you are welcome and encouraged to join us. Most importantly, we want you to take what you learn and experience at the Diversity Lunch back to both your companies and your open source communities, so that you can help us make positive changes not only within our industry, but beyond. No-one lives [or works] in a bubble. We hope that the things you learn here will carry over and bring about positive change in the world as a whole.&lt;/p&gt;

&lt;p&gt;We look forward to seeing you!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Special thanks to &lt;a href=&#34;https://www.linkedin.com/in/leahstunts/&#34; target=&#34;_blank&#34;&gt;Leah Petersen&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sarah-conway-6166151/&#34; target=&#34;_blank&#34;&gt;Sarah Conway&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/parispittman/&#34; target=&#34;_blank&#34;&gt;Paris Pittman&lt;/a&gt; for their help in editing this post.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: How You Can Help Localize Kubernetes Docs</title>
      <link>https://kubernetes.io/blog/2019/04/26/how-you-can-help-localize-kubernetes-docs/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/26/how-you-can-help-localize-kubernetes-docs/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author: Zach Corleissen (Linux Foundation)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Last year we optimized the Kubernetes website for &lt;a href=&#34;https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/&#34;&gt;hosting multilingual content&lt;/a&gt;. Contributors responded by adding multiple new localizations: as of April 2019, Kubernetes docs are partially available in nine different languages, with six added in 2019 alone. You can see a list of available languages in the language selector at the top of each page.&lt;/p&gt;

&lt;p&gt;By &lt;em&gt;partially available&lt;/em&gt;, I mean that localizations are ongoing projects. They range from mostly complete (&lt;a href=&#34;https://v1-12.docs.kubernetes.io/zh/&#34; target=&#34;_blank&#34;&gt;Chinese docs for 1.12&lt;/a&gt;) to brand new (1.14 docs in &lt;a href=&#34;https://kubernetes.io/pt/&#34; target=&#34;_blank&#34;&gt;Portuguese&lt;/a&gt;). If you&amp;rsquo;re interested in helping an existing localization, read on!&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-localization&#34;&gt;What is a localization?&lt;/h2&gt;

&lt;p&gt;Translation is about words and meaning. Localization is about words, meaning, process, and design.&lt;/p&gt;

&lt;p&gt;A localization is like a translation, but more thorough. Instead of just translating words, a localization optimizes the framework for writing and publishing words. For example, most site navigation features (button text) on kubernetes.io are strings contained in a &lt;a href=&#34;https://github.com/kubernetes/website/tree/master/i18n&#34; target=&#34;_blank&#34;&gt;single file&lt;/a&gt;. Part of creating a new localization involves adding a language-specific version of that file and translating the strings it contains.&lt;/p&gt;

&lt;p&gt;Localization matters because it reduces barriers to adoption and support. When we can read Kubernetes docs in our own language, it&amp;rsquo;s easier to get started using Kubernetes and contributing to its development.&lt;/p&gt;

&lt;h2 id=&#34;how-do-localizations-happen&#34;&gt;How do localizations happen?&lt;/h2&gt;

&lt;p&gt;The availability of docs in different languages is a feature&amp;mdash;and like all Kubernetes features, contributors develop localized docs in a SIG, share them for review, and add them to the project.&lt;/p&gt;

&lt;p&gt;Contributors work in teams to localize content. Because folks can&amp;rsquo;t approve their own PRs, localization teams have a minimum size of two&amp;mdash;for example, the Italian localization has two contributors. Teams can also be quite large: the Chinese team has several dozen contributors.&lt;/p&gt;

&lt;p&gt;Each team has its own workflow. Some teams localize all content manually; others use editors with translation plugins and review machine output for accuracy. SIG Docs focuses on standards of output; this leaves teams free to adopt the workflow that works best for them. That said, teams frequently collaborate with each other on best practices, and sharing abounds in the best spirit of the Kubernetes community.&lt;/p&gt;

&lt;h2 id=&#34;helping-with-localizations&#34;&gt;Helping with localizations&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re interested in starting a new localization for Kubernetes docs, the &lt;a href=&#34;https://kubernetes.io/docs/contribute/localization/&#34; target=&#34;_blank&#34;&gt;Kubernetes contribution guide&lt;/a&gt; shows you how.&lt;/p&gt;

&lt;p&gt;Existing localizations also need help. If you&amp;rsquo;d like to contribute to an existing project, join the localization team&amp;rsquo;s Slack channel and introduce yourself. Folks on that team can help you get started.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Localization&lt;/th&gt;
&lt;th&gt;Slack channel&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Chinese (中文)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CE3LNFYJ1/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-zh&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/C1J0BPD2M/&#34; target=&#34;_blank&#34;&gt;#sig-docs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;French (Français)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CG838BFT9/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-fr&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;German (Deutsch)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH4UJ2BAL/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-de&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hindi&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-hi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Indonesian&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ1LUCUHM/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-id&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Italian&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CGB1MCK7X/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-it&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Japanese (日本語)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CAG2M83S8/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ja&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Korean (한국어)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CA1MMR86S/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ko&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Portuguese (Português)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ21AS0NA/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-pt&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Spanish (Español)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH7GB2E3B/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-es&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a new &lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;Hindi localization&lt;/a&gt; beginning. Why not add your language, too?&lt;/p&gt;

&lt;p&gt;As a chair of SIG Docs, I&amp;rsquo;d love to see localization spread beyond the docs and into Kubernetes components. Is there a Kubernetes component you&amp;rsquo;d like to see supported in a different language? Consider making a &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps&#34; target=&#34;_blank&#34;&gt;Kubernetes Enhancement Proposal&lt;/a&gt; to support the change.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Hardware Accelerated SSL/TLS Termination in Ingress Controllers using Kubernetes Device Plugins and RuntimeClass</title>
      <link>https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mikko Ylinen (Intel)&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;A Kubernetes Ingress is a way to connect cluster services to the world outside the cluster. In order
to correctly route the traffic to service backends, the cluster needs an Ingress controller. The
Ingress controller is responsible for setting the right destinations to backends based on the
Ingress API objects’ information. The actual traffic is routed through a proxy server that
is responsible for tasks such as load balancing and SSL/TLS (later “SSL” refers to both SSL
or TLS ) termination. The SSL termination is a CPU heavy operation due to the crypto operations
involved. To offload some of the CPU intensive work away from the CPU, OpenSSL based proxy
servers can take the benefit of OpenSSL Engine API and dedicated crypto hardware. This frees
CPU cycles for other things and improves the overall throughput of the proxy server.&lt;/p&gt;

&lt;p&gt;In this blog post, we will show how easy it is to make hardware accelerated crypto available
for containers running the Ingress controller proxy using some of the recently created Kubernetes
building blocks: Device plugin framework and RuntimeClass. At the end, a reference setup is given
using an HAproxy based Ingress controller accelerated using Intel&amp;reg; QuickAssist Technology cards.&lt;/p&gt;

&lt;h2 id=&#34;about-proxies-openssl-engine-and-crypto-hardware&#34;&gt;About Proxies, OpenSSL Engine and Crypto Hardware&lt;/h2&gt;

&lt;p&gt;The proxy server plays a vital role in a Kubernetes Ingress Controller function. It proxies
the traffic to the backends per Ingress objects routes. Under heavy traffic load, the performance
becomes critical especially if the proxying involves CPU intensive operations like SSL crypto.&lt;/p&gt;

&lt;p&gt;The OpenSSL project provides the widely adopted library for implementing the SSL protocol. Of
the commonly known proxy servers used by Kubernetes Ingress controllers, Nginx and HAproxy use
OpenSSL. The CNCF graduated Envoy proxy uses BoringSSL but there seems to be &lt;a href=&#34;https://github.com/envoyproxy/envoy/pull/5161#issuecomment-446374130&#34; target=&#34;_blank&#34;&gt;community interest
in having OpenSSL as the alternative&lt;/a&gt; for it too.&lt;/p&gt;

&lt;p&gt;The OpenSSL SSL protocol library relies on libcrypto that implements the cryptographic functions.
For quite some time now (first introduced in 0.9.6 release), OpenSSL has provided an &lt;a href=&#34;https://github.com/openssl/openssl/blob/master/README.ENGINE&#34; target=&#34;_blank&#34;&gt;ENGINE
concept&lt;/a&gt; that allows these cryptographic operations to be offloaded to a dedicated crypto
acceleration hardware. Later, a special &lt;em&gt;dynamic&lt;/em&gt; ENGINE enabled the crypto hardware specific
pieces to be implemented in an independent loadable module that can be developed outside the
OpenSSL code base and distributed separately. From the application’s perspective, this is also
ideal because they don’t need to know the details of how to use the hardware, and the hardware
specific module can be loaded/used when the hardware is available.&lt;/p&gt;

&lt;p&gt;Hardware based crypto can greatly improve Cloud applications’ performance due to hardware
accelerated processing in SSL operations as discussed, and can provide other crypto
services like key/random number generation. Clouds can make the hardware easily available
using the dynamic ENGINE and several loadable module implementations exist, for
example, &lt;a href=&#34;https://docs.aws.amazon.com/cloudhsm/latest/userguide/openssl-library.html&#34; target=&#34;_blank&#34;&gt;CloudHSM&lt;/a&gt;, &lt;a href=&#34;https://github.com/opencryptoki/openssl-ibmca&#34; target=&#34;_blank&#34;&gt;IBMCA&lt;/a&gt;, or &lt;a href=&#34;https://github.com/intel/QAT_Engine/&#34; target=&#34;_blank&#34;&gt;QAT Engine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For Cloud deployments, the ideal scenario is for these modules to be shipped as part of
the container workload. The workload would get scheduled on a node that provides the
underlying hardware that the module needs to access. On the other hand, the workloads
should run the same way and without code modifications regardless of the crypto acceleration
hardware being available or not. The OpenSSL dynamic engine enables this. Figure 1 below
illustrates these two scenarios using a typical Ingress Controller container as an example.
The red colored boxes indicate the differences between a container with a crypto hardware
engine enabled container vs. a “standard” one. It’s worth pointing out that the configuration
changes shown do not necessarily require another version of the container since the configurations
could be managed, e.g., using ConfigMaps.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig1.png&#34;
         alt=&#34;Figure 1. Examples of Ingress controller containers&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 1. Examples of Ingress controller containers&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;hardware-resources-and-isolation&#34;&gt;Hardware Resources and Isolation&lt;/h2&gt;

&lt;p&gt;To be able to deploy workloads with hardware dependencies, Kubernetes provides excellent extension
and configurability mechanisms. Let’s take a closer look into Kubernetes the &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/&#34; target=&#34;_blank&#34;&gt;device plugin framework&lt;/a&gt;
(beta in 1.14) and &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/runtime-class/&#34; target=&#34;_blank&#34;&gt;RuntimeClass&lt;/a&gt; (beta in 1.14) and learn how they can be leveraged to expose crypto
hardware to workloads.&lt;/p&gt;

&lt;p&gt;The device plugin framework, first introduced in Kubernetes 1.8, provides a way for hardware vendors
to register and allocate node hardware resources to Kubelets. The plugins implement the hardware
specific initialization logic and resource management. The pods can request hardware resources in
their PodSpec, which also guarantees the pod is scheduled on a node that can provide those resources.&lt;/p&gt;

&lt;p&gt;The device resource allocation for containers is non-trivial. For applications dealing with security,
the hardware level isolation is critical. The PCIe based crypto acceleration device functions can
benefit from IO hardware virtualization, through an I/O Memory Management Unit (IOMMU), to provide
the isolation: an &lt;em&gt;IOMMU group&lt;/em&gt; the device belongs to provides the isolated resource for a workload
(assuming the crypto cards do not share the IOMMU group with other devices). The number of isolated
resources can be further increased if the PCIe device supports the Single-Root I/O Virtualization
(SR-IOV) specification. SR-IOV allows the PCIe device to be split further to &lt;em&gt;virtual functions&lt;/em&gt; (VF),
derived from &lt;em&gt;physical function&lt;/em&gt; (PF) devices, and each belonging to their own IOMMU group. To expose
these IOMMU isolated device functions to user space and containers, the host kernel should bind
them to a specific device driver. In Linux, this  driver is vfio-pci and it makes each device
available through a character device in user space. The kernel vfio-pci driver provides user space
applications with a direct, IOMMU backed access to PCIe devices and functions, using a mechanism
called &lt;em&gt;PCI passthrough&lt;/em&gt;. The interface can be leveraged by user space frameworks, such as the
Data Plane Development Kit (DPDK). Additionally, virtual machine (VM) hypervisors can provide
these user space device nodes to VMs and expose them as PCI devices to the guest kernel.
Assuming support from the guest kernel, the VM gets close to native performant direct access to the
underlying host devices.&lt;/p&gt;

&lt;p&gt;To advertise these device resources to Kubernetes, we can have a simple Kubernetes device plugin
that runs the initialization (i.e., binding), calls kubelet’s &lt;code&gt;Registration&lt;/code&gt; gRPC service, and
implements the DevicePlugin gRPC service that kubelet calls to, e.g., to &lt;code&gt;Allocate&lt;/code&gt; the resources
upon Pod creation.&lt;/p&gt;

&lt;h2 id=&#34;device-assignment-and-pod-deployment&#34;&gt;Device Assignment and Pod Deployment&lt;/h2&gt;

&lt;p&gt;At this point, you may ask what the container could do with a VFIO device node? The answer comes
after we first take a quick look into the Kubernetes RuntimeClass.&lt;/p&gt;

&lt;p&gt;The Kubernetes RuntimeClass was created to provide better control and configurability
over a variety of &lt;em&gt;runtimes&lt;/em&gt; (an earlier &lt;a href=&#34;https://kubernetes.io/blog/2018/10/10/kubernetes-v1.12-introducing-runtimeclass/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; goes into the details of the needs,
status and roadmap for it) that are available in the cluster. In essence, the RuntimeClass
provides cluster users better tools to pick and use the runtime that best suits for the pod use case.&lt;/p&gt;

&lt;p&gt;The OCI compatible &lt;a href=&#34;https://katacontainers.io/&#34; target=&#34;_blank&#34;&gt;Kata Containers runtime&lt;/a&gt; provides workloads with a hardware virtualized
isolation layer. In addition to workload isolation, the Kata Containers VM has the added
side benefit that the VFIO devices, as &lt;code&gt;Allocate&lt;/code&gt;’d by the device plugin, can be passed
through to the container as hardware isolated devices. The only requirement is that the
Kata Containers kernel has driver for the exposed device enabled.&lt;/p&gt;

&lt;p&gt;That’s all it really takes to enable hardware accelerated crypto for container workloads. To summarize:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cluster needs a device plugin running on the node that provides the hardware&lt;/li&gt;
&lt;li&gt;Device plugin exposes the hardware to user space using  the VFIO driver&lt;/li&gt;
&lt;li&gt;Pod requests the device resources and Kata Containers as the RuntimeClass in the PodSpec&lt;/li&gt;
&lt;li&gt;The container has the hardware adaptation library and the OpenSSL engine module&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 2 shows the overall setup using the Container A illustrated earlier.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig2.png&#34;
         alt=&#34;Figure 2. Deployment overview&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 2. Deployment overview&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;reference-setup&#34;&gt;Reference Setup&lt;/h2&gt;

&lt;p&gt;Finally, we describe the necessary building blocks and steps to build a functional
setup described in Figure 2 that enables hardware accelerated SSL termination in
an Ingress Controller using an Intel&amp;reg; QuickAssist Technology (QAT) PCIe device.
It should be noted that the use cases are not limited to Ingress controllers, but
any OpenSSL based workload can be accelerated.&lt;/p&gt;

&lt;h3 id=&#34;cluster-configuration&#34;&gt;Cluster configuration:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.14 (&lt;code&gt;RuntimeClass&lt;/code&gt; and &lt;code&gt;DevicePlugin&lt;/code&gt; feature gates enabled (both are &lt;code&gt;true&lt;/code&gt; in 1.14)&lt;/li&gt;
&lt;li&gt;RuntimeClass ready runtime and Kata Containers configured&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;host-configuration&#34;&gt;Host configuration:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Intel&amp;reg; QAT driver release with the kernel drivers installed for both host kernel and Kata Containers kernel (or on a rootfs as loadable modules)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/intel/intel-device-plugins-for-kubernetes/tree/master/cmd/qat_plugin&#34; target=&#34;_blank&#34;&gt;QAT device plugin&lt;/a&gt; DaemonSet deployed&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ingress-controller-configuration-and-deployment&#34;&gt;Ingress controller configuration and deployment:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress&#34; target=&#34;_blank&#34;&gt;HAproxy-ingress&lt;/a&gt; ingress controller in a modified container that has

&lt;ul&gt;
&lt;li&gt;the QAT HW HAL user space library (part of Intel&amp;reg; QAT SW release) and&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://github.com/intel/QAT_Engine/&#34; target=&#34;_blank&#34;&gt;OpenSSL QAT Engine&lt;/a&gt; built in&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Haproxy-ingress ConfigMap to enable QAT engine usage

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ssl-engine=”qat”&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ssl-mode-async=true&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Haproxy-ingress deployment &lt;code&gt;.yaml&lt;/code&gt; to

&lt;ul&gt;
&lt;li&gt;Request &lt;code&gt;qat.intel.com: n&lt;/code&gt; resources&lt;/li&gt;
&lt;li&gt;Request &lt;code&gt;runtimeClassName: kata-containers&lt;/code&gt; (name value depends on cluster config)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;(QAT device config file for each requested device resource with OpenSSL engine configured available in the container)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once the building blocks are available, the hardware accelerated SSL/TLS can be tested by following the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/tls-termination&#34; target=&#34;_blank&#34;&gt;TLS termination
example&lt;/a&gt; steps. In order to verify the hardware is used, you can check &lt;code&gt;/sys/kernel/debug/*/fw_counters&lt;/code&gt; files on host as they
get updated by the Intel&amp;reg; QAT firmware.&lt;/p&gt;

&lt;p&gt;Haproxy-ingress and HAproxy are used because HAproxy can be directly configured to use the OpenSSL engine using
&lt;code&gt;ssl-engine &amp;lt;name&amp;gt; [algo ALGOs]&lt;/code&gt; configuration flag without modifications to the global openssl configuration file.
Moreover, HAproxy can offload configured algorithms using asynchronous calls (with &lt;code&gt;ssl-mode-async&lt;/code&gt;) to further improve performance.&lt;/p&gt;

&lt;h2 id=&#34;call-to-action&#34;&gt;Call to Action&lt;/h2&gt;

&lt;p&gt;In this blog post we have shown how Kubernetes Device Plugins and RuntimeClass can be used to provide isolated hardware
access for applications in pods to offload crypto operations to hardware accelerators. Hardware accelerators can be used
to speed up crypto operations and also save CPU cycles to other tasks. We demonstrated the setup using HAproxy that already
supports asynchronous crypto offload with OpenSSL.&lt;/p&gt;

&lt;p&gt;The next steps for our team is to repeat the same for Envoy (with an OpenSSL based TLS transport socket built
as an extension). Furthermore, we are working to enhance Envoy to be able to &lt;a href=&#34;https://github.com/envoyproxy/envoy/issues/6248&#34; target=&#34;_blank&#34;&gt;offload BoringSSL asynchronous
private key operations&lt;/a&gt; to a crypto acceleration hardware. Any review feedback or help is appreciated!&lt;/p&gt;

&lt;p&gt;How many CPU cycles can your crypto application save for other tasks when offloading crypto processing to a dedicated accelerator?&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing kube-iptables-tailer: Better Networking Visibility in Kubernetes Clusters</title>
      <link>https://kubernetes.io/blog/2019/04/19/introducing-kube-iptables-tailer/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/19/introducing-kube-iptables-tailer/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Saifuding Diliyaer, Software Engineer, Box&lt;/p&gt;

&lt;p&gt;At Box, we use Kubernetes to empower our engineers to own the whole lifecycle of their microservices. When it comes to networking, our engineers use Tigera’s &lt;a href=&#34;https://www.tigera.io/tigera-calico/&#34; target=&#34;_blank&#34;&gt;Project Calico&lt;/a&gt; to declaratively manage network policies for their apps running in our Kubernetes clusters. App owners define a Calico policy in order to enable their Pods to send/receive network traffic, which is instantiated as iptables rules.&lt;/p&gt;

&lt;p&gt;There may be times, however, when such network policy is missing or declared incorrectly by app owners. In this situation, the iptables rules will cause network packet drops between the affected Pods, which get logged in a file that is inaccessible to app owners. We needed a mechanism to seamlessly deliver alerts about those iptables packet drops based on their network policies to help app owners quickly diagnose the corresponding issues. To solve this, we developed a service called &lt;a href=&#34;https://github.com/box/kube-iptables-tailer&#34; target=&#34;_blank&#34;&gt;kube-iptables-tailer&lt;/a&gt; to detect packet drops from iptables logs and report them as Kubernetes events. We are proud to open-source kube-iptables-tailer for you to utilize in your own cluster, regardless of whether you use Calico or other network policy tools.&lt;/p&gt;

&lt;h2 id=&#34;improved-experience-for-app-owners&#34;&gt;Improved Experience for App Owners&lt;/h2&gt;

&lt;p&gt;App owners do not have to apply any additional changes to utilize kube-iptables-tailer. They can simply run &lt;code&gt;kubectl describe pods&lt;/code&gt; to check if any of their Pods&amp;rsquo; traffic has been dropped due to iptables rules. All the results sent from kube-iptables-tailer will be shown under the &lt;em&gt;Events&lt;/em&gt; section, which is a much better experience for developers when compared to reading through raw iptables logs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kubectl describe pods --namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;YOUR_NAMESPACE

...
Events:
 Type     Reason      Age    From                    Message
 ----     ------      ----   ----                    -------    
 Warning  PacketDrop  5s     kube-iptables-tailer    Packet dropped when receiving traffic from example-service-2 &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;IP: &lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;.222.22.222&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.

 Warning  PacketDrop  10m    kube-iptables-tailer    Packet dropped when sending traffic to example-service-1 &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;IP: &lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;.111.11.111&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;* output of events sent from kube-iptables-tailer to Kubernetes Pods having networking issues&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;process-behind-kube-iptables-tailer&#34;&gt;Process behind kube-iptables-tailer&lt;/h2&gt;

&lt;p&gt;Before we had kube-iptables-tailer, the only way for Box’s engineers to get information about packet drops related to their network policies was parsing through the raw iptables logs and matching their service IPs. This was a suboptimal experience because iptables logs only contain basic IP address information. Mapping these IPs to specific Pods could be painful, especially in the Kubernetes world where Pods and containers are ephemeral and IPs are frequently changing. This process involved a bunch of manual commands for our engineers. Additionally, iptables logs could be noisy due to a number of drops, and if IP addresses were being reused, the app owners might even have some stale data. With the help of kube-iptables-tailer, life now becomes much easier for our developers. As shown in the following diagram, the principle of this service can be divided into three steps:
&lt;img src=&#34;https://i.imgur.com/fGAIVuS.png&#34; alt=&#34;sequence diagram for kube-iptables-tailer&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;* sequence diagram for kube-iptables-tailer&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-watch-changes-on-iptables-log-file&#34;&gt;1. Watch changes on iptables log file&lt;/h3&gt;

&lt;p&gt;Instead of requiring human engineers to manually decipher the raw iptables logs, we now use kube-iptables-tailer to help identify changes in that file. We run the service as a &lt;strong&gt;DaemonSet&lt;/strong&gt; on every host node in our cluster, and it tails the iptables log file periodically. The service itself is written in Go, and it has multiple goroutines for the different service components running concurrently. We use channels to share information among those various components. In this step, for instance, the service will send out any changes it detected in iptables log file to a Go channel to be parsed later.&lt;/p&gt;

&lt;h3 id=&#34;2-parse-iptables-logs-based-on-log-prefix&#34;&gt;2. Parse iptables logs based on log prefix&lt;/h3&gt;

&lt;p&gt;Once the parser receives a new log message through a particular Go channel, it will first check whether the log message includes any network policy related packet drop information by parsing the log prefix. Packet drops based on our Calico policies will be logged containing “calico-drop:” as the log prefix in iptables log file. In this case, an object will be created by the parser with the data from the log message being stored as the object’s fields. These handy objects will be later used to locate the relevant Pods running in Kubernetes and post notifications directly to them. The parser is also able to identify duplicate logs and filter them to avoid causing confusion and consuming extra resources. After the parsing process, it will come to the final step for kube-iptables-tailer to send out the results.&lt;/p&gt;

&lt;h3 id=&#34;3-locate-pods-and-send-out-events&#34;&gt;3. Locate pods and send out events&lt;/h3&gt;

&lt;p&gt;Using the Kubernetes API, kube-iptables-tailer will try locating both senders and receivers in our cluster by matching the IPs stored in objects parsed from the previous step. As a result, an event will be posted to these affected Pods if they are located successfully. Kubernetes events are objects designed to provide information about what is happening inside a Kubernetes component. At Box, one of the use cases for Kubernetes events is to report errors directly to the corresponding applications (for more details, please refer to this &lt;a href=&#34;https://kubernetes.io/blog/2018/01/reporting-errors-using-kubernetes-events/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;). The event generated by kube-iptables-tailer includes useful information such as traffic direction, IPs and the namespace of Pods from the other side. We have added DNS lookup as well because our Pods also send and receive traffic from services running on bare-metal hosts and VMs. Besides, exponential backoff is implemented to avoid overwhelming the Kubernetes API server.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;At Box, kube-iptables-tailer has saved time as well as made life happier for many developers across various teams. Instead of flying blind with regards to packet drops based on network policies, the service is able to help detect changes in iptables log file and get the corresponding information delivered right to the Pods inside Kubernetes clusters. If you’re not using Calico, you can still apply any other log prefix (configured as an environment variable in the service) to match whatever is defined in your iptables rules and get notified about the network policy related packet drops. You may also find other cases where it is useful to make information from host systems available to Pods via the Kubernetes API. As an open-sourced project, every contribution is more than welcome to help improve the project together. You can find this project hosted on Github at &lt;a href=&#34;https://github.com/box/kube-iptables-tailer&#34; target=&#34;_blank&#34;&gt;https://github.com/box/kube-iptables-tailer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Special thanks to &lt;a href=&#34;https://www.linkedin.com/in/kunalparmar/&#34; target=&#34;_blank&#34;&gt;Kunal Parmar&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/greg-lyons-8277a188/&#34; target=&#34;_blank&#34;&gt;Greg Lyons&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/shrenikd/&#34; target=&#34;_blank&#34;&gt;Shrenik Dedhia&lt;/a&gt; for contributing to this project.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The Future of Cloud Providers in Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Andrew Sy Kim (VMware), Mike Crute (AWS), Walter Fender (Google)&lt;/p&gt;

&lt;p&gt;Approximately 9 months ago, the Kubernetes community agreed to form the Cloud Provider Special Interest Group (SIG). The justification was to have a single governing SIG to own and shape the integration points between Kubernetes and the many cloud providers it supported. A lot has been in motion since then and we’re here to share with you what has been accomplished so far and what we hope to see in the future.&lt;/p&gt;

&lt;h2 id=&#34;the-mission&#34;&gt;The Mission&lt;/h2&gt;

&lt;p&gt;First and foremost, I want to share what the mission of the SIG is, because we use it to guide our present &amp;amp; future work. Taken straight from our &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-cloud-provider/CHARTER.md&#34; target=&#34;_blank&#34;&gt;charter&lt;/a&gt;, the mission of the SIG is to simplify, develop and maintain cloud provider integrations as extensions, or add-ons, to Kubernetes clusters. The motivation behind this is two-fold: to ensure Kubernetes remains extensible and cloud agnostic.&lt;/p&gt;

&lt;h2 id=&#34;the-current-state-of-cloud-providers&#34;&gt;The Current State of Cloud Providers&lt;/h2&gt;

&lt;p&gt;In order to gain a forward looking perspective to our work, I think it’s important to take a step back to look at the current state of cloud providers. Today, each core Kubernetes component (except the scheduler and kube-proxy) has a &amp;ndash;cloud-provider flag you can configure to enable a set of functionalities that integrate with the underlying infrastructure provider, a.k.a the cloud provider. Enabling this integration unlocks a wide set of features for your clusters such as: node address &amp;amp; zone discovery, cloud load balancers for Services with Type=LoadBalancer, IP address management, and cluster networking via VPC routing tables. Today, the cloud provider integrations can be done either in-tree or out-of-tree.&lt;/p&gt;

&lt;h2 id=&#34;in-tree-out-of-tree-providers&#34;&gt;In-Tree &amp;amp; Out-of-Tree Providers&lt;/h2&gt;

&lt;p&gt;In-tree cloud providers are the providers we develop &amp;amp; release in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/pkg/cloudprovider/providers&#34; target=&#34;_blank&#34;&gt;main Kubernetes repository&lt;/a&gt;. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/docs/pre-ccm-arch.png&#34;
         alt=&#34;In-Tree Cloud Provider Architecture (source: kubernetes.io)&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;In-Tree Cloud Provider Architecture (source: kubernetes.io)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Out-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/docs/post-ccm-arch.png&#34;
         alt=&#34;Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;When cloud provider integrations were initially developed, they were developed natively (in-tree). We integrated each provider close to the core of Kubernetes and within the monolithic repository that is k8s.io/kubernetes today. As Kubernetes became more ubiquitous and more infrastructure providers wanted to support Kubernetes natively, we realized that this model was not going to scale. Each provider brings along a large set of dependencies which increases potential vulnerabilities in our code base and significantly increases the binary size of each component. In addition to this, more of the Kubernetes release notes started to focus on provider specific changes rather than core changes that impacted all Kubernetes users.&lt;/p&gt;

&lt;p&gt;In late 2017, we developed a way for cloud providers to build integrations without adding them to the main Kubernetes tree (out-of-tree). This became the de-facto way for new infrastructure providers in the ecosystem to integrate with Kubernetes.  Since then, we’ve been actively working towards migrating all cloud providers to use the out-of-tree architecture as most clusters today are still using the in-tree cloud providers.&lt;/p&gt;

&lt;h2 id=&#34;looking-ahead&#34;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;Looking ahead, the goal of the SIG is to remove all existing in-tree cloud providers in favor of their out-of-tree equivalents with minimal impact to users. In addition to the core cloud provider integration mentioned above, there are more extension points for cloud integrations like CSI and the image credential provider that are actively being worked on for v1.15. Getting to this point would mean that Kubernetes is truly cloud-agnostic with no native integrations for any cloud provider. By doing this work we empower each cloud provider to develop and release new versions at their own cadence independent of Kubernetes.  We’ve learned by now that this is a large feat with a unique set of challenges. Migrating workloads is never easy, especially when it’s an essential part of the control plane. Providing a safe and easy migration path between in-tree and out-of-tree cloud providers is of the highest priority for our SIG in the upcoming releases. If any of this sounds interesting to you, I encourage you to check out of some of our &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider&#34; target=&#34;_blank&#34;&gt;KEPs&lt;/a&gt; and get in touch with our SIG by joining the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-cloud-provider&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt; or our slack channel (#sig-cloud-provider in Kubernetes slack).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Pod Priority and Preemption in Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/04/16/pod-priority-and-preemption-in-kubernetes/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/16/pod-priority-and-preemption-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Bobby Salamat&lt;/p&gt;

&lt;p&gt;Kubernetes is well-known for running scalable workloads. It scales your workloads based on their resource usage. When a workload is scaled up, more instances of the application get created. When the application is critical for your product, you want to make sure that these new instances are scheduled even when your cluster is under resource pressure. One obvious solution to this problem is to over-provision your cluster resources to have some amount of slack resources available for scale-up situations. This approach often works, but costs more as you would have to pay for the resources that are idle most of the time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt; is a scheduler feature made generally available in Kubernetes 1.14 that allows you to achieve high levels of scheduling confidence for your critical workloads without overprovisioning your clusters. It also provides a way to improve resource utilization in your clusters without sacrificing the reliability of your essential workloads.&lt;/p&gt;

&lt;h2 id=&#34;guaranteed-scheduling-with-controlled-cost&#34;&gt;Guaranteed scheduling with controlled cost&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling&#34; target=&#34;_blank&#34;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt; is an excellent tool in the ecosystem which adds more nodes to your cluster when your applications need them. However, cluster autoscaler has some limitations and may not work for all users:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It does not work in physical clusters.&lt;/li&gt;
&lt;li&gt;Adding more nodes to the cluster costs more.&lt;/li&gt;
&lt;li&gt;Adding nodes is not instantaneous and could take minutes before those nodes become available for scheduling.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An alternative is Pod Priority and Preemption. In this approach, you combine multiple workloads in a single cluster. For example, you may run your CI/CD pipeline, ML workloads, and your critical service in the same cluster. When multiple workloads run in the same cluster, the size of your cluster is larger than a cluster that you would use to run only your critical service. If you give your critical service the highest priority and your CI/CD and ML workloads lower priority, when your service needs more computing resources, the scheduler preempts (evicts) enough pods of your lower priority workloads, e.g., ML workload, to allow all your higher priority pods to schedule.&lt;/p&gt;

&lt;p&gt;With pod priority and preemption you can set a maximum size for your cluster in the Autoscaler configuration to ensure your costs get controlled without sacrificing availability of your service. Moreover, preemption is much faster than adding new nodes to the cluster. Within seconds your high priority pods are scheduled, which is critical for latency sensitive services.&lt;/p&gt;

&lt;h2 id=&#34;improve-cluster-resource-utilization&#34;&gt;Improve cluster resource utilization&lt;/h2&gt;

&lt;p&gt;Cluster operators who run critical services learn over time a rough estimate of the number of nodes that they need in their clusters to achieve high service availability. The estimate is usually conservative. Such estimates take bursts of traffic into account to find the number of required nodes. Cluster autoscaler can be configured never to reduce the size of the cluster below this level. The only problem is that such estimates are often conservative and cluster resources may remain underutilized most of the time. Pod priority and preemption allows you to improve resource utilization significantly by running a non-critical workload in the cluster.&lt;/p&gt;

&lt;p&gt;The non-critical workload may have many more pods that can fit in the cluster. If you give a negative priority to your non-critical workload, Cluster Autoscaler does not add more nodes to your cluster when the non-critical pods are pending. Therefore, you won’t incur higher expenses. When your critical workload requires more computing resources, the scheduler preempts non-critical pods and schedules critical ones.&lt;/p&gt;

&lt;p&gt;The non-critical pods fill the “holes” in your cluster resources which improves resource utilization without raising your costs.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-scheduling&#34; target=&#34;_blank&#34;&gt;Scheduling Special Interest Group&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Process ID Limiting for Stability Improvements in Kubernetes 1.14</title>
      <link>https://kubernetes.io/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author: Derek Carr&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming “Om nom nom nom.”&lt;/p&gt;

&lt;p&gt;In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.&lt;/p&gt;

&lt;h2 id=&#34;can-you-spare-some-pids&#34;&gt;Can You Spare Some PIDs?&lt;/h2&gt;

&lt;p&gt;Here, we’re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.&lt;/p&gt;

&lt;p&gt;In such a scenario, it’s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.&lt;/p&gt;

&lt;h2 id=&#34;fixing-the-problem&#34;&gt;Fixing the Problem&lt;/h2&gt;

&lt;p&gt;Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.&lt;/p&gt;

&lt;p&gt;This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, we’ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs&amp;ndash;similar to how one reserves CPU or memory today&amp;ndash;and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, we’ll have protection against an easily starved Linux resource.&lt;/p&gt;

&lt;p&gt;Get started with &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.14&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34; target=&#34;_blank&#34;&gt;Node Special Interest Group&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;about-the-author&#34;&gt;About the author:&lt;/h3&gt;

&lt;p&gt;Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.14: Local Persistent Volumes GA</title>
      <link>https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Michelle Au (Google), Matt Schallert (Uber), Celina Ward (Uber)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;Local Persistent Volumes&lt;/a&gt;
feature has been promoted to GA in Kubernetes 1.14.
It was first introduced as alpha in Kubernetes 1.7, and then
&lt;a href=&#34;https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/&#34; target=&#34;_blank&#34;&gt;beta&lt;/a&gt; in Kubernetes
1.10. The GA milestone indicates that Kubernetes users may depend on the feature
and its API for production use. GA features are protected by the Kubernetes
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;deprecation
policy&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-local-persistent-volume&#34;&gt;What is a Local Persistent Volume?&lt;/h2&gt;

&lt;p&gt;A local persistent volume represents a local disk directly-attached to a single
Kubernetes Node.&lt;/p&gt;

&lt;p&gt;Kubernetes provides a powerful volume plugin system that enables Kubernetes
workloads to use a &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes&#34; target=&#34;_blank&#34;&gt;wide
variety&lt;/a&gt;
of block and file storage to persist data. Most
of these plugins enable remote storage &amp;ndash; these remote storage systems persist
data independent of the Kubernetes node where the data originated. Remote
storage usually can not offer the consistent high performance guarantees of
local directly-attached storage. With the Local Persistent Volume plugin,
Kubernetes workloads can now consume high performance local storage using the
same volume APIs that app developers have become accustomed to.&lt;/p&gt;

&lt;h2 id=&#34;how-is-it-different-from-a-hostpath-volume&#34;&gt;How is it different from a HostPath Volume?&lt;/h2&gt;

&lt;p&gt;To better understand the benefits of a Local Persistent Volume, it is useful to
compare it to a &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#hostpath&#34; target=&#34;_blank&#34;&gt;HostPath volume&lt;/a&gt;.
HostPath volumes mount a file or directory from
the host node’s filesystem into a Pod. Similarly a Local Persistent Volume
mounts a local disk or partition into a Pod.&lt;/p&gt;

&lt;p&gt;The biggest difference is that the Kubernetes scheduler understands which node a
Local Persistent Volume belongs to. With HostPath volumes, a pod referencing a
HostPath volume may be moved by the scheduler to a different node resulting in
data loss. But with Local Persistent Volumes, the Kubernetes scheduler ensures
that a pod using a Local Persistent Volume is always scheduled to the same node.&lt;/p&gt;

&lt;p&gt;While HostPath volumes may be referenced via a Persistent Volume Claim (PVC) or
directly inline in a pod definition, Local Persistent Volumes can only be
referenced via a PVC. This provides additional security benefits since
Persistent Volume objects are managed by the administrator, preventing Pods from
being able to access any path on the host.&lt;/p&gt;

&lt;p&gt;Additional benefits include support for formatting of block devices during
mount, and volume ownership using fsGroup.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-with-ga&#34;&gt;What&amp;rsquo;s New With GA?&lt;/h2&gt;

&lt;p&gt;Since 1.10, we have mainly focused on improving stability and scalability of the
feature so that it is production ready.&lt;/p&gt;

&lt;p&gt;The only major feature addition is the ability to specify a raw block device and
have Kubernetes automatically format and mount the filesystem. This reduces the
previous burden of having to format and mount devices before giving it to
Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;limitations-of-ga&#34;&gt;Limitations of GA&lt;/h2&gt;

&lt;p&gt;At GA, Local Persistent Volumes do not support &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/&#34; target=&#34;_blank&#34;&gt;dynamic volume
provisioning&lt;/a&gt;.
However there is an &lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner&#34; target=&#34;_blank&#34;&gt;external
controller&lt;/a&gt;
available to help manage the local
PersistentVolume lifecycle for individual disks on your nodes. This includes
creating the PersistentVolume objects, cleaning up and reusing disks once they
have been released by the application.&lt;/p&gt;

&lt;h2 id=&#34;how-to-use-a-local-persistent-volume&#34;&gt;How to Use a Local Persistent Volume?&lt;/h2&gt;

&lt;p&gt;Workloads can request a local persistent volume using the same
PersistentVolumeClaim interface as remote storage backends. This makes it easy
to swap out the storage backend across clusters, clouds, and on-prem
environments.&lt;/p&gt;

&lt;p&gt;First, a StorageClass should be created that sets &lt;code&gt;volumeBindingMode:
WaitForFirstConsumer&lt;/code&gt; to enable &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&#34; target=&#34;_blank&#34;&gt;volume topology-aware
scheduling&lt;/a&gt;.
This mode instructs Kubernetes to wait to bind a PVC until a Pod using it is scheduled.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, the external static provisioner can be &lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#user-guide&#34; target=&#34;_blank&#34;&gt;configured and
run&lt;/a&gt; to create PVs
for all the local disks on your nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM  STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Available          local-storage              8s
local-pv-3796b049   368Gi      RWO            Delete           Available          local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Available          local-storage              7s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Afterwards, workloads can start using the PVs by creating a PVC and Pod or a
StatefulSet with volumeClaimTemplates.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: local-test
spec:
  serviceName: &amp;quot;local-service&amp;quot;
  replicas: 3
  selector:
    matchLabels:
      app: local-test
  template:
    metadata:
      labels:
        app: local-test
    spec:
      containers:
      - name: test-container
        image: k8s.gcr.io/busybox
        command:
        - &amp;quot;/bin/sh&amp;quot;
        args:
        - &amp;quot;-c&amp;quot;
        - &amp;quot;sleep 100000&amp;quot;
        volumeMounts:
        - name: local-vol
          mountPath: /usr/test-pod
  volumeClaimTemplates:
  - metadata:
      name: local-vol
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;local-storage&amp;quot;
      resources:
        requests:
          storage: 368Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the StatefulSet is up and running, the PVCs are all bound:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc
NAME                     STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS      AGE
local-vol-local-test-0   Bound    local-pv-27c0f084   368Gi      RWO            local-storage     3m45s
local-vol-local-test-1   Bound    local-pv-3ddecaea   368Gi      RWO            local-storage     3m40s
local-vol-local-test-2   Bound    local-pv-3796b049   368Gi      RWO            local-storage     3m36s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the disk is no longer needed, the PVC can be deleted. The external static provisioner
will clean up the disk and make the PV available for use again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch sts local-test -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:2}}&#39;
statefulset.apps/local-test patched

$ kubectl delete pvc local-vol-local-test-2
persistentvolumeclaim &amp;quot;local-vol-local-test-2&amp;quot; deleted

$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                            STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Bound       default/local-vol-local-test-0   local-storage              11m
local-pv-3796b049   368Gi      RWO            Delete           Available                                    local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Bound       default/local-vol-local-test-1   local-storage              19m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find full &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;
for the feature on the Kubernetes website.&lt;/p&gt;

&lt;h2 id=&#34;what-are-suitable-use-cases&#34;&gt;What Are Suitable Use Cases?&lt;/h2&gt;

&lt;p&gt;The primary benefit of Local Persistent Volumes over remote persistent storage
is performance: local disks usually offer higher IOPS and throughput and lower
latency compared to remote storage systems.&lt;/p&gt;

&lt;p&gt;However, there are important limitations and caveats to consider when using
Local Persistent Volumes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using local storage ties your application to a specific node, making your
application harder to schedule. Applications which use local storage should
specify a high priority so that lower priority pods, that don’t require local
storage, can be preempted if necessary.&lt;/li&gt;
&lt;li&gt;If that node or local volume encounters a failure and becomes inaccessible, then
that pod also becomes inaccessible. Manual intervention, external controllers,
or operators may be needed to recover from these situations.&lt;/li&gt;
&lt;li&gt;While most remote storage systems implement synchronous replication, most local
disk offerings do not provide data durability guarantees. Meaning loss of the
disk or node may result in loss of all the data on that disk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For these reasons, local persistent storage should only be considered for
workloads that handle data replication and backup at the application layer, thus
making the applications resilient to node or data failures and unavailability
despite the lack of such guarantees at the individual disk level.&lt;/p&gt;

&lt;p&gt;Examples of good workloads include software defined storage systems and
replicated databases. Other types of applications should continue to use highly
available, remotely accessible, durable storage.&lt;/p&gt;

&lt;h2 id=&#34;how-uber-uses-local-storage&#34;&gt;How Uber Uses Local Storage&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://eng.uber.com/m3/&#34; target=&#34;_blank&#34;&gt;M3&lt;/a&gt;, Uber’s in-house metrics platform,
piloted Local Persistent Volumes at scale
in an effort to evaluate &lt;a href=&#34;https://m3db.io/&#34; target=&#34;_blank&#34;&gt;M3DB&lt;/a&gt; —
an open-source, distributed timeseries database
created by Uber. One of M3DB’s notable features is its ability to shard its
metrics into partitions, replicate them by a factor of three, and then evenly
disperse the replicas across separate failure domains.&lt;/p&gt;

&lt;p&gt;Prior to the pilot with local persistent volumes, M3DB ran exclusively in
Uber-managed environments. Over time, internal use cases arose that required the
ability to run M3DB in environments with fewer dependencies. So the team began
to explore options. As an open-source project, we wanted to provide the
community with a way to run M3DB as easily as possible, with an open-source
stack, while meeting M3DB’s requirements for high throughput, low-latency
storage, and the ability to scale itself out.&lt;/p&gt;

&lt;p&gt;The Kubernetes Local Persistent Volume interface, with its high-performance,
low-latency guarantees, quickly emerged as the perfect abstraction to build on
top of. With Local Persistent Volumes, individual M3DB instances can comfortably
handle up to 600k writes per-second. This leaves plenty of headroom for spikes
on clusters that typically process a few million metrics per-second.&lt;/p&gt;

&lt;p&gt;Because M3DB also gracefully handles losing a single node or volume, the limited
data durability guarantees of Local Persistent Volumes are not an issue. If a
node fails, M3DB finds a suitable replacement and the new node begins streaming
data from its two peers.&lt;/p&gt;

&lt;p&gt;Thanks to the Kubernetes scheduler’s intelligent handling of volume topology,
M3DB is able to programmatically evenly disperse its replicas across multiple
local persistent volumes in all available cloud zones, or, in the case of
on-prem clusters, across all available server racks.&lt;/p&gt;

&lt;h2 id=&#34;uber-s-operational-experience&#34;&gt;Uber&amp;rsquo;s Operational Experience&lt;/h2&gt;

&lt;p&gt;As mentioned above, while Local Persistent Volumes provide many benefits, they
also require careful planning and careful consideration of constraints before
committing to them in production. When thinking about our local volume strategy
for M3DB, there were a few things Uber had to consider.&lt;/p&gt;

&lt;p&gt;For one, we had to take into account the hardware profiles of the nodes in our
Kubernetes cluster. For example, how many local disks would each node cluster
have? How would they be partitioned?&lt;/p&gt;

&lt;p&gt;The local static provisioner provides
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/best-practices.md&#34; target=&#34;_blank&#34;&gt;guidance&lt;/a&gt;
to help answer these questions. It’s best to be able to dedicate a full disk to each local volume
(for IO isolation) and a full partition per-volume (for capacity isolation).
This was easier in our cloud environments where we could mix and match local
disks. However, if using local volumes on-prem, hardware constraints may be a
limiting factor depending on the number of disks available and their
characteristics.&lt;/p&gt;

&lt;p&gt;When first testing local volumes, we wanted to have a thorough understanding of
the effect
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/&#34; target=&#34;_blank&#34;&gt;disruptions&lt;/a&gt;
(voluntary and involuntary) would have on pods using
local storage, and so we began testing some failure scenarios. We found that
when a local volume becomes unavailable while the node remains available (such
as when performing maintenance on the disk), a pod using the local volume will
be stuck in a ContainerCreating state until it can mount the volume. If a node
becomes unavailable, for example if it is removed from the cluster or is
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/&#34; target=&#34;_blank&#34;&gt;drained&lt;/a&gt;,
then pods using local volumes on that node are stuck in an Unknown or
Pending state depending on whether or not the node was removed gracefully.&lt;/p&gt;

&lt;p&gt;Recovering pods from these interim states means having to delete the PVC binding
the pod to its local volume and then delete the pod in order for it to be
rescheduled (or wait until the node and disk are available again). We took this
into account when building our &lt;a href=&#34;https://github.com/m3db/m3db-operator&#34; target=&#34;_blank&#34;&gt;operator&lt;/a&gt;
for M3DB, which makes changes to the
cluster topology when a pod is rescheduled such that the new one gracefully
streams data from the remaining two peers. Eventually we plan to automate the
deletion and rescheduling process entirely.&lt;/p&gt;

&lt;p&gt;Alerts on pod states can help call attention to stuck local volumes, and
workload-specific controllers or operators can remediate them automatically.
Because of these constraints, it’s best to exclude nodes with local volumes from
automatic upgrades or repairs, and in fact some cloud providers explicitly
mention this as a best practice.&lt;/p&gt;

&lt;h2 id=&#34;portability-between-on-prem-and-cloud&#34;&gt;Portability Between On-Prem and Cloud&lt;/h2&gt;

&lt;p&gt;Local Volumes played a big role in Uber’s decision to build orchestration for
M3DB using Kubernetes, in part because it is a storage abstraction that works
the same across on-prem and cloud environments. Remote storage solutions have
different characteristics across cloud providers, and some users may prefer not
to use networked storage at all in their own data centers. On the other hand,
local disks are relatively ubiquitous and provide more predictable performance
characteristics.&lt;/p&gt;

&lt;p&gt;By orchestrating M3DB using local disks in the cloud, where it was easier to get
up and running with Kubernetes, we gained confidence that we could still use our
operator to run M3DB in our on-prem environment without any modifications. As we
continue to work on how we’d run Kubernetes on-prem, having solved such an
important pending question is a big relief.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next-for-local-persistent-volumes&#34;&gt;What&amp;rsquo;s Next for Local Persistent Volumes?&lt;/h2&gt;

&lt;p&gt;As we’ve seen with Uber’s M3DB, local persistent volumes have successfully been
used in production environments. As adoption of local persistent volumes
continues to increase, SIG Storage continues to seek feedback for ways to
improve the feature.&lt;/p&gt;

&lt;p&gt;One of the most frequent asks has been for a controller that can help with
recovery from failed nodes or disks, which is currently a manual process (or
something that has to be built into an operator). SIG Storage is investigating
creating a common controller that can be used by workloads with simple and
similar recovery processes.&lt;/p&gt;

&lt;p&gt;Another popular ask has been to support dynamic provisioning using lvm. This can
simplify disk management, and improve disk utilization. SIG Storage is
evaluating the performance tradeoffs for the viability of this feature.&lt;/p&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with
the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage
Special-Interest-Group&lt;/a&gt;
(SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors that helped bring this feature to GA,
including Chuqiang Li (lichuqiang), Dhiraj Hedge (dhirajh), Ian Chakeres
(ianchakeres), Jan Šafránek (jsafrane), Michelle Au (msau42), Saad Ali
(saad-ali), Yecheng Fu (cofyc) and Yuquan Ren (nickrenren).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes v1.14 delivers production-level support for Windows nodes and Windows containers</title>
      <link>https://kubernetes.io/blog/2019/04/01/kubernetes-v1.14-delivers-production-level-support-for-windows-nodes-and-windows-containers/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/01/kubernetes-v1.14-delivers-production-level-support-for-windows-nodes-and-windows-containers/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Michael Michael (VMware), Patrick Lang (Microsoft)&lt;/p&gt;

&lt;p&gt;The first release of Kubernetes in 2019 brings a highly anticipated feature -  production-level support for Windows workloads. Up until now Windows node support in Kubernetes has been in beta, allowing many users to experiment and see the value of Kubernetes for Windows containers. While in beta, developers in the Kubernetes community and Windows Server team worked together to improve the container runtime, build a continuous testing process, and complete features needed for a good user experience. Kubernetes now officially supports adding Windows nodes as worker nodes and scheduling Windows containers, enabling a vast ecosystem of Windows applications to leverage the power of our platform.&lt;/p&gt;

&lt;p&gt;As Windows developers and devops engineers have been adopting containers over the last few years, they&amp;rsquo;ve been looking for a way to manage all their workloads with a common interface. Kubernetes has taken the lead for container orchestration, and this gives users a consistent way to manage their container workloads whether they need to run on Linux or Windows.&lt;/p&gt;

&lt;p&gt;The journey to a stable release of Windows in Kubernetes was not a walk in the park. The community has been working on Windows support for 3 years, delivering an alpha release with v1.5, a beta with v1.9, and now a stable release with v1.14. We would not be here today without rallying broad support and getting significant contributions from companies including Microsoft, Docker, VMware, Pivotal, Cloudbase Solutions, Google and Apprenda. During this journey, there were 3 critical points in time that significantly advanced our progress.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Advancements in Windows Server container networking that provided the infrastructure to create CNI (Container Network Interface) plugins&lt;/li&gt;
&lt;li&gt;Enhancements shipped in Windows Server semi-annual channel releases enabled Kubernetes development to move forward - culminating with Windows Server 2019 on the Long-Term Servicing Channel. This is the best release of Windows Server for running containers.&lt;/li&gt;
&lt;li&gt;The adoption of the KEP (Kubernetes Enhancement Proposals) &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/README.md&#34; target=&#34;_blank&#34;&gt;process&lt;/a&gt;. The Windows KEP outlined a clear and agreed upon set of goals, expectations, and deliverables based on review and feedback from stakeholders across multiple SIGs. This created a clear plan that SIG-Windows could follow, paving the path towards this stable release.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With v1.14, we&amp;rsquo;re declaring that Windows node support is stable, well-tested, and ready for adoption in production scenarios. This is a huge milestone for many reasons. For Kubernetes, it strengthens its position in the industry, enabling a vast ecosystem of Windows-based applications to be deployed on the platform. For Windows operators and developers, this means they can use the same tools and processes to manage their Windows and Linux workloads, taking full advantage of the efficiencies of the cloud-native ecosystem powered by Kubernetes. Let’s dig in a little bit into these.&lt;/p&gt;

&lt;h2 id=&#34;operator-advantages&#34;&gt;Operator Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gain operational efficiencies by leveraging existing investments in solutions, tools, and technologies to manage Windows containers the same way as Linux containers&lt;/li&gt;
&lt;li&gt;Knowledge, training and expertise on container orchestration transfers to Windows container support&lt;/li&gt;
&lt;li&gt;IT can deliver a scalable self-service container platform to Linux and Windows developers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;developer-advantages&#34;&gt;Developer Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Containers simplify packaging and deploying applications during development and test. Now you also get to take advantage of Kubernetes’ benefits in creating reliable, secure, and scalable distributed applications.&lt;/li&gt;
&lt;li&gt;Windows developers can now take advantage of the growing ecosystem of cloud and container-native tools to build and deploy faster, resulting in a faster time to market for their applications&lt;/li&gt;
&lt;li&gt;Taking advantage of Kubernetes as the leader in container orchestration, developers only need to learn how to use Kubernetes and that skillset will transfer across development environments and across clouds&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cio-advantages&#34;&gt;CIO Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Leverage the operational and cost efficiencies that are introduced with Kubernetes&lt;/li&gt;
&lt;li&gt;Containerize existing.NET applications or Windows-based workloads to eliminate old hardware or underutilized virtual machines, and streamline migration from end-of-support OS versions. You  retain the benefit your application brings to the business, but decrease  the cost of keeping it running&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Using Kubernetes on Windows allows us to run our internal web applications as microservices. This provides quick scaling in response to load, smoother upgrades, and allows for different development groups to build without worry of other group&amp;rsquo;s version dependencies. We save money because development times are shorter and operation&amp;rsquo;s time is not spent maintaining multiple virtual machine environments,” said Jeremy, a lead devops engineer working for a top multinational legal firm, one of the early adopters of Windows on Kubernetes.&lt;/p&gt;

&lt;p&gt;There are many features that are surfaced with this release. We want to turn your attention to a few key features and enablers of Windows support in Kubernetes. For a detailed list of supported functionality, you can read our &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#supported-functionality&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can now add Windows Server 2019 worker nodes&lt;/li&gt;
&lt;li&gt;You can now schedule Windows containers utilizing deployments, pods, services, and workload controllers&lt;/li&gt;
&lt;li&gt;Out of tree CNI plugins are provided for Azure, OVN-Kubernetes, and Flannel&lt;/li&gt;
&lt;li&gt;Containers can utilize a variety of in and out-of-tree storage plugins&lt;/li&gt;
&lt;li&gt;Improved support for metrics/quotas closely matches the capabilities offered for Linux containers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When looking at Windows support in Kubernetes, many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between &lt;strong&gt;operational limitations and differences between the Windows and Linux operating systems&lt;/strong&gt;. From a container management standpoint, we must  strike a balance between preserving OS-specific behaviors required for application compatibility, and reaching operational consistency in Kubernetes across multiple operating systems. For example, some Linux-specific file system features, user IDs and permissions exposed through Kubernetes will not work on Windows today, and users are familiar with these fundamental differences. We will also be adding support for Windows-specific configurations to meet the needs of Windows customers that may not exist on Linux. The alpha support for Windows Group Managed Service Accounts is one example. Other areas such as memory reservations for Windows pods and the Windows kubelet are a work in progress and highlight an operational limitation. We will continue working on operational limitations based on what’s important to our community in future releases.&lt;/p&gt;

&lt;p&gt;Today, Kubernetes master components will continue to run on Linux. That way users can add Windows nodes without having to create a separate Kubernetes cluster. As always, our future direction is set by the community, so more components, features and deployment methods will come over time. Users should understand the differences between Windows and Linux and utilize the advantages of each platform. Our goal with this release is not to make Windows interchangeable with Linux or to answer the question of Windows vs Linux. We offer consistency in management. Managing workloads without automation is tedious and expensive. Rewriting or re-architecting workloads is even more expensive. Containers provide a clear path forward whether your app runs on Linux or Windows, and Kubernetes brings an IT organization operational consistency.&lt;/p&gt;

&lt;p&gt;As a community, our work is not complete. As already mentioned , we still have a fair bit of &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#limitations&#34; target=&#34;_blank&#34;&gt;limitations&lt;/a&gt; and a healthy &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#what-s-next&#34; target=&#34;_blank&#34;&gt;roadmap&lt;/a&gt;. We will continue making progress and enhancing Windows container support in Kubernetes, with some notable upcoming features including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support for CRI-ContainerD and Hyper-V isolation, bringing hypervisor-level isolation between pods for additional security and extending our container-to-node compatibility matrix&lt;/li&gt;
&lt;li&gt;Additional network plugins, including the stable release of Flannel overlay support&lt;/li&gt;
&lt;li&gt;Simple heterogeneous cluster creation using kubeadm on Windows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read our getting started and contributor guides, which include links to the community meetings and past recordings, at &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-windows&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explore our documentation at &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/windows/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/setup/production-environment/windows/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join us on &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-windows&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt; or the &lt;a href=&#34;https://discuss.kubernetes.io/c/general-discussions/windows&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Forums&lt;/a&gt; to chat about Windows containers on Kubernetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you and feel free to reach us individually if you have any questions.&lt;/p&gt;

&lt;p&gt;Michael Michael
&lt;br&gt;
SIG-Windows Chair
&lt;br&gt;
Director of Product Management, VMware
&lt;br&gt;
@michmike77 on Twitter
&lt;br&gt;
@m2 on Slack&lt;/p&gt;

&lt;p&gt;Patrick Lang
&lt;br&gt;
SIG-Windows Chair
&lt;br&gt;
Senior Software Engineer, Microsoft
&lt;br&gt;
@PatrickLang on Slack&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: kube-proxy Subtleties: Debugging an Intermittent Connection Reset</title>
      <link>https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/</link>
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;mailto:ygui@google.com&#34; target=&#34;_blank&#34;&gt;Yongkun Gui&lt;/a&gt;, Google&lt;/p&gt;

&lt;p&gt;I recently came across a bug that causes intermittent connection resets.  After
some digging, I found it was caused by a subtle combination of several different
network subsystems. It helped me understand Kubernetes networking better, and I
think it’s worthwhile to share with a wider audience who are interested in the same
topic.&lt;/p&gt;

&lt;h2 id=&#34;the-symptom&#34;&gt;The symptom&lt;/h2&gt;

&lt;p&gt;We received a user report claiming they were getting connection resets while using a
Kubernetes service of type ClusterIP to serve large files to pods running in the
same cluster. Initial debugging of the cluster did not yield anything
interesting: network connectivity was fine and downloading the files did not hit
any issues. However, when we ran the workload in parallel across many clients,
we were able to reproduce the problem. Adding to the mystery was the fact that
the problem could not be reproduced when the workload was run using VMs without
Kubernetes. The problem, which could be easily reproduced by &lt;a href=&#34;https://github.com/tcarmet/k8s-connection-reset&#34; target=&#34;_blank&#34;&gt;a simple
app&lt;/a&gt;, clearly has something to
do with Kubernetes networking, but what?&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-networking-basics&#34;&gt;Kubernetes networking basics&lt;/h2&gt;

&lt;p&gt;Before digging into this problem, let’s talk a little bit about some basics of
Kubernetes networking, as Kubernetes handles network traffic from a pod
very differently depending on different destinations.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-pod&#34;&gt;Pod-to-Pod&lt;/h3&gt;

&lt;p&gt;In Kubernetes, every pod has its own IP address. The benefit is that the
applications running inside pods could use their canonical port, instead of
remapping to a different random port. Pods have L3 connectivity between each
other. They can ping each other, and send TCP or UDP packets to each other.
&lt;a href=&#34;https://github.com/containernetworking/cni&#34; target=&#34;_blank&#34;&gt;CNI&lt;/a&gt; is the standard that solves
this problem for containers running on different hosts. There are tons of
different plugins that support CNI.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-external&#34;&gt;Pod-to-external&lt;/h3&gt;

&lt;p&gt;For the traffic that goes from pod to external addresses, Kubernetes simply uses
&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_address_translation&#34; target=&#34;_blank&#34;&gt;SNAT&lt;/a&gt;. What it does
is replace the pod’s internal source IP:port with the host’s IP:port. When
the return packet comes back to the host, it rewrites the pod’s IP:port as the
destination and sends it back to the original pod. The whole process is transparent
to the original pod, who doesn’t know the address translation at all.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-service&#34;&gt;Pod-to-Service&lt;/h3&gt;

&lt;p&gt;Pods are mortal. Most likely, people want reliable service. Otherwise, it’s
pretty much useless. So Kubernetes has this concept called &amp;ldquo;service&amp;rdquo; which is
simply a L4 load balancer in front of pods. There are several different types of
services. The most basic type is called ClusterIP. For this type of service, it
has a unique VIP address that is only routable inside the cluster.&lt;/p&gt;

&lt;p&gt;The component in Kubernetes that implements this feature is called kube-proxy.
It sits on every node, and programs complicated iptables rules to do all kinds
of filtering and NAT between pods and services. If you go to a Kubernetes node
and type &lt;code&gt;iptables-save&lt;/code&gt;, you’ll see the rules that are inserted by Kubernetes
or other programs. The most important chains are &lt;code&gt;KUBE-SERVICES&lt;/code&gt;, &lt;code&gt;KUBE-SVC-*&lt;/code&gt;
and &lt;code&gt;KUBE-SEP-*&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KUBE-SERVICES&lt;/code&gt; is the entry point for service packets. What it does is to
match the destination IP:port and dispatch the packet to the corresponding
&lt;code&gt;KUBE-SVC-*&lt;/code&gt; chain.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KUBE-SVC-*&lt;/code&gt; chain acts as a load balancer, and distributes the packet to
&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chain equally. Every &lt;code&gt;KUBE-SVC-*&lt;/code&gt; has the same number of
&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chains as the number of endpoints behind it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chain represents a Service EndPoint. It simply does DNAT,
replacing service IP:port with pod&amp;rsquo;s endpoint IP:Port.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For DNAT, conntrack kicks in and tracks the connection state using a state
machine. The state is needed because it needs to remember the destination
address it changed to, and changed it back when the returning packet came back.
Iptables could also rely on the conntrack state (ctstate) to decide the destiny
of a packet. Those 4 conntrack states are especially important:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;NEW&lt;/em&gt;: conntrack knows nothing about this packet, which happens when the SYN
packet is received.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ESTABLISHED&lt;/em&gt;: conntrack knows the packet belongs to an established connection,
which happens after handshake is complete.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;RELATED&lt;/em&gt;: The packet doesn’t belong to any connection, but it is affiliated
to another connection, which is especially useful for protocols like FTP.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;INVALID&lt;/em&gt;: Something is wrong with the packet, and conntrack doesn’t know how
to deal with it. This state plays a centric role in this Kubernetes issue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is a diagram of how a TCP connection works between pod and service. The
sequence of events are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Client pod from left hand side sends a packet to a
service: 192.168.0.2:80&lt;/li&gt;
&lt;li&gt;The packet is going through iptables rules in client
node and the destination is changed to pod IP, 10.0.1.2:80&lt;/li&gt;
&lt;li&gt;Server pod handles the packet and sends back a packet with destination 10.0.0.2&lt;/li&gt;
&lt;li&gt;The packet is going back to the client node, conntrack recognizes the packet and rewrites the source
address back to 192.169.0.2:80&lt;/li&gt;
&lt;li&gt;Client pod receives the response packet&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/good-packet-flow.png&#34;
         alt=&#34;Good packet flow&#34; width=&#34;100%&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Good packet flow&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;what-caused-the-connection-reset&#34;&gt;What caused the connection reset?&lt;/h2&gt;

&lt;p&gt;Enough of the background, so what really went wrong and caused the unexpected
connection reset?&lt;/p&gt;

&lt;p&gt;As the diagram below shows, the problem is packet 3. When conntrack cannot
recognize a returning packet, and mark it as &lt;em&gt;INVALID&lt;/em&gt;. The most common
reasons include: conntrack cannot keep track of a connection because it is out
of capacity, the packet itself is out of a TCP window, etc. For those packets
that have been marked as &lt;em&gt;INVALID&lt;/em&gt; state by conntrack, we don’t have the
iptables rule to drop it, so it will be forwarded to client pod, with source IP
address not rewritten (as shown in packet 4)! Client pod doesn’t recognize this
packet because it has a different source IP, which is pod IP, not service IP. As
a result, client pod says, &amp;ldquo;Wait a second, I don&amp;rsquo;t recall this connection to
this IP ever existed, why does this dude keep sending this packet to me?&amp;rdquo; Basically,
what the client does is simply send a RST packet to the server pod IP, which
is packet 5. Unfortunately, this is a totally legit pod-to-pod packet, which can
be delivered to server pod. Server pod doesn’t know all the address translations
that happened on the client side. From its view, packet 5 is a totally legit
packet, like packet 2 and 3. All server pod knows is, &amp;ldquo;Well, client pod doesn’t
want to talk to me, so let’s close the connection!&amp;rdquo; Boom! Of course, in order
for all these to happen, the RST packet has to be legit too, with the right TCP
sequence number, etc. But when it happens, both parties agree to close the
connection.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/connection-reset-packet-flow.png&#34;
         alt=&#34;Connection reset packet flow&#34; width=&#34;100%&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Connection reset packet flow&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;how-to-address-it&#34;&gt;How to address it?&lt;/h2&gt;

&lt;p&gt;Once we understand the root cause, the fix is not hard. There are at least 2
ways to address it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make conntrack more liberal on packets, and don’t mark the packets as
&lt;em&gt;INVALID&lt;/em&gt;. In Linux, you can do this by &lt;code&gt;echo 1 &amp;gt;
/proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Specifically add an iptables rule to drop the packets that are marked as
&lt;em&gt;INVALID&lt;/em&gt;, so it won’t reach to client pod and cause harm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The fix is drafted (&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/74840&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/kubernetes/pull/74840&lt;/a&gt;), but
unfortunately it didn’t catch the v1.14 release window. However, for the users
that are affected by this bug, there is a way to mitigate the problem by applying
the following rule in your cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DaemonSet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;hostPID:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gcr.io/google-containers/startup-script:v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;IfNotPresent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;STARTUP_SCRIPT&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;            #! /bin/bash&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;done&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Obviously, the bug has existed almost forever. I am surprised that it
hasn’t been noticed until recently. I believe the reasons could be: (1) this
happens more in a congested server serving large payloads, which might not be a
common use case; (2) the application layer handles the retry to be tolerant of
this kind of reset. Anyways, regardless of how fast Kubernetes has been growing,
it’s still a young project. There are no other secrets than listening closely to
customers’ feedback, not taking anything for granted but digging deep, we can
make it the best platform to run applications.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&#34;https://github.com/bowei&#34; target=&#34;_blank&#34;&gt;bowei&lt;/a&gt; for the consulting for both
debugging process and the blog, to &lt;a href=&#34;https://github.com/tcarmet&#34; target=&#34;_blank&#34;&gt;tcarmet&lt;/a&gt; for
reporting the issue and providing a reproduction.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Running Kubernetes locally on Linux with Minikube - now with Kubernetes 1.14 support</title>
      <link>https://kubernetes.io/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt;, Developer Advocate, Cloud Native Computing Foundation&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-28-running-kubernetes-locally-on-linux-with-minikube/ihor-dvoretskyi-1470985-unsplash.jpg&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A few days ago, the Kubernetes community announced &lt;a href=&#34;https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.14&lt;/a&gt;, the most recent version of Kubernetes. Alongside it, Minikube, a part of the Kubernetes project, recently hit the &lt;a href=&#34;https://github.com/kubernetes/minikube/releases/tag/v1.0.0&#34; target=&#34;_blank&#34;&gt;1.0 milestone&lt;/a&gt;, which supports &lt;a href=&#34;https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.14&lt;/a&gt; by default.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes is a real winner (and a de facto standard) in the world of distributed Cloud Native computing. While it can handle up to &lt;a href=&#34;https://kubernetes.io/blog/2017/03/scalability-updates-in-kubernetes-1.6&#34; target=&#34;_blank&#34;&gt;5000 nodes&lt;/a&gt; in a single cluster, local deployment on a single machine (e.g. a laptop, a developer workstation, etc.) is an increasingly common scenario for using Kubernetes.&lt;/p&gt;

&lt;p&gt;A few weeks ago I ran a poll on Twitter asking the community to specify their preferred option for running Kubernetes locally on Linux:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ok, Twitter ✋&lt;br&gt;&lt;br&gt;Your local Kubernetes cluster on Linux is deployed by:&lt;/p&gt;&amp;mdash; ihor dvoretskyi (@idvoretskyi) &lt;a href=&#34;https://twitter.com/idvoretskyi/status/1093154369040773120?ref_src=twsrc%5Etfw&#34;&gt;February 6, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;This is post #1 in a series about the local deployment options on Linux, and it will cover Minikube, the most popular community-built solution for running Kubernetes on a local machine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube&#34; target=&#34;_blank&#34;&gt;Minikube&lt;/a&gt; is a cross-platform, community-driven &lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; distribution, which is targeted to be used primarily in local environments. It deploys a single-node cluster, which is an excellent option for having a simple Kubernetes cluster up and running on localhost.&lt;/p&gt;

&lt;p&gt;Minikube is designed to be used as a virtual machine (VM), and the default VM runtime is &lt;a href=&#34;https://www.virtualbox.org/&#34; target=&#34;_blank&#34;&gt;VirtualBox&lt;/a&gt;. At the same time, extensibility is one of the critical benefits of Minikube, so it&amp;rsquo;s possible to use it with &lt;a href=&#34;https://github.com/kubernetes/minikube/blob/master/docs/drivers.md&#34; target=&#34;_blank&#34;&gt;drivers&lt;/a&gt; outside of VirtualBox.&lt;/p&gt;

&lt;p&gt;By default, Minikube uses Virtualbox as a runtime for running the virtual machine. Virtualbox is a cross-platform solution, which can be used on a variety of operating systems, including GNU/Linux, Windows, and macOS.&lt;/p&gt;

&lt;p&gt;At the same time, QEMU/KVM is a Linux-native virtualization solution, which may offer benefits compared to Virtualbox. For example, it&amp;rsquo;s much easier to use KVM on a GNU/Linux server, so you can run a single-node Minikube cluster not only on a Linux workstation or laptop with GUI, but also on a remote headless server.&lt;/p&gt;

&lt;p&gt;Unfortunately, Virtualbox and KVM can&amp;rsquo;t be used simultaneously, so if you are already running KVM workloads on a machine and want to run Minikube there as well, using the KVM minikube driver is the preferred way to go.&lt;/p&gt;

&lt;p&gt;In this guide, we&amp;rsquo;ll focus on running Minikube with the KVM driver on Ubuntu 18.04 (I am using a bare metal machine running on &lt;a href=&#34;https://www.packet.com&#34; target=&#34;_blank&#34;&gt;packet.com&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-28-running-kubernetes-locally-on-linux-with-minikube/module_01_cluster.png&#34;
         alt=&#34;Minikube architecture (source: kubernetes.io)&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Minikube architecture (source: kubernetes.io)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;This is not an official guide to Minikube. You may find detailed information on running and using Minikube on it&amp;rsquo;s official &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; target=&#34;_blank&#34;&gt;webpage&lt;/a&gt;, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this guide is to provide clear and easy guidelines for running Minikube with KVM on Linux.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Any Linux you like (in this tutorial we&amp;rsquo;ll use Ubuntu 18.04 LTS, and all the instructions below are applicable to it. If you prefer using a different Linux distribution, please check out the relevant documentation)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libvirt&lt;/code&gt; and QEMU-KVM installed and properly configured&lt;/li&gt;
&lt;li&gt;The Kubernetes CLI (&lt;code&gt;kubectl&lt;/code&gt;) for operating the Kubernetes cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;qemu-kvm-and-libvirt-installation&#34;&gt;QEMU/KVM and libvirt installation&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;NOTE: skip if already installed&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we proceed, we have to verify if our host can run KVM-based virtual machines. This can be easily checked using the &lt;a href=&#34;https://manpages.ubuntu.com/manpages/bionic/man1/kvm-ok.1.html&#34; target=&#34;_blank&#34;&gt;kvm-ok&lt;/a&gt; tool, available on Ubuntu.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo apt install cpu-checker &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo kvm-ok&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you receive the following output after running &lt;code&gt;kvm-ok&lt;/code&gt;, you can use KVM on your machine (otherwise, please check out your configuration):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ sudo kvm-ok
INFO: /dev/kvm exists
KVM acceleration can be used&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;rsquo;s install KVM and libvirt and add our current user to the &lt;code&gt;libvirt&lt;/code&gt; group to grant sufficient permissions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo apt install libvirt-clients libvirt-daemon-system qemu-kvm &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo usermod -a -G libvirt &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;whoami&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; newgrp libvirt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After installing libvirt, you may verify the host validity to run the virtual machines with &lt;code&gt;virt-host-validate&lt;/code&gt; tool, which is a part of libvirt.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo virt-host-validate&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;kubectl-kubernetes-cli-installation&#34;&gt;kubectl (Kubernetes CLI) installation&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;NOTE: skip if already installed&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In order to manage the Kubernetes cluster, we need to install &lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/overview/&#34; target=&#34;_blank&#34;&gt;kubectl&lt;/a&gt;, the Kubernetes CLI tool.&lt;/p&gt;

&lt;p&gt;The recommended way to install it on Linux is to download the pre-built binary and move it to a directory under the &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -LO https://storage.googleapis.com/kubernetes-release/release/&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;/bin/linux/amd64/kubectl &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo install kubectl /usr/local/bin &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively, kubectl can be installed with a big variety of different methods (eg. as a .deb or snap package - check out the &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;kubectl documentation&lt;/a&gt; to find the best one for you).&lt;/p&gt;

&lt;h2 id=&#34;minikube-installation&#34;&gt;Minikube installation&lt;/h2&gt;

&lt;h3 id=&#34;minikube-kvm-driver-installation&#34;&gt;Minikube KVM driver installation&lt;/h3&gt;

&lt;p&gt;A VM driver is an essential requirement for local deployment of Minikube. As we&amp;rsquo;ve chosen to use KVM as the Minikube driver in this tutorial, let&amp;rsquo;s install the KVM driver with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo install docker-machine-driver-kvm2 /usr/local/bin/ &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm docker-machine-driver-kvm2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;minikube-installation-1&#34;&gt;Minikube installation&lt;/h3&gt;

&lt;p&gt;Now let&amp;rsquo;s install Minikube itself:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo install minikube-linux-amd64 /usr/local/bin/minikube &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm minikube-linux-amd64&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;verify-the-minikube-installation&#34;&gt;Verify the Minikube installation&lt;/h3&gt;

&lt;p&gt;Before we proceed, we need to verify that Minikube is correctly installed. The simplest way to do this is to check Minikube’s status.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;minikube version&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;to-use-the-kvm2-driver&#34;&gt;To use the KVM2 driver:&lt;/h3&gt;

&lt;p&gt;Now let&amp;rsquo;s run the local Kubernetes cluster with Minikube and KVM:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;minikube start --vm-driver kvm2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;set-kvm2-as-a-default-vm-driver-for-minikube&#34;&gt;Set KVM2 as a default VM driver for Minikube&lt;/h3&gt;

&lt;p&gt;If KVM is used as the single driver for Minikube on our machine, it&amp;rsquo;s more convenient to set it as a default driver and run Minikube with fewer command-line arguments. The following command sets the KVM driver as the default:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;minikube config &lt;span style=&#34;color:#a2f&#34;&gt;set&lt;/span&gt; vm-driver kvm2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So now let&amp;rsquo;s run Minikube as usual:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;minikube start&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;verify-the-kubernetes-installation&#34;&gt;Verify the Kubernetes installation&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s check if the Kubernetes cluster is up and running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;rsquo;s run a simple sample app (nginx in our case):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create deployment nginx --image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;nginx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let’s also check that the Kubernetes pods are correctly provisioned:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;screencast&#34;&gt;Screencast&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://asciinema.org/a/237106&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/237106.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;At this point, a Kubernetes cluster with Minikube and KVM is adequately set up and configured on your local machine.&lt;/p&gt;

&lt;p&gt;To proceed, you may check out the Kubernetes tutorials on the project website:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tutorials/hello-minikube/&#34; target=&#34;_blank&#34;&gt;Hello Minikube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s also worth checking out the &amp;ldquo;Introduction to Kubernetes&amp;rdquo; course by The Linux Foundation/Cloud Native Computing Foundation, available for free on EDX:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.edx.org/course/introduction-to-kubernetes#&#34; target=&#34;_blank&#34;&gt;Introduction to Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.14: Production-level support for Windows Nodes, Kubectl Updates, Persistent Local Volumes GA</title>
      <link>https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; The 1.14 &lt;a href=&#34;https://bit.ly/k8s114-team&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.14, our first release of 2019!&lt;/p&gt;

&lt;p&gt;Kubernetes 1.14 consists of 31 enhancements: 10 moving to stable, 12 in beta, and 7 net new. The main themes of this release are extensibility and supporting more workloads on Kubernetes with three major features moving to general availability, and an important security feature moving to beta.&lt;/p&gt;

&lt;p&gt;More enhancements graduated to stable in this release than any prior Kubernetes release. This represents an important milestone for users and operators in terms of setting support expectations. In addition, there are notable Pod and RBAC enhancements in this release, which are discussed in the “additional notable features” section below.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;production-level-support-for-windows-nodes&#34;&gt;Production-level Support for Windows Nodes&lt;/h2&gt;

&lt;p&gt;Up until now Windows Node support in Kubernetes has been in beta, allowing many users to experiment and see the value of Kubernetes for Windows containers. Kubernetes now officially supports adding Windows nodes as worker nodes and scheduling Windows containers, enabling a vast ecosystem of Windows applications to leverage the power of our platform. Enterprises with investments in Windows-based applications and Linux-based applications don’t have to look for separate orchestrators to manage their workloads, leading to increased operational efficiencies across their deployments, regardless of operating system.&lt;/p&gt;

&lt;p&gt;Some of the key features of enabling Windows containers in Kubernetes include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support for Windows Server 2019 for worker nodes and containers&lt;/li&gt;
&lt;li&gt;Support for out of tree networking with Azure-CNI, OVN-Kubernetes, and Flannel&lt;/li&gt;
&lt;li&gt;Improved support for pods, service types, workload controllers, and metrics/quotas to closely match the capabilities offered for Linux containers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;notable-kubectl-updates&#34;&gt;Notable Kubectl Updates&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;New Kubectl Docs and Logo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The documentation for kubectl has been rewritten from the ground up with a focus on managing Resources using declarative Resource Config. The documentation has been published as a standalone site with the format of a book, and it is linked from the main k8s.io documentation (available at &lt;a href=&#34;https://kubectl.docs.kubernetes.io&#34; target=&#34;_blank&#34;&gt;https://kubectl.docs.kubernetes.io&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The new kubectl logo and mascot (pronounced &lt;em&gt;kubee-cuddle&lt;/em&gt;) are shown on the new docs site logo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kustomize Integration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The declarative Resource Config authoring capabilities of &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34; target=&#34;_blank&#34;&gt;kustomize&lt;/a&gt; are now available in kubectl through the &lt;code&gt;-k&lt;/code&gt; flag (e.g. for commands like &lt;code&gt;apply, get&lt;/code&gt;) and the &lt;code&gt;kustomize&lt;/code&gt; subcommand.  Kustomize helps users author and reuse Resource Config using Kubernetes native concepts. Users can now apply directories with &lt;code&gt;kustomization.yaml&lt;/code&gt; to a cluster using &lt;code&gt;kubectl apply -k dir/&lt;/code&gt;. Users can also emit customized Resource Config to stdout without applying them via &lt;code&gt;kubectl kustomize dir/&lt;/code&gt;. The new capabilities are documented in the new docs at &lt;a href=&#34;https://kubectl.docs.kubernetes.io&#34; target=&#34;_blank&#34;&gt;https://kubectl.docs.kubernetes.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The kustomize subcommand will continue to be developed in the Kubernetes owned &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34; target=&#34;_blank&#34;&gt;kustomize&lt;/a&gt; repo. The latest kustomize features will be available from a standalone kustomize binary (published to the kustomize repo) at a frequent release cadence, and will be updated in kubectl prior to each Kubernetes releases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kubectl Plugin Mechanism Graduating to Stable&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The kubectl plugin mechanism allows developers to publish their own custom kubectl subcommands in the form of standalone binaries. This may be used to extend kubectl with new higher-level functionality and with additional porcelain (e.g. adding a &lt;code&gt;set-ns&lt;/code&gt; command).&lt;/p&gt;

&lt;p&gt;Plugins must have the &lt;code&gt;kubectl-&lt;/code&gt; name prefix and exist on the user’s $PATH. The plugin mechanics have been simplified significantly for GA, and are similar to the git plugin system.&lt;/p&gt;

&lt;h2 id=&#34;persistent-local-volumes-are-now-ga&#34;&gt;Persistent Local Volumes are Now GA&lt;/h2&gt;

&lt;p&gt;This feature, graduating to stable, makes locally attached storage available as a persistent volume source. Distributed file systems and databases are the primary use cases for persistent local storage due performance and cost. On cloud providers, local SSDs give better performance than remote disks. On bare metal, in addition to performance, local storage is typically cheaper and using it is a necessity to provision distributed file systems.&lt;/p&gt;

&lt;h2 id=&#34;pid-limiting-is-moving-to-beta&#34;&gt;PID Limiting is Moving to Beta&lt;/h2&gt;

&lt;p&gt;Process IDs (PIDs) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine. Administrators require mechanisms to ensure that user pods cannot induce PID exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that PIDs are limited among pods in order to ensure they have limited impact to other workloads on the node.&lt;/p&gt;

&lt;p&gt;Administrators are able to provide pod-to-pod PID isolation by defaulting the number of PIDs per pod as a beta feature. In addition, administrators can enable node-to-pod PID isolation as an alpha feature by reserving a number of allocatable PIDs to user pods via node allocatable. The community hopes to graduate this feature to beta in the next release.&lt;/p&gt;

&lt;h2 id=&#34;additional-notable-feature-updates&#34;&gt;Additional Notable Feature Updates&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/564&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt; enables Kubernetes scheduler to schedule more important Pods first and when cluster is out of resources, it removes less important pods to create room for more important ones. The importance is specified by priority.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/580&#34; target=&#34;_blank&#34;&gt;Pod Readiness Gates&lt;/a&gt; introduce an extension point for external feedback on pod readiness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/789&#34; target=&#34;_blank&#34;&gt;Harden the default RBAC discovery clusterrolebindings&lt;/a&gt; removes discovery from the set of APIs which allow for unauthenticated access by default, improving privacy for CRDs and the default security posture of default clusters in general.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.14 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.14 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;features-blog-series&#34;&gt;Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back next week for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Day 1 - Windows Server Containers&lt;/li&gt;
&lt;li&gt;Day 2 - Harden the default RBAC discovery clusterrolebindings&lt;/li&gt;
&lt;li&gt;Day 3 - Pod Priority and Preemption in Kubernetes&lt;/li&gt;
&lt;li&gt;Day 4 - PID Limiting&lt;/li&gt;
&lt;li&gt;Day 5 - Persistent Local Volumes&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release Team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://bit.ly/k8s114-team&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Aaron Crickenberger, Senior Test Engineer at Google. The 43 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over 28,000 individual contributors to date and an active community of more than 57,000 people.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, 381 different companies and over 2,458 individuals contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NetEase&lt;/strong&gt; moving to &lt;a href=&#34;https://www.cncf.io/netease-case-study/&#34; target=&#34;_blank&#34;&gt;Kubernetes has increased their R&amp;amp;D efficiency&lt;/a&gt; by more than 100% and deployment efficiency by 280%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VSCO&lt;/strong&gt; found that moving to continuous integration, containerization, and Kubernetes, &lt;a href=&#34;https://www.cncf.io/blog/2019/02/20/how-vsco-saved-with-kubernetes/&#34; target=&#34;_blank&#34;&gt;velocity was increased dramatically&lt;/a&gt;. The time from code-complete to deployment in production on real infrastructure went from 1-2 weeks to 2-4 hours for a typical service.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NAV&lt;/strong&gt;’s move to &lt;a href=&#34;https://www.cncf.io/blog/2019/03/21/nav-saved-the-company-50-in-infrastructure-costs-with-kubernetes/&#34; target=&#34;_blank&#34;&gt;Kubernetes saved the company 50% in infrastructure costs&lt;/a&gt;, deployments increased 5x from 10 a day to 50 a day and resource utilization increased from 1% to 40%.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes is &lt;a href=&#34;https://summerofcode.withgoogle.com/organizations/5638078861934592/&#34; target=&#34;_blank&#34;&gt;participating in GSoC 2019&lt;/a&gt; under the CNCF. Check out the full list of project ideas for 2019 on &lt;a href=&#34;https://github.com/cncf/soc#project-ideas&#34; target=&#34;_blank&#34;&gt;CNCF’s GitHub page&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.cncf.io/cncf-annual-report-2018/&#34; target=&#34;_blank&#34;&gt;CNCF Annual report 2018&lt;/a&gt; provides a look back at the growth of the Kubernetes and the foundation, community engagement, ecosystem tools, test conformance projects, KubeCon and more.&lt;/li&gt;
&lt;li&gt;Out of 8,000 attendees at &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon North America 2018&lt;/a&gt;, 73% were first-time KubeCon-ers, highlighting massive growth and new interest in Kubernetes and cloud native technologies. Conference transparency &lt;a href=&#34;https://events.linuxfoundation.org/wp-content/uploads/2019/02/KCCNC-NA-18-Report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona&lt;/a&gt; from May 20-23, 2019 and &lt;a href=&#34;https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; (co-located with Open Source Summit) from June 24-26, 2019. These conferences will feature technical sessions, case studies, developer deep dives, salons, and more! &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.14 release team on April 23rd at 10am PDT to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/WN_ViJ0aL4ARiCM15i6erX-pA&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes End-to-end Testing for Everyone</title>
      <link>https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Patrick Ohly (Intel)&lt;/p&gt;

&lt;p&gt;More and more components that used to be part of Kubernetes are now
being developed outside of Kubernetes. For example, storage drivers
used to be compiled into Kubernetes binaries, then were moved into
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md&#34; target=&#34;_blank&#34;&gt;stand-alone FlexVolume
binaries&lt;/a&gt;
on the host, and now are delivered as &lt;a href=&#34;https://github.com/container-storage-interface/spec&#34; target=&#34;_blank&#34;&gt;Container Storage Interface
(CSI) drivers&lt;/a&gt;
that get deployed in pods inside the Kubernetes cluster itself.&lt;/p&gt;

&lt;p&gt;This poses a challenge for developers who work on such components: how
can end-to-end (E2E) testing on a Kubernetes cluster be done for such
external components? The E2E framework that is used for testing
Kubernetes itself has all the necessary functionality. However, trying
to use it outside of Kubernetes was difficult and only possible by
carefully selecting the right versions of a large number of
dependencies. E2E testing has become a lot simpler in Kubernetes 1.13.&lt;/p&gt;

&lt;p&gt;This blog post summarizes the changes that went into Kubernetes
1.13. For CSI driver developers, it will cover the ongoing effort to
also make the storage tests available for testing of third-party CSI
drivers. How to use them will be shown based on two Intel CSI drivers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/intel/oim/&#34; target=&#34;_blank&#34;&gt;Open Infrastructure Manager (OIM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/intel/pmem-csi&#34; target=&#34;_blank&#34;&gt;PMEM-CSI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Testing those drivers was the main motivation behind most of these
enhancements.&lt;/p&gt;

&lt;h2 id=&#34;e2e-overview&#34;&gt;E2E overview&lt;/h2&gt;

&lt;p&gt;E2E testing consists of several phases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implementing a test suite. This is the main focus of this blog
post. The Kubernetes E2E framework is written in Go. It relies on
&lt;a href=&#34;https://onsi.github.io/ginkgo/&#34; target=&#34;_blank&#34;&gt;Ginkgo&lt;/a&gt; for managing tests and
&lt;a href=&#34;http://onsi.github.io/gomega/&#34; target=&#34;_blank&#34;&gt;Gomega&lt;/a&gt; for assertions. These tools
support “behavior driven development”, which describes expected
behavior in “specs”. In this blog post, “test” is used to reference
an individual &lt;code&gt;Ginkgo.It&lt;/code&gt; spec. Tests interact with the Kubernetes
cluster using
&lt;a href=&#34;https://godoc.org/k8s.io/client-go/kubernetes&#34; target=&#34;_blank&#34;&gt;client-go&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Bringing up a test cluster. Tools like
&lt;a href=&#34;https://github.com/kubernetes/test-infra/blob/master/kubetest/README.md&#34; target=&#34;_blank&#34;&gt;kubetest&lt;/a&gt;
can help here.&lt;/li&gt;
&lt;li&gt;Running an E2E test suite against that cluster. Ginkgo test suites
can be run with the &lt;code&gt;ginkgo&lt;/code&gt; tool or as a normal Go test with &lt;code&gt;go
test&lt;/code&gt;. Without any parameters, a Kubernetes E2E test suite will
connect to the default cluster based on environment variables like
KUBECONFIG, exactly like kubectl. Kubetest also knows how to run the
Kubernetes E2E suite.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;e2e-framework-enhancements-in-kubernetes-1-13&#34;&gt;E2E framework enhancements in Kubernetes 1.13&lt;/h2&gt;

&lt;p&gt;All of the following enhancements follow the same basic pattern: they
make the E2E framework more useful and easier to use outside of
Kubernetes, without changing the behavior of the original Kubernetes
e2e.test binary.&lt;/p&gt;

&lt;h3 id=&#34;splitting-out-provider-support&#34;&gt;Splitting out provider support&lt;/h3&gt;

&lt;p&gt;The main reason why using the E2E framework from Kubernetes &amp;lt;= 1.12
was difficult were the dependencies on provider-specific SDKs, which
pulled in a large number of packages. Just getting it compiled was
non-trivial.&lt;/p&gt;

&lt;p&gt;Many of these packages are only needed for certain tests. For example,
testing the mounting of a pre-provisioned volume must first provision
such a volume the same way as an administrator would, by talking
directly to a specific storage backend via some non-Kubernetes API.&lt;/p&gt;

&lt;p&gt;There is an effort to &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/70194&#34; target=&#34;_blank&#34;&gt;remove cloud provider-specific
tests&lt;/a&gt; from
core Kubernetes. The approach taken in &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/68483&#34; target=&#34;_blank&#34;&gt;PR
#68483&lt;/a&gt; can be
seen as an incremental step towards that goal: instead of ripping out
the code immediately and breaking all tests that depend on it, all
cloud provider-specific code was moved into optional packages under
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.13/test/e2e/framework/providers&#34; target=&#34;_blank&#34;&gt;test/e2e/framework/providers&lt;/a&gt;. The
E2E framework then accesses it via &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/6c1e64b94a3e111199c934c39a0c25bc219ed5f9/test/e2e/framework/provider.go#L79-L99&#34; target=&#34;_blank&#34;&gt;an
interface&lt;/a&gt;
that gets implemented separately by each vendor package.&lt;/p&gt;

&lt;p&gt;The author of a E2E test suite decides which of these packages get
imported into the test suite. The vendor support is then activated via
the &lt;code&gt;--provider&lt;/code&gt; command line flag. The Kubernetes e2e.test binary in
1.13 and 1.14 still contains support for the same providers as in
1.12. It is also okay to include no packages, which means that only
the generic providers will be available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“skeleton”: cluster is accessed via the Kubernetes API and nothing
else&lt;/li&gt;
&lt;li&gt;“local”: like “skeleton”, but in addition the scripts in
kubernetes/kubernetes/cluster can retrieve logs via ssh after a test
suite is run&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;external-files&#34;&gt;External files&lt;/h3&gt;

&lt;p&gt;Tests may have to read additional files at runtime, like .yaml
manifests. But the Kubernetes e2e.test binary is supposed to be usable
and entirely stand-alone because that simplifies shipping and running
it. The solution in the Kubernetes build system is to link all files
under &lt;code&gt;test/e2e/testing-manifests&lt;/code&gt; into the binary with
&lt;a href=&#34;https://github.com/jteeuwen/go-bindata&#34; target=&#34;_blank&#34;&gt;go-bindata&lt;/a&gt;. The
E2E framework used to have a hard dependency on the output of
&lt;code&gt;go-bindata&lt;/code&gt;, now &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/69103&#34; target=&#34;_blank&#34;&gt;bindata support is
optional&lt;/a&gt;. When
accessing a file via the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/testfiles/testfiles.go&#34; target=&#34;_blank&#34;&gt;testfiles
package&lt;/a&gt;,
files will be retrieved from different sources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;relative to the directory specified with &lt;code&gt;--repo-root&lt;/code&gt; parameter&lt;/li&gt;
&lt;li&gt;zero or more bindata chunks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;test-parameters&#34;&gt;Test parameters&lt;/h3&gt;

&lt;p&gt;The e2e.test binary takes additional parameters which control test
execution. In 2016, an effort was started to replace all E2E command
line parameters with a Viper configuration file. But that effort
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/0ed33881dc4355495f623c6f22e7dd0b7632b7c0/test/e2e/framework/test_context.go#L318-L319&#34; target=&#34;_blank&#34;&gt;stalled&lt;/a&gt;, which left developers without clear guidance how they should handle
test-specific parameters.&lt;/p&gt;

&lt;p&gt;The approach in v1.12 was to add all flags to the central
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.12.0/test/e2e/framework/test_context.go&#34; target=&#34;_blank&#34;&gt;test/e2e/framework/test_context.go&lt;/a&gt;,
which does not work for tests developed independently from the
framework.  Since &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/69105&#34; target=&#34;_blank&#34;&gt;PR
#69105&lt;/a&gt; the
recommendation has been to use the normal &lt;code&gt;flag&lt;/code&gt; package to
define its parameters, in its own source code. Flag names must be
hierarchical with dots separating different levels, for example
&lt;code&gt;my.test.parameter&lt;/code&gt;, and must be unique. Uniqueness is enforced by the
&lt;code&gt;flag&lt;/code&gt; package which panics when registering a flag a second time. The
new
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/config/config.go&#34; target=&#34;_blank&#34;&gt;config&lt;/a&gt;
package simplifies the definition of multiple options, which are
stored in a single struct.&lt;/p&gt;

&lt;p&gt;To summarize, this is how parameters are handled now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The init code in test packages defines tests and parameters. The
actual parameter &lt;em&gt;values&lt;/em&gt; are not available yet, so test definitions
cannot use them.&lt;/li&gt;
&lt;li&gt;The init code of the test suite parses parameters and (optionally)
the configuration file.&lt;/li&gt;
&lt;li&gt;The tests run and now can use parameter values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, recently it &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/69105#discussion_r267960062&#34; target=&#34;_blank&#34;&gt;was pointed
out&lt;/a&gt;
that it is desirable and was possible to not expose test settings as
command line flags and only set them via a configuration file. There
is an &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/75590&#34; target=&#34;_blank&#34;&gt;open bug&lt;/a&gt; and a
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/75593&#34; target=&#34;_blank&#34;&gt;pending PR&lt;/a&gt;
about this.&lt;/p&gt;

&lt;p&gt;Viper support has been enhanced. Like the provider support, it is
completely optional. It gets pulled into a e2e.test binary by
importing the &lt;code&gt;viperconfig&lt;/code&gt; package and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/ddf47ac13c1a9483ea035a79cd7c10005ff21a6d/test/e2e/e2e_test.go#L49-L57&#34; target=&#34;_blank&#34;&gt;calling
it&lt;/a&gt;
after parsing the normal command line flags. This has been implemented
so that all variables which can be set via command line flags are also
set when the flag appears in a Viper config file. For example, the
Kubernetes v1.13 &lt;code&gt;e2e.test&lt;/code&gt; binary accepts
&lt;code&gt;--viper-config=/tmp/my-config.yaml&lt;/code&gt; and that file will set the
&lt;code&gt;my.test.parameter&lt;/code&gt; to &lt;code&gt;value&lt;/code&gt; when it has this content: my: test:
parameter: value&lt;/p&gt;

&lt;p&gt;In older Kubernetes releases, that option could only load a file from
the current directory, the suffix had to be left out, and only a few
parameters actually could be set this way. Beware that one limitation
of Viper still exists: it works by matching config file entries
against known flags, without warning about unknown config file entries
and thus leaving typos undetected. A &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/1040&#34; target=&#34;_blank&#34;&gt;better config file
parser&lt;/a&gt; for
Kubernetes is still work in progress.&lt;/p&gt;

&lt;h3 id=&#34;creating-items-from-yaml-manifests&#34;&gt;Creating items from .yaml manifests&lt;/h3&gt;

&lt;p&gt;In Kubernetes 1.12, there was some support for loading individual
items from a .yaml file, but then creating that item had to be done by
hand-written code. Now the framework has &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/create.go&#34; target=&#34;_blank&#34;&gt;new
methods&lt;/a&gt;
for loading a .yaml file that has multiple items, patching those items
(for example, setting the namespace created for the current test), and
creating them.  This is currently &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/ddf47ac13c1a9483ea035a79cd7c10005ff21a6d/test/e2e/storage/drivers/csi.go#L192-L209&#34; target=&#34;_blank&#34;&gt;used to deploy CSI
drivers&lt;/a&gt; anew for each test from exactly the same .yaml files that are also
used for deployment via kubectl. If the CSI driver supports running
under different names, then tests are completely independent and can
run in parallel.&lt;/p&gt;

&lt;p&gt;However, redeploying a driver slows down test execution and it does
not cover concurrent operations against the driver. A more realistic
test scenario is to deploy a driver once when bringing up the test
cluster, then run all tests against that deployment. Eventually the
Kubernetes E2E testing will move to that model, once it is clearer how
test cluster bringup can be extended such that it also includes
installing additional entities like CSI drivers.&lt;/p&gt;

&lt;h2 id=&#34;upcoming-enhancements-in-kubernetes-1-14&#34;&gt;Upcoming enhancements in Kubernetes 1.14&lt;/h2&gt;

&lt;h3 id=&#34;reusing-storage-tests&#34;&gt;Reusing storage tests&lt;/h3&gt;

&lt;p&gt;Being able to use the framework outside of Kubernetes enables building
a custom test suite. But a test suite without tests is still
useless. Several of the existing tests, in particular for storage, can
also be applied to out-of-tree components. Thanks to the work done by
Masaki Kimura, &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/e2e/storage/testsuites&#34; target=&#34;_blank&#34;&gt;storage
tests&lt;/a&gt;
in Kubernetes 1.13 are defined such that they can be instantiated
multiple times for different drivers.&lt;/p&gt;

&lt;p&gt;But history has a habit of repeating itself. As with providers, the
package defining these tests also pulled in driver definitions for all
in-tree storage backends, which in turn pulled in more additional
packages than were needed. This has been
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/70862&#34; target=&#34;_blank&#34;&gt;fixed&lt;/a&gt; for the
upcoming Kubernetes 1.14.&lt;/p&gt;

&lt;h3 id=&#34;skipping-unsupported-tests&#34;&gt;Skipping unsupported tests&lt;/h3&gt;

&lt;p&gt;Some of the storage tests depend on features of the cluster (like
running on a host that supports XFS) or of the driver (like supporting
block volumes). These conditions are checked while the test runs,
leading to skipped tests when they are not satisfied. The good thing
is that this records an explanation why the test did not run.&lt;/p&gt;

&lt;p&gt;Starting a test is slow, in particular when it must first deploy the
CSI driver, but also in other scenarios. Creating the namespace for a
test has been measured at 5 seconds on a fast cluster, and it produces
a lot of noisy test output. It would have been possible to address
that by &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/70992&#34; target=&#34;_blank&#34;&gt;skipping the definition of unsupported
tests&lt;/a&gt;, but then
reporting why a test isn’t even part of the test suite becomes
tricky. This approach has been dropped in favor of reorganizing the
storage test suite such that it &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/72434&#34; target=&#34;_blank&#34;&gt;first checks
conditions&lt;/a&gt;
before doing the more expensive test setup steps.&lt;/p&gt;

&lt;h3 id=&#34;more-readable-test-definitions&#34;&gt;More readable test definitions&lt;/h3&gt;

&lt;p&gt;The same PR also rewrites the tests to operate like conventional
Ginkgo tests, with test cases and their local variables in &lt;a href=&#34;https://github.com/pohly/kubernetes/blob/ec3655a1d40ced6b1873e627b736aae1cf242477/test/e2e/storage/testsuites/provisioning.go#L82&#34; target=&#34;_blank&#34;&gt;a single
function&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;testing-external-drivers&#34;&gt;Testing external drivers&lt;/h3&gt;

&lt;p&gt;Building a custom E2E test suite is still quite a bit of work. The
e2e.test binary that will get distributed in the &lt;a href=&#34;https://dl.k8s.io/v1.14.0/kubernetes-test.tar.gz&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.14 test
archive&lt;/a&gt; will have
the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/72836&#34; target=&#34;_blank&#34;&gt;ability to
test&lt;/a&gt; already
installed storage drivers without rebuilding the test suite. See this
&lt;a href=&#34;https://github.com/pohly/kubernetes/blob/6644db9914379a4a7b3d3487b41b2010f226e4dc/test/e2e/storage/external/README.md&#34; target=&#34;_blank&#34;&gt;README&lt;/a&gt;
for further instructions.&lt;/p&gt;

&lt;h2 id=&#34;e2e-test-suite-howto&#34;&gt;E2E test suite HOWTO&lt;/h2&gt;

&lt;h3 id=&#34;test-suite-initialization&#34;&gt;Test suite initialization&lt;/h3&gt;

&lt;p&gt;The first step is to set up the necessary boilerplate code that
defines the test suite. &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/e2e&#34; target=&#34;_blank&#34;&gt;In Kubernetes
E2E&lt;/a&gt;,
this is done in the &lt;code&gt;e2e.go&lt;/code&gt; and &lt;code&gt;e2e_test.go&lt;/code&gt; files. It could also be
done in a single &lt;code&gt;e2e_test.go&lt;/code&gt; file. Kubernetes imports all of the
various providers, in-tree tests, Viper configuration support, and
bindata file lookup in &lt;code&gt;e2e_test.go&lt;/code&gt;. &lt;code&gt;e2e.go&lt;/code&gt; controls the actual
execution, including some cluster preparations and metrics collection.&lt;/p&gt;

&lt;p&gt;A simpler starting point are the &lt;code&gt;e2e_[test].go&lt;/code&gt; files &lt;a href=&#34;https://github.com/intel/pmem-csi/tree/586ae281ac2810cb4da6f1e160cf165c7daf0d80/test/e2e&#34; target=&#34;_blank&#34;&gt;from
PMEM-CSI&lt;/a&gt;. It
doesn’t use any providers, no Viper, no bindata, and imports just the
storage tests.&lt;/p&gt;

&lt;p&gt;Like PMEM-CSI, OIM drops all of the extra features, but is a bit more
complex because it integrates a custom cluster startup directly into
the &lt;a href=&#34;https://github.com/intel/pmem-csi/blob/a7b0d66b59771bf615e07fcd3d4f0ba08cfdf90f/test/e2e/e2e.go&#34; target=&#34;_blank&#34;&gt;test
suite&lt;/a&gt;,
which was useful in this case because some additional components have
to run on the host side. By running them directly in the E2E binary,
interactive debugging with &lt;code&gt;dlv&lt;/code&gt; becomes easier.&lt;/p&gt;

&lt;p&gt;Both CSI drivers follow the Kubernetes example and use the &lt;code&gt;test/e2e&lt;/code&gt;
directory for their test suites, but any other directory and other
file names would also work.&lt;/p&gt;

&lt;h3 id=&#34;adding-e2e-storage-tests&#34;&gt;Adding E2E storage tests&lt;/h3&gt;

&lt;p&gt;Tests are defined by packages that get imported into a test suite. The
only thing specific to E2E tests is that they instantiate a
&lt;code&gt;framework.Framework&lt;/code&gt; pointer (usually called &lt;code&gt;f&lt;/code&gt;) with
&lt;code&gt;framework.NewDefaultFramework&lt;/code&gt;. This variable gets initialized anew
in a &lt;code&gt;BeforeEach&lt;/code&gt; for each test and freed in an &lt;code&gt;AfterEach&lt;/code&gt;. It has a
&lt;code&gt;f.ClientSet&lt;/code&gt; and &lt;code&gt;f.Namespace&lt;/code&gt; at runtime (and only at runtime!)
which can be used by a test.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/intel/pmem-csi/blob/devel/test/e2e/storage/csi_volumes.go#L51&#34; target=&#34;_blank&#34;&gt;PMEM-CSI storage
test&lt;/a&gt;
imports the Kubernetes storage test suite and sets up one instance of
the provisioning tests for a PMEM-CSI driver which must be already
installed in the test cluster. The storage test suite changes the
storage class to run tests with different filesystem types. Because of
this requirement, the storage class is created from a .yaml file.&lt;/p&gt;

&lt;p&gt;Explaining all the various utility methods available in the framework
is out of scope for this blog post. Reading existing tests and the
source code of the framework is a good way to get started.&lt;/p&gt;

&lt;h3 id=&#34;vendoring&#34;&gt;Vendoring&lt;/h3&gt;

&lt;p&gt;Vendoring Kubernetes code is still not trivial, even after eliminating
many of the unnecessary dependencies. &lt;code&gt;k8s.io/kubernetes&lt;/code&gt; is not meant
to be included in other projects and does not define its dependencies
in a way that is understood by tools like &lt;code&gt;dep&lt;/code&gt;. The other &lt;code&gt;k8s.io&lt;/code&gt;
packages are meant to be included, but &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/72638&#34; target=&#34;_blank&#34;&gt;don’t follow semantic
versioning
yet&lt;/a&gt; or don’t
tag any releases (&lt;code&gt;k8s.io/kube-openapi&lt;/code&gt;, &lt;code&gt;k8s.io/utils&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;PMEM-CSI uses &lt;a href=&#34;https://golang.github.io/dep/&#34; target=&#34;_blank&#34;&gt;dep&lt;/a&gt;. It’s
&lt;a href=&#34;https://github.com/intel/pmem-csi/blob/0ad8251c064b1010c91e7fc1dd423b95d5594bba/Gopkg.toml&#34; target=&#34;_blank&#34;&gt;Gopkg.toml&lt;/a&gt;
file is a good starting point. It enables pruning (not enabled in dep
by default) and locks certain projects onto versions that are
compatible with the Kubernetes version that is used. When &lt;code&gt;dep&lt;/code&gt;
doesn’t pick a compatible version, then checking Kubernetes’
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/Godeps/Godeps.json&#34; target=&#34;_blank&#34;&gt;Godeps.json&lt;/a&gt;
helps to determine which revision might be the right one.&lt;/p&gt;

&lt;h3 id=&#34;compiling-and-running-the-test-suite&#34;&gt;Compiling and running the test suite&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;go test ./test/e2e -args -help&lt;/code&gt; is the fastest way to test that the
test suite compiles.&lt;/p&gt;

&lt;p&gt;Once it does compile and a cluster has been set up, the command &lt;code&gt;go
test -timeout=0 -v ./test/e2e -ginkgo.v&lt;/code&gt; runs all tests. In order to
run tests in parallel, use the &lt;code&gt;ginkgo -p ./test/e2e&lt;/code&gt; command instead.&lt;/p&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting involved&lt;/h2&gt;

&lt;p&gt;The Kubernetes E2E framework is owned by the testing-commons
sub-project in
&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-testing&#34; target=&#34;_blank&#34;&gt;SIG-testing&lt;/a&gt;. See
that page for contact information.&lt;/p&gt;

&lt;p&gt;There are various tasks that could be worked on, including but not
limited to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Moving test/e2e/framework into a staging repo and restructuring it
so that it is more modular
(&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/74352&#34; target=&#34;_blank&#34;&gt;#74352&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Simplifying &lt;code&gt;e2e.go&lt;/code&gt; by moving more of its code into
&lt;code&gt;test/e2e/framework&lt;/code&gt;
(&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/74353&#34; target=&#34;_blank&#34;&gt;#74353&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Removing provider-specific code from the Kubernetes E2E test suite
(&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/70194&#34; target=&#34;_blank&#34;&gt;#70194&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to the reviewers of this article:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Olev Kartau (&lt;a href=&#34;https://github.com/okartau&#34; target=&#34;_blank&#34;&gt;https://github.com/okartau&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mary Camp (&lt;a href=&#34;https://github.com/MCamp859&#34; target=&#34;_blank&#34;&gt;https://github.com/MCamp859&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: A Guide to Kubernetes Admission Controllers</title>
      <link>https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Malte Isberner (StackRox)&lt;/p&gt;

&lt;p&gt;Kubernetes has greatly improved the speed and manageability of backend clusters in production today. Kubernetes has emerged as the de facto standard in container orchestrators thanks to its flexibility, scalability, and ease of use. Kubernetes also provides a range of features that secure production workloads. A more recent introduction in security features is a set of plugins called “&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&#34; target=&#34;_blank&#34;&gt;admission controllers&lt;/a&gt;.” Admission controllers must be enabled to use some of the more advanced security features of Kubernetes, such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34; target=&#34;_blank&#34;&gt;pod security policies&lt;/a&gt; that enforce a security configuration baseline across an entire namespace. The following must-know tips and tricks will help you leverage admission controllers to make the most of these security capabilities in Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;what-are-kubernetes-admission-controllers&#34;&gt;What are Kubernetes admission controllers?&lt;/h2&gt;

&lt;p&gt;In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercept (authenticated) API requests and may change the request object or deny the request altogether. The admission control process has two phases: the &lt;em&gt;mutating&lt;/em&gt; phase is executed first, followed by the &lt;em&gt;validating&lt;/em&gt; phase. Consequently, admission controllers can act as mutating or validating controllers or as a combination of both. For example, the &lt;strong&gt;LimitRanger&lt;/strong&gt; admission controller can augment pods with default resource requests and limits (mutating phase), as well as verify that pods with explicitly set resource requirements do not exceed the per-namespace limits specified in the &lt;strong&gt;LimitRange object&lt;/strong&gt; (validating phase).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-21-a-guide-to-kubernetes-admission-controllers/admission-controller-phases.png&#34;
         alt=&#34;Admission Controller Phases&#34; width=&#34;800&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Admission Controller Phases&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;It is worth noting that some aspects of Kubernetes’ operation that many users would consider built-in are in fact governed by admission controllers. For example, when a namespace is deleted and subsequently enters the &lt;code&gt;Terminating&lt;/code&gt; state, the &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespacelifecycle&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;NamespaceLifecycle&lt;/code&gt;&lt;/a&gt; admission controller is what prevents any new objects from being created in this namespace.&lt;/p&gt;

&lt;p&gt;Among the more than 30 admission controllers shipped with Kubernetes, two take a special role because of their nearly limitless flexibility - &lt;code&gt;ValidatingAdmissionWebhooks&lt;/code&gt; and &lt;code&gt;MutatingAdmissionWebhooks&lt;/code&gt;, both of which are in beta status as of Kubernetes 1.13. We will examine these two admission controllers closely, as they do not implement any policy decision logic themselves. Instead, the respective action is obtained from a REST endpoint (a &lt;em&gt;webhook&lt;/em&gt;) of a service running inside the cluster. This approach decouples the admission controller logic from the Kubernetes API server, thus allowing users to implement custom logic to be executed whenever resources are created, updated, or deleted in a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The difference between the two kinds of admission controller webhooks is pretty much self-explanatory: mutating admission webhooks may mutate the objects, while validating admission webhooks may not. However, even a mutating admission webhook can reject requests and thus act in a validating fashion. Validating admission webhooks have two main advantages over mutating ones: first, for security reasons it might be desirable to disable the &lt;code&gt;MutatingAdmissionWebhook&lt;/code&gt; admission controller (or apply stricter RBAC restrictions as to who may create &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt; objects) because of its potentially confusing or even dangerous side effects. Second, as shown in the previous diagram, validating admission controllers (and thus webhooks) are run after any mutating ones. As a result, whatever request object a validating webhook sees is the final version that would be persisted to &lt;code&gt;etcd&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The set of enabled admission controllers is configured by passing a flag to the Kubernetes API server. Note that the old &lt;strong&gt;&amp;ndash;admission-control&lt;/strong&gt; flag was deprecated in 1.10 and replaced with &lt;strong&gt;&amp;ndash;enable-admission-plugins&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--enable-admission-plugins=ValidatingAdmissionWebhook,MutatingAdmissionWebhook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kubernetes recommends the following admission controllers to be enabled by default.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota,PodSecurityPolicy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The complete list of admission controllers with their descriptions can be found &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do&#34; target=&#34;_blank&#34;&gt;in the official  Kubernetes reference&lt;/a&gt;. This discussion will focus only on the webhook-based admission controllers.&lt;/p&gt;

&lt;h2 id=&#34;why-do-i-need-admission-controllers&#34;&gt;Why do I need admission controllers?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; Admission controllers can increase security by mandating a reasonable security baseline across an entire namespace or cluster. The built-in &lt;code&gt;PodSecurityPolicy&lt;/code&gt; admission controller is perhaps the most prominent example; it can be used for disallowing containers from running as root or making sure the container’s root filesystem is always mounted read-only, for example. Further use cases that can be realized by custom, webhook-based admission controllers include:

&lt;ul&gt;
&lt;li&gt;Allow pulling images only from specific registries known to the enterprise, while denying unknown image registries.&lt;/li&gt;
&lt;li&gt;Reject deployments that do not meet security standards. For example, containers using the &lt;code&gt;privileged&lt;/code&gt; flag can circumvent a lot of security checks. This risk could be mitigated by a webhook-based admission controller that either rejects such deployments (validating) or overrides the &lt;code&gt;privileged&lt;/code&gt; flag, setting it to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Governance:&lt;/strong&gt; Admission controllers allow you to enforce the adherence to certain practices such as having good labels, annotations, resource limits, or other settings. Some of the common scenarios include:

&lt;ul&gt;
&lt;li&gt;Enforce label validation on different objects to ensure proper labels are being used for various objects, such as every object being assigned to a team or project, or every deployment specifying an app label.&lt;/li&gt;
&lt;li&gt;Automatically add annotations to objects, such as attributing the correct cost center for a “dev” deployment resource.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configuration management:&lt;/strong&gt; Admission controllers allow you to validate the configuration of the objects running in the cluster and prevent any obvious misconfigurations from hitting your cluster. Admission controllers can be useful in detecting and fixing images deployed without semantic tags, such as by:

&lt;ul&gt;
&lt;li&gt;automatically adding resource limits or validating resource limits,&lt;/li&gt;
&lt;li&gt;ensuring reasonable labels are added to pods, or&lt;/li&gt;
&lt;li&gt;ensuring image references used in production deployments are not using the &lt;code&gt;latest&lt;/code&gt; tags, or tags with a &lt;code&gt;-dev&lt;/code&gt; suffix.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this way, admission controllers and policy management help make sure that applications stay in compliance within an ever-changing landscape of controls.&lt;/p&gt;

&lt;h2 id=&#34;example-writing-and-deploying-an-admission-controller-webhook&#34;&gt;Example: Writing and Deploying an Admission Controller Webhook&lt;/h2&gt;

&lt;p&gt;To illustrate how admission controller webhooks can be leveraged to establish custom security policies, let’s consider an example that addresses one of the shortcomings of Kubernetes: a lot of its defaults are optimized for ease of use and reducing friction, sometimes at the expense of security. One of these settings is that containers are by default allowed to run as root (and, without further configuration and no &lt;code&gt;USER&lt;/code&gt; directive in the Dockerfile, will also do so). Even though containers are isolated from the underlying host to a certain extent, running containers as root does increase the risk profile of your deployment— and should be avoided as one of many &lt;a href=&#34;https://www.stackrox.com/post/2018/12/6-container-security-best-practices-you-should-be-following/&#34; target=&#34;_blank&#34;&gt;security best practices&lt;/a&gt;. The &lt;a href=&#34;https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/&#34; target=&#34;_blank&#34;&gt;recently exposed runC vulnerability&lt;/a&gt; (&lt;a href=&#34;https://nvd.nist.gov/vuln/detail/CVE-2019-5736&#34; target=&#34;_blank&#34;&gt;CVE-2019-5736&lt;/a&gt;), for example, could be exploited only if the container ran as root.&lt;/p&gt;

&lt;p&gt;You can use a custom mutating admission controller webhook to apply more secure defaults: unless explicitly requested, our webhook will ensure that pods run as a non-root user (we assign the user ID 1234 if no explicit assignment has been made). Note that this setup does not prevent you from deploying any workloads in your cluster, including those that legitimately require running as root. It only requires you to explicitly enable this risker mode of operation in the deployment configuration, while defaulting to non-root mode for all other workloads.&lt;/p&gt;

&lt;p&gt;The full code along with deployment instructions can be found in our accompanying &lt;a href=&#34;https://github.com/stackrox/admission-controller-webhook-demo&#34; target=&#34;_blank&#34;&gt;GitHub repository&lt;/a&gt;. Here, we will highlight a few of the more subtle aspects about how webhooks work.&lt;/p&gt;

&lt;h2 id=&#34;mutating-webhook-configuration&#34;&gt;Mutating Webhook Configuration&lt;/h2&gt;

&lt;p&gt;A mutating admission controller webhook is defined by creating a &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt; object in Kubernetes. In our example, we use the following configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: &amp;quot;/mutate&amp;quot;
      caBundle: ${CA_PEM_B64}
    rules:
      - operations: [ &amp;quot;CREATE&amp;quot; ]
        apiGroups: [&amp;quot;&amp;quot;]
        apiVersions: [&amp;quot;v1&amp;quot;]
        resources: [&amp;quot;pods&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration defines a &lt;code&gt;webhook webhook-server.webhook-demo.svc&lt;/code&gt;, and instructs the Kubernetes API server to consult the service &lt;code&gt;webhook-server&lt;/code&gt; in n&lt;code&gt;amespace webhook-demo&lt;/code&gt; whenever a pod is created by making a HTTP POST request to the &lt;code&gt;/mutate&lt;/code&gt; URL. For this configuration to work, several prerequisites have to be met.&lt;/p&gt;

&lt;h2 id=&#34;webhook-rest-api&#34;&gt;Webhook REST API&lt;/h2&gt;

&lt;p&gt;The Kubernetes API server makes an HTTPS POST request to the given service and URL path, with a JSON-encoded &lt;a href=&#34;https://github.com/kubernetes/api/blob/master/admission/v1beta1/types.go#L29&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;AdmissionReview&lt;/code&gt;&lt;/a&gt; (with the &lt;code&gt;Request&lt;/code&gt; field set) in the request body. The response should in turn be a JSON-encoded &lt;code&gt;AdmissionReview&lt;/code&gt;, this time with the Response field set.&lt;/p&gt;

&lt;p&gt;Our demo repository contains a &lt;a href=&#34;https://github.com/stackrox/admission-controller-webhook-demo/blob/master/cmd/webhook-server/admission_controller.go#L132&#34; target=&#34;_blank&#34;&gt;function&lt;/a&gt; that takes care of the serialization/deserialization boilerplate code and allows you to focus on implementing the logic operating on Kubernetes API objects. In our example, the function implementing the admission controller logic is called &lt;code&gt;applySecurityDefaults&lt;/code&gt;, and an HTTPS server serving this function under the /mutate URL can be set up as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mux := http.NewServeMux()
mux.Handle(&amp;quot;/mutate&amp;quot;, admitFuncHandler(applySecurityDefaults))
server := &amp;amp;http.Server{
  Addr:    &amp;quot;:8443&amp;quot;,
  Handler: mux,
}
log.Fatal(server.ListenAndServeTLS(certPath, keyPath))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that for the server to run without elevated privileges, we have the HTTP server listen on port 8443. Kubernetes does not allow specifying a port in the webhook configuration; it always assumes the HTTPS port 443. However, since a service object is required anyway, we can easily map port 443 of the service to port 8443 on the container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server  # specified by the deployment/pod
  ports:
    - port: 443
      targetPort: webhook-api  # name of port 8443 of the container
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;object-modification-logic&#34;&gt;Object Modification Logic&lt;/h2&gt;

&lt;p&gt;In a mutating admission controller webhook, mutations are performed via &lt;a href=&#34;https://tools.ietf.org/html/rfc6902&#34; target=&#34;_blank&#34;&gt;JSON patches&lt;/a&gt;. While the JSON patch standard includes a lot of intricacies that go well beyond the scope of this discussion, the Go data structure in our example as well as its usage should give the user a good initial overview of how JSON patches work:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type patchOperation struct {
  Op    string      `json:&amp;quot;op&amp;quot;`
  Path  string      `json:&amp;quot;path&amp;quot;`
  Value interface{} `json:&amp;quot;value,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For setting the field &lt;code&gt;.spec.securityContext.runAsNonRoot&lt;/code&gt; of a pod to true, we construct the following &lt;code&gt;patchOperation&lt;/code&gt; object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;patches = append(patches, patchOperation{
  Op:    &amp;quot;add&amp;quot;,
  Path:  &amp;quot;/spec/securityContext/runAsNonRoot&amp;quot;,
  Value: true,
})
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tls-certificates&#34;&gt;TLS Certificates&lt;/h2&gt;

&lt;p&gt;Since a webhook must be served via HTTPS, we need proper certificates for the server. These certificates can be self-signed (rather: signed by a self-signed CA), but we need Kubernetes to instruct the respective CA certificate when talking to the webhook server. In addition, the common name (CN) of the certificate must match the server name used by the Kubernetes API server, which for internal services is &lt;code&gt;&amp;lt;service-name&amp;gt;&lt;/code&gt;.&lt;code&gt;&amp;lt;namespace&amp;gt;.svc&lt;/code&gt;, i.e., &lt;code&gt;webhook-server.webhook-demo.svc&lt;/code&gt; in our case. Since the generation of self-signed TLS certificates is well documented across the Internet, we simply refer to the respective &lt;a href=&#34;https://github.com/stackrox/admission-controller-webhook-demo/blob/master/deployment/generate-keys.sh&#34; target=&#34;_blank&#34;&gt;shell script&lt;/a&gt; in our example.&lt;/p&gt;

&lt;p&gt;The webhook configuration shown previously contains a placeholder &lt;code&gt;${CA_PEM_B64}&lt;/code&gt;. Before we can create this configuration, we need to replace this portion with the Base64-encoded PEM certificate of the CA. The &lt;code&gt;openssl base64 -A&lt;/code&gt; command can be used for this purpose.&lt;/p&gt;

&lt;h2 id=&#34;testing-the-webhook&#34;&gt;Testing the Webhook&lt;/h2&gt;

&lt;p&gt;After deploying the webhook server and configuring it, which can be done by invoking the ./deploy.sh script from the repository, it is time to test and verify that the webhook indeed does its job. The repository contains &lt;a href=&#34;https://github.com/stackrox/admission-controller-webhook-demo/tree/master/examples&#34; target=&#34;_blank&#34;&gt;three examples&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A pod that does not specify a security context (&lt;code&gt;pod-with-defaults&lt;/code&gt;). We expect this pod to be run as non-root with user id 1234.&lt;/li&gt;
&lt;li&gt;A pod that does specify a security context, explicitly allowing it to run as root (&lt;code&gt;pod-with-override&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;A pod with a conflicting configuration, specifying it must run as non-root but with a user id of 0 (&lt;code&gt;pod-with-conflict&lt;/code&gt;). To showcase the rejection of object creation requests, we have augmented our admission controller logic to reject such obvious misconfigurations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Create one of these pods by running &lt;code&gt;kubectl create -f examples/&amp;lt;name&amp;gt;.yaml&lt;/code&gt;. In the first two examples, you can verify the user id under which the pod ran by inspecting the logs, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f examples/pod-with-defaults.yaml
$ kubectl logs pod-with-defaults
I am running as user 1234
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the third example, the object creation should be rejected with an appropriate error message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f examples/pod-with-conflict.yaml
Error from server (InternalError): error when creating &amp;quot;examples/pod-with-conflict.yaml&amp;quot;: Internal error occurred: admission webhook &amp;quot;webhook-server.webhook-demo.svc&amp;quot; denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Feel free to test this with your own workloads as well. Of course, you can also experiment a little bit further by changing the logic of the webhook and see how the changes affect object creation. More information on how to do experiment with such changes can be found in the &lt;a href=&#34;https://github.com/stackrox/admission-controller-webhook-demo/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;repository’s readme&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Kubernetes admission controllers offer significant advantages for security. Digging into two powerful examples, with accompanying available code, will help you get started on leveraging these powerful capabilities.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.okd.io/latest/architecture/additional_concepts/dynamic_admission_controllers.html&#34; target=&#34;_blank&#34;&gt;https://docs.okd.io/latest/architecture/additional_concepts/dynamic_admission_controllers.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/ibm-cloud/diving-into-kubernetes-mutatingadmissionwebhook-6ef3c5695f74&#34; target=&#34;_blank&#34;&gt;https://medium.com/ibm-cloud/diving-into-kubernetes-mutatingadmissionwebhook-6ef3c5695f74&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.10.0-beta.1/test/images/webhook/main.go&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/kubernetes/blob/v1.10.0-beta.1/test/images/webhook/main.go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/istio/istio&#34; target=&#34;_blank&#34;&gt;https://github.com/istio/istio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/&#34; target=&#34;_blank&#34;&gt;https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: A Look Back and What&#39;s in Store for Kubernetes Contributor Summits</title>
      <link>https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt; for Barcelona Contributor Summit information.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/celebrationsig.jpg&#34;
         alt=&#34;Seattle Contributor Summit&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Seattle Contributor Summit&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;As our contributing community grows in great numbers, with more than 16,000 contributors this year across 150+ GitHub repositories, it’s important to provide face to face connections for our large distributed teams to have opportunities for collaboration and learning. In &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-contributor-experience&#34; target=&#34;_blank&#34;&gt;Contributor Experience&lt;/a&gt;, our methodology with planning events is a lot like our documentation; we build from personas &amp;ndash; interests, skills, and motivators to name a few. This way we ensure there is valuable content and learning for everyone.&lt;/p&gt;

&lt;p&gt;We build the contributor summits around you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New Contributor&lt;/li&gt;
&lt;li&gt;Current Contributor

&lt;ul&gt;
&lt;li&gt;docs&lt;/li&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;li&gt;community management&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/community-membership.md&#34; target=&#34;_blank&#34;&gt;Subproject OWNERs&lt;/a&gt; - aka maintainers in other OSS circles.&lt;/li&gt;
&lt;li&gt;Special Interest Group (SIG) / Working Group (WG) &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md&#34; target=&#34;_blank&#34;&gt;Chair or Tech Lead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Active Contributors&lt;/li&gt;
&lt;li&gt;Casual Contributors&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/newcontrib.jpg&#34;
         alt=&#34;New Contributor Workshop&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;New Contributor Workshop&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;These personas combined with ample feedback from previous events, produce the   altogether experience that welcomed over 600 contributors in Copenhagen (May), Shanghai(November), and Seattle(December) in 2018. Seattle&amp;rsquo;s event drew over 300+ contributors, equal to Shanghai and Copenhagen combined, for the 6th contributor event in Kubernetes history. In true Kubernetes fashion, we expect another record breaking year of attendance. We&amp;rsquo;ve pre-ordered 900+ &lt;a href=&#34;https://store.cncf.io/collections/kubernetes/products/copy-of-kubernetes-decal&#34; target=&#34;_blank&#34;&gt;contributor patches&lt;/a&gt;, a tradition, and we are   looking forward to giving them to you!&lt;/p&gt;

&lt;p&gt;With that said&amp;hellip;&lt;br /&gt;
&lt;strong&gt;Save the Dates:&lt;/strong&gt;&lt;br /&gt;
Barcelona: May 19th (evening) and 20th (all day)&lt;br /&gt;
Shanghai: June 24th (all day)&lt;br /&gt;
San Diego: November 18th, 19th, and activities in KubeCon/CloudNativeCon week&lt;/p&gt;

&lt;p&gt;In an effort of continual improvement, here&amp;rsquo;s what to expect from us this year:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Large new contributor workshops and contributor socials at all three events expected to break previous attendance records&lt;/li&gt;
&lt;li&gt;A multiple track event in San Diego for all contributor types including workshops, birds of a feather, lightning talks and more&lt;/li&gt;
&lt;li&gt;Addition of a “201” / “Intermediate” edition of the new contributor workshop in San Diego&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;An event website&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Follow along with updates: kubernetes-dev@googlegroups.com is our main communication hub as always; however, we will also blog here, our &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/events/community-meeting.md&#34; target=&#34;_blank&#34;&gt;Thursday Kubernetes Community Meeting&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;twitter&lt;/a&gt;, SIG meetings, event site, discuss.kubernetes.io, and #contributor-summit on Slack.&lt;/li&gt;
&lt;li&gt;Opportunities to get involved: We still have 2019 roles available!
Reach out to Contributor Experience via community@kubernetes.io, stop by a Wednesday SIG update meeting, or catch us on Slack (#sig-contribex).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/unconference.jpg&#34;
         alt=&#34;Unconference voting&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Unconference voting&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;thanks&#34;&gt;Thanks!&lt;/h2&gt;

&lt;p&gt;Our 2018 crew 🥁&lt;br /&gt;
Jorge Castro, Paris Pittman, Bob Killen, Jeff Sica, Megan Lehn, Guinevere Saenger, Josh Berkus, Noah Abrahams, Yang Li, Xiangpeng Zhao, Puja Abbassi, Lindsey Tulloch, Zach Corleissen, Tim Pepper, Ihor Dvoretskyi, Nancy Mohamed, Chris Short, Mario Loria, Jason DeTiberus, Sahdev Zala, Mithra Raja&lt;/p&gt;

&lt;p&gt;And an introduction to our 2019 crew (a thanks in advance ;) )&amp;hellip;&lt;br /&gt;
Jonas Rosland, Josh Berkus, Paris Pittman, Jorge Castro, Bob Killen, Deb Giles, Guinevere Saenger, Noah Abrahams, Yang Li, Xiangpeng Zhao, Puja Abbassi, Rui Chen, Tim Pepper, Ihor Dvoretskyi, Dawn Foster&lt;/p&gt;

&lt;h2 id=&#34;relive-seattle-contributor-summit&#34;&gt;Relive Seattle Contributor Summit&lt;/h2&gt;

&lt;p&gt;📈 80% growth rate since the Austin 2017 December event&lt;/p&gt;

&lt;p&gt;📜 Event waiting list: 103&lt;/p&gt;

&lt;p&gt;🎓 76 contributors were on-boarded through the New Contributor Workshop&lt;/p&gt;

&lt;p&gt;🎉 92% of the current contributors RSVPs attended and of those:&lt;/p&gt;

&lt;p&gt;👩🏻‍🚒 25% were &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Group&lt;/a&gt; or Working Group Chairs or Tech Leads&lt;/p&gt;

&lt;p&gt;🗳 70% were eligible to vote in the last &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/elections.md&#34; target=&#34;_blank&#34;&gt;steering committee election&lt;/a&gt; - more than 50 contributions in 2018&lt;/p&gt;

&lt;p&gt;📹 20+ &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP0kaZWKZc9KizriafE4pzh0&#34; target=&#34;_blank&#34;&gt;Sessions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;👀 Most watched to date: Technical Vision, Security, API Code Base Tour&lt;/p&gt;

&lt;p&gt;🌟 Top 3 according to survey: Live API Code Review, Deflaking Unconference, Technical Vision&lt;/p&gt;

&lt;p&gt;🎱 🎳 160 attendees for the social at &lt;a href=&#34;https://www.garagebilliards.com/&#34; target=&#34;_blank&#34;&gt;Garage&lt;/a&gt; on Sunday night where we sunk eight balls and recorded strikes (out in some cases)&lt;/p&gt;

&lt;p&gt;🏆 Special recognition: SIG Storage, @dims, and @jordan&lt;/p&gt;

&lt;p&gt;📸 Pictures (special thanks to &lt;a href=&#34;https://github.com/rdodev&#34; target=&#34;_blank&#34;&gt;rdodev&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Garage Pic
Reg Desk&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/grouppicseatle.JPG&#34;
         alt=&#34;Some of the group in Seattle&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Some of the group in Seattle&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;“I love Contrib Summit! The intros and deep dives during KubeCon were a great extension of Contrib Summit. Y&amp;rsquo;all did an excellent job in the morning to level set expectations and prime everyone.” &amp;ndash; julianv&lt;br /&gt;
“great work! really useful and fun!” - coffeepac&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: KubeEdge, a Kubernetes Native Edge Computing Framework</title>
      <link>https://kubernetes.io/blog/2019/03/19/kubeedge-k8s-based-edge-intro/</link>
      <pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/19/kubeedge-k8s-based-edge-intro/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Sanil Kumar D (Huawei), Jun Du(Huawei)&lt;/p&gt;

&lt;h2 id=&#34;kubeedge-becomes-the-first-kubernetes-native-edge-computing-platform-with-both-edge-and-cloud-components-open-sourced&#34;&gt;KubeEdge becomes the first Kubernetes Native Edge Computing Platform with both Edge and Cloud components open sourced!&lt;/h2&gt;

&lt;p&gt;Open source edge computing is going through its most dynamic phase of development in the industry. So many open source platforms, so many consolidations and so many initiatives for standardization! This shows the strong drive to build better platforms to bring cloud computing to the edges to meet ever increasing demand. &lt;a href=&#34;https://github.com/kubeedge/kubeedge&#34; target=&#34;_blank&#34;&gt;KubeEdge&lt;/a&gt;, which was announced last year, now brings great news for cloud native computing! It provides a complete edge computing solution based on Kubernetes with separate cloud and edge core modules. Currently, both the cloud and edge modules are open sourced.&lt;/p&gt;

&lt;p&gt;Unlike certain light weight kubernetes platforms available around, KubeEdge is made to build edge computing solutions extending the cloud. The control plane resides in cloud, though scalable and extendable. At the same time, the edge can work in offline mode. Also it is lightweight and containerized, and can support heterogeneous hardware at the edge. With the optimization in edge resource utlization, KubeEdge positions to save significant setup and operation cost for edge solutions. This makes it the most compelling edge computing platform in the world currently, based on Kubernetes!&lt;/p&gt;

&lt;h2 id=&#34;kube-rnetes-edge-opening-up-a-new-kubernetes-based-ecosystem-for-edge-computing&#34;&gt;Kube(rnetes)Edge! - Opening up a new Kubernetes-based ecosystem for Edge Computing&lt;/h2&gt;

&lt;p&gt;The key goal for KubeEdge is extending Kubernetes ecosystem from cloud to edge. From the time it was announced to the public at KubeCon in Shanghai in November 2018, the architecture direction for KubeEdge was aligned to Kubernetes, as its name!&lt;/p&gt;

&lt;p&gt;It started with its v0.1 providing the basic edge computing features. Now, with its latest release v0.2, it brings the cloud components to connect and complete the loop. With consistent and scalable Kubernetes-based interfaces, KubeEdge enables the orchestration and management of edge clusters similar to how Kubernetes manages in the cloud. This opens up seamless possibilities of bringing cloud computing capabilities to the edge, quickly and efficiently.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-12-kubeedge-k8s-based-edge-intro/kubeedge-logo.png&#34;&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KubeEdge Links:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubeedge.io&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeedge/kubeedge&#34; target=&#34;_blank&#34;&gt;SourceCode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.kubeedge.io&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on its roadmap and architecture, KubeEdge tries to support all edge nodes, applications, devices and even the cluster management consistent with the Kuberenetes interface. This will help the edge cloud act exactly like a cloud cluster. This can save a lot of time and cost on the edge cloud development deployment based on KubeEdge.&lt;/p&gt;

&lt;p&gt;KubeEdge provides a containerized edge computing platform, which is inherently scalable. As it&amp;rsquo;s modular and optimized, it is lightweight (66MB foot print and ~30MB running memory) and could be deployed on low resource devices. Similarly, the edge node can be of different hardware architecture and with different hardware configurations. For the device connectivity, it can support multiple protocols and it uses a standard MQTT-based communication. This helps in scaling the edge clusters with new nodes and devices efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;p align=&#34;center&#34;&gt;You heard it right!&lt;/p&gt;&lt;/strong&gt;
&lt;strong&gt;&lt;p align=&#34;center&#34;&gt;KubeEdge Cloud Core modules are open sourced!&lt;/p&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By open sourcing both the edge and cloud modules, KubeEdge brings a complete cloud vendor agnostic lightweight heterogeneous edge computing platform. It is now ready to support building a complete Kubernetes ecosystem for edge computing, exploiting most of the existing cloud native projects or software modules. This can enable a mini-cloud at the edge to support demanding use cases like data analytics, video analytics, machine learning and more.&lt;/p&gt;

&lt;h2 id=&#34;kubeedge-architecture-building-kuberenetes-native-edge-computing&#34;&gt;KubeEdge Architecture: Building Kuberenetes Native Edge computing!&lt;/h2&gt;

&lt;p&gt;The core architecture tenet for KubeEdge is to build interfaces that are consistent with Kubernetes, be it on the cloud side or edge side.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-12-kubeedge-k8s-based-edge-intro/kubeedge-highlevel-arch.png&#34;&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edged&lt;/strong&gt;: Manages containerized Applications at the Edge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EdgeHub&lt;/strong&gt;: Communication interface module at the Edge. It is a web socket client responsible for interacting with Cloud Service for edge computing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CloudHub&lt;/strong&gt;: Communication interface module at the Cloud. A web socket server responsible for watching changes on the cloud side, caching and sending messages to EdgeHub.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EdgeController&lt;/strong&gt;: Manages the Edge nodes. It is an extended Kubernetes controller which manages edge nodes and pods metadata so that the data can be targeted to a specific edge node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EventBus&lt;/strong&gt;: Handles the internal edge communications using MQTT. It is an MQTT client to interact with MQTT servers (mosquitto), offering publish and subscribe capabilities to other components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeviceTwin&lt;/strong&gt;: It is software mirror for devices that handles the device metadata. This module helps in handling device status and syncing the same to cloud. It also provides query interfaces for applications, as it interfaces to a lightweight database (SQLite).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MetaManager&lt;/strong&gt;: It manages the metadata at the edge node. This is the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata to/from a lightweight database (SQLite).&lt;/p&gt;

&lt;p&gt;Even if you want to add more control plane modules based on the architecture refinement and improvement (for example enhanced security), it is simple as it uses consistent registration and modular communication within these modules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;p align=&#34;center&#34;&gt;KubeEdge provides scalable lightweight Kubernetes Native Edge Computing Platform which can work in offline mode.&lt;/p&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;p align=&#34;center&#34;&gt;It helps simplify edge application development and deployment.&lt;/p&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;p align=&#34;center&#34;&gt;Cloud vendor agnostic and can run the cloud core modules on any compute node.&lt;/p&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;release-0-1-to-0-2-game-changer&#34;&gt;Release 0.1 to 0.2 &amp;ndash; game changer!&lt;/h2&gt;

&lt;p&gt;KubeEdge v0.1 was released at the end of December 2018 with very basic edge features to manage edge applications along with Kubernetes API primitives for node, pod, config etc. In ~2 months, KubeEdge v0.2 was release on March 5th, 2019. This release provides the cloud core modules and enables the end to end open source edge computing solution. The cloud core modules can be deployed to any compute node from any cloud vendors or on-prem.&lt;/p&gt;

&lt;p&gt;Now, the complete edge solution can be installed and tested very easily, also with a laptop.&lt;/p&gt;

&lt;h2 id=&#34;run-anywhere-simple-and-light&#34;&gt;Run Anywhere - Simple and Light&lt;/h2&gt;

&lt;p&gt;As described, the KubeEdge Edge and Cloud core components can be deployed easily and can run the user applications. The edge core has a foot print of 66MB and just needs 30MB memory to run. Similarly the cloud core can run on any cloud nodes. (User can experience by running it on a laptop as well)&lt;/p&gt;

&lt;p&gt;The installation is simple and can be done in few steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setup the pre-requisites Docker, Kubernetes, MQTT and openssl&lt;/li&gt;
&lt;li&gt;Clone and Build KubeEdge Cloud and Edge&lt;/li&gt;
&lt;li&gt;Run Cloud&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Run Edge&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The detailed steps for each are available at &lt;a href=&#34;https://github.com/kubeedge/kubeedge&#34; target=&#34;_blank&#34;&gt;KubeEdge/kubeedge&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;future-taking-off-with-competent-features-and-community-collaboration&#34;&gt;Future: Taking off with competent features and community collaboration&lt;/h2&gt;

&lt;p&gt;KubeEdge has been developed by members from the community who are active contributors to Kubernetes/CNCF and doing research in edge computing. The KubeEdge team is also actively collaborating with Kubernetes IOT/EDGE WORKING GROUP. Within a few months of the KubeEdge announcement it has attracted members from different organizations including JingDong, Zhejiang University, SEL Lab, Eclipse, China Mobile, ARM, Intel to collaborate in building the platform and ecosystem.&lt;/p&gt;

&lt;p&gt;KubeEdge has a clear roadmap for its upcoming major releases in 2019. vc1.0 targets to provide a complete edge cluster and device management solution with standard edge to edge communication, while v2.0 targets to have advanced features like service mesh, function service , data analytics etc at edge. Also, for all the features, KubeEdge architecture would attempt to utilize the existing CNCF projects/software.&lt;/p&gt;

&lt;p&gt;The KubeEdge community needs varied organizations, their requirements, use cases and support to build it. Please join to make a kubernetes native edge computing platform which can extend the cloud native computing paradigm to edge cloud.&lt;/p&gt;

&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to Get Involved?&lt;/h2&gt;

&lt;p&gt;We welcome more collaboration to build the Kubernetes native edge computing ecosystem. Please join us!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/kubeedge&#34; target=&#34;_blank&#34;&gt;https://twitter.com/kubeedge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Slack: &lt;a href=&#34;mailto:kubeedge.slack.com&#34; target=&#34;_blank&#34;&gt;kubeedge.slack.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href=&#34;https://kubeedge.io&#34; target=&#34;_blank&#34;&gt;https://kubeedge.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/kubeedge/kubeedge&#34; target=&#34;_blank&#34;&gt;https://github.com/kubeedge/kubeedge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email: &lt;a href=&#34;mailto:kubeedge@gmail.com&#34; target=&#34;_blank&#34;&gt;kubeedge@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Setup Using Ansible and Vagrant</title>
      <link>https://kubernetes.io/blog/2019/03/15/kubernetes-setup-using-ansible-and-vagrant/</link>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/15/kubernetes-setup-using-ansible-and-vagrant/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Naresh L J (Infosys)&lt;/p&gt;

&lt;h2 id=&#34;objective&#34;&gt;Objective&lt;/h2&gt;

&lt;p&gt;This blog post describes the steps required to setup a multi node Kubernetes cluster for development purposes. This setup provides a production-like cluster that can be setup on your local machine.&lt;/p&gt;

&lt;h2 id=&#34;why-do-we-require-multi-node-cluster-setup&#34;&gt;Why do we require multi node cluster setup?&lt;/h2&gt;

&lt;p&gt;Multi node Kubernetes clusters offer a production-like environment which has various advantages. Even though Minikube provides an excellent platform for getting started, it doesn&amp;rsquo;t provide the opportunity to work with multi node clusters which can help solve problems or bugs that are related to application design and architecture. For instance, Ops can reproduce an issue in a multi node cluster environment, Testers can deploy multiple versions of an application for executing test cases and verifying changes. These benefits enable teams to resolve issues faster which make the more agile.&lt;/p&gt;

&lt;h2 id=&#34;why-use-vagrant-and-ansible&#34;&gt;Why use Vagrant and Ansible?&lt;/h2&gt;

&lt;p&gt;Vagrant is a tool that will allow us to create a virtual environment easily and it eliminates pitfalls that cause the works-on-my-machine phenomenon. It can be used with multiple providers such as Oracle VirtualBox, VMware, Docker, and so on. It allows us to create a disposable environment by making use of configuration files.&lt;/p&gt;

&lt;p&gt;Ansible is an infrastructure automation engine that automates software configuration management. It is agentless and allows us to use SSH keys for connecting to remote machines. Ansible playbooks are written in yaml and offer inventory management in simple text files.&lt;/p&gt;

&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vagrant should be installed on your machine. Installation binaries can be found &lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Oracle VirtualBox can be used as a Vagrant provider or make use of similar providers as described in Vagrant&amp;rsquo;s official &lt;a href=&#34;https://www.vagrantup.com/docs/providers/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ansible should be installed in your machine. Refer to the &lt;a href=&#34;https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html&#34; target=&#34;_blank&#34;&gt;Ansible installation guide&lt;/a&gt; for platform specific installation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup-overview&#34;&gt;Setup overview&lt;/h2&gt;

&lt;p&gt;We will be setting up a Kubernetes cluster that will consist of one master and two worker nodes. All the nodes will run Ubuntu Xenial 64-bit OS and Ansible playbooks will be used for provisioning.&lt;/p&gt;

&lt;h4 id=&#34;step-1-creating-a-vagrantfile&#34;&gt;Step 1: Creating a Vagrantfile&lt;/h4&gt;

&lt;p&gt;Use the text editor of your choice and create a file with named &lt;code&gt;Vagrantfile&lt;/code&gt;, inserting the code below. The value of N denotes the number of nodes present in the cluster, it can be modified accordingly. In the below example, we are setting the value of N as 2.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-ruby&#34; data-lang=&#34;ruby&#34;&gt;&lt;span style=&#34;color:#800&#34;&gt;IMAGE_NAME&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bento/ubuntu-16.04&amp;#34;&lt;/span&gt;
N &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;

&lt;span style=&#34;color:#800&#34;&gt;Vagrant&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;configure(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;config&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
    config&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;ssh&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;insert_key &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;false&lt;/span&gt;

    config&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;provider &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;virtualbox&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;v&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
        v&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;memory &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1024&lt;/span&gt;
        v&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;cpus &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;
      
    config&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;define &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s-master&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;master&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
        master&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;box &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#800&#34;&gt;IMAGE_NAME&lt;/span&gt;
        master&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;network &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;private_network&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b8860b&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.10&amp;#34;&lt;/span&gt;
        master&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;hostname &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s-master&amp;#34;&lt;/span&gt;
        master&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;provision &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ansible&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;ansible&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
            ansible&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;playbook &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;kubernetes-setup/master-playbook.yml&amp;#34;&lt;/span&gt;
            ansible&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;extra_vars &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {
                &lt;span style=&#34;color:#b8860b&#34;&gt;node_ip&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.10&amp;#34;&lt;/span&gt;,
            }
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;

    (&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;..&lt;/span&gt;N)&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;each &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;i&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
        config&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;define &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;node-&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;#{&lt;/span&gt;i&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;node&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
            node&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;box &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#800&#34;&gt;IMAGE_NAME&lt;/span&gt;
            node&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;network &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;private_network&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b8860b&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;#{&lt;/span&gt;i &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;
            node&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;hostname &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;node-&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;#{&lt;/span&gt;i&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;
            node&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;vm&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;provision &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ansible&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;ansible&lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
                ansible&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;playbook &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;kubernetes-setup/node-playbook.yml&amp;#34;&lt;/span&gt;
                ansible&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;extra_vars &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {
                    &lt;span style=&#34;color:#b8860b&#34;&gt;node_ip&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;#{&lt;/span&gt;i &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;,
                }
            &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;step-2-create-an-ansible-playbook-for-kubernetes-master&#34;&gt;Step 2: Create an Ansible playbook for Kubernetes master.&lt;/h3&gt;

&lt;p&gt;Create a directory named &lt;code&gt;kubernetes-setup&lt;/code&gt; in the same directory as the &lt;code&gt;Vagrantfile&lt;/code&gt;. Create two files named &lt;code&gt;master-playbook.yml&lt;/code&gt; and &lt;code&gt;node-playbook.yml&lt;/code&gt; in the directory &lt;code&gt;kubernetes-setup&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the file &lt;code&gt;master-playbook.yml&lt;/code&gt;, add the code below.&lt;/p&gt;

&lt;h4 id=&#34;step-2-1-install-docker-and-its-dependent-components&#34;&gt;Step 2.1: Install Docker and its dependent components.&lt;/h4&gt;

&lt;p&gt;We will be installing the following packages, and then adding a user named “vagrant” to the “docker” group.
- docker-ce
- docker-ce-cli
- containerd.io&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hosts:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;all&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;become:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;tasks:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Install&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;packages&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;that&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;allow&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;be&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;used&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;over&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ packages }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;update_cache:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;yes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;vars:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;packages:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt-transport-https&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-certificates&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;curl&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gnupg-agent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;software-properties-common&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Add&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;an&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;signing&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;key&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;for&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Docker&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt_key:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;url:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://download.docker.com/linux/ubuntu/gpg&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Add&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;repository&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;for&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;stable&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;version&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt_repository:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;repo:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deb&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[arch=amd64]&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://download.docker.com/linux/ubuntu&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;xenial&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;stable&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Install&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;and&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;its&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;dependecies&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ packages }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;update_cache:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;yes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;vars:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;packages:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker-ce&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker-ce-cli&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;containerd.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;notify:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;status&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Add&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vagrant&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;user&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;group&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;user:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vagrant&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;group:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-2-kubelet-will-not-start-if-the-system-has-swap-enabled-so-we-are-disabling-swap-using-the-below-code&#34;&gt;Step 2.2: Kubelet will not start if the system has swap enabled, so we are disabling swap using the below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Remove&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;swapfile&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;from&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/fstab&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;mount:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ item }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;fstype:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;swap&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;absent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;with_items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;swap&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;none&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Disable&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;swap&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;swapoff&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-a&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;when:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ansible_swaptotal_mb&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-3-installing-kubelet-kubeadm-and-kubectl-using-the-below-code&#34;&gt;Step 2.3: Installing kubelet, kubeadm and kubectl using the below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Add&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;an&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;signing&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;key&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;for&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt_key:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;url:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://packages.cloud.google.com/apt/doc/apt-key.gpg&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Adding&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apt&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;repository&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;for&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt_repository:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;repo:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deb&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://apt.kubernetes.io/&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubernetes-xenial&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;main&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;filename:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubernetes.list&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Install&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Kubernetes&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;binaries&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apt:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ packages }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;present&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;update_cache:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;yes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;vars:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;packages:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubelet&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubectl&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Configure&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ip&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;lineinfile:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/default/kubelet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;line:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;KUBELET_EXTRA_ARGS=--node-ip={{&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node_ip&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;}}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Restart&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubelet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;service:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubelet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;daemon_reload:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;yes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;restarted&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-3-initialize-the-kubernetes-cluster-with-kubeadm-using-the-below-code-applicable-only-on-master-node&#34;&gt;Step 2.3: Initialize the Kubernetes cluster with kubeadm using the below code (applicable only on master node).&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Initialize&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;the&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Kubernetes&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cluster&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;using&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;init&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--apiserver-advertise-address=&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.10&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--apiserver-cert-extra-sans=&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.50.10&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;--node-name&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8s-master&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--pod-network-cidr=&lt;span style=&#34;color:#666&#34;&gt;192.168.0.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-4-setup-the-kube-config-file-for-the-vagrant-user-to-access-the-kubernetes-cluster-using-the-below-code&#34;&gt;Step 2.4: Setup the kube config file for the vagrant user to access the Kubernetes cluster using the below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Setup&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeconfig&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;for&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vagrant&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;user&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ item }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;with_items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mkdir&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-p&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/home/vagrant/.kube&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cp&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-i&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes/admin.conf&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/home/vagrant/.kube/config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;chown&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vagrant:vagrant&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/home/vagrant/.kube/config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-5-setup-the-container-networking-provider-and-the-network-policy-engine-using-the-below-code&#34;&gt;Step 2.5: Setup the container networking provider and the network policy engine using the below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Install&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;calico&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pod&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;network&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;become:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubectl&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;create&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-f&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://docs.projectcalico.org/v3&lt;span style=&#34;color:#666&#34;&gt;.4&lt;/span&gt;/getting-started/kubernetes/installation/hosted/calico.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-6-generate-kube-join-command-for-joining-the-node-to-the-kubernetes-cluster-and-store-the-command-in-the-file-named-join-command&#34;&gt;Step 2.6: Generate kube join command for joining the node to the Kubernetes cluster and store the command in the file named &lt;code&gt;join-command&lt;/code&gt;.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Generate&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;join&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;token&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;create&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--print-join-command&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;register:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;join_command&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Copy&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;join&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;local&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;file&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;local_action:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;copy&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;content=&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{{ join_command.stdout_lines[0] }}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;dest=&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;./join-command&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-2-7-setup-a-handler-for-checking-docker-daemon-using-the-below-code&#34;&gt;Step 2.7: Setup a handler for checking Docker daemon using the below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;handlers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;status&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;service:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name=docker&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;state=started&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-3-create-the-ansible-playbook-for-kubernetes-node&#34;&gt;Step 3: Create the Ansible playbook for Kubernetes node.&lt;/h4&gt;

&lt;p&gt;Create a file named &lt;code&gt;node-playbook.yml&lt;/code&gt; in the directory &lt;code&gt;kubernetes-setup&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Add the code below into &lt;code&gt;node-playbook.yml&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;step-3-1-start-adding-the-code-from-steps-2-1-till-2-3&#34;&gt;Step 3.1: Start adding the code from Steps 2.1 till 2.3.&lt;/h4&gt;

&lt;h4 id=&#34;step-3-2-join-the-nodes-to-the-kubernetes-cluster-using-below-code&#34;&gt;Step 3.2: Join the nodes to the Kubernetes cluster using below code.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Copy&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;the&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;join&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;server&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;location&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;copy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;src=join-command&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;dest=/tmp/join-command.sh&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mode=&lt;span style=&#34;color:#666&#34;&gt;0777&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Join&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;the&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cluster&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;sh&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/tmp/join-command.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;step-3-3-add-the-code-from-step-2-7-to-finish-this-playbook&#34;&gt;Step 3.3: Add the code from step 2.7 to finish this playbook.&lt;/h4&gt;

&lt;h4 id=&#34;step-4-upon-completing-the-vagrantfile-and-playbooks-follow-the-below-steps&#34;&gt;Step 4: Upon completing the Vagrantfile and playbooks follow the below steps.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ &lt;span style=&#34;color:#a2f&#34;&gt;cd&lt;/span&gt; /path/to/Vagrantfile
$ vagrant up&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Upon completion of all the above steps, the Kubernetes cluster should be up and running.
We can login to the master or worker nodes using Vagrant as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;## Accessing master&lt;/span&gt;
$ vagrant ssh k8s-master
vagrant@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   18m     v1.13.3
node-1       Ready    &amp;lt;none&amp;gt;   12m     v1.13.3
node-2       Ready    &amp;lt;none&amp;gt;   6m22s   v1.13.3

$ &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;## Accessing nodes&lt;/span&gt;
$ vagrant ssh node-1
$ vagrant ssh node-2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Blog: Raw Block Volume support to Beta</title>
      <link>https://kubernetes.io/blog/2019/03/07/raw-block-volume-support-to-beta/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/07/raw-block-volume-support-to-beta/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p&gt;

&lt;p&gt;Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.&lt;/p&gt;

&lt;h2 id=&#34;what-are-block-devices&#34;&gt;What are block devices?&lt;/h2&gt;

&lt;p&gt;Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.&lt;/p&gt;

&lt;p&gt;Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.&lt;/p&gt;

&lt;h2 id=&#34;why-add-raw-block-volumes-to-kubernetes&#34;&gt;Why add raw block volumes to kubernetes?&lt;/h2&gt;

&lt;p&gt;There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).&lt;/p&gt;

&lt;p&gt;From a programmer&amp;rsquo;s perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.&lt;/p&gt;

&lt;p&gt;As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-raw-blocks&#34;&gt;Which volume plugins support raw blocks?&lt;/h2&gt;

&lt;p&gt;As of the publishing of this blog, the following in-tree volumes types support raw blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AWS EBS&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;Fibre Channel&lt;/li&gt;
&lt;li&gt;GCE PD&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;Local volumes&lt;/li&gt;
&lt;li&gt;RBD (Ceph)&lt;/li&gt;
&lt;li&gt;Vsphere&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Out-of-tree &lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34; target=&#34;_blank&#34;&gt;CSI volume drivers&lt;/a&gt; may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation &lt;a href=&#34;https://kubernetes-csi.github.io/docs/raw-block.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-raw-block-volume-api&#34;&gt;Kubernetes raw block volume API&lt;/h2&gt;

&lt;p&gt;Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; objects which bind to &lt;code&gt;PersistentVolume&lt;/code&gt; objects, and are attached to Pods in Kubernetes by including them in the volumes array of the &lt;code&gt;PodSpec&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are 2 important differences however. First, to request a raw block &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, you must set &lt;code&gt;volumeMode = &amp;quot;Block&amp;quot;&lt;/code&gt; in the &lt;code&gt;PersistentVolumeClaimSpec&lt;/code&gt;. Leaving &lt;code&gt;volumeMode&lt;/code&gt; blank is the same as specifying &lt;code&gt;volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code&gt; which results in the traditional behavior. &lt;code&gt;PersistentVolumes&lt;/code&gt; also have a &lt;code&gt;volumeMode&lt;/code&gt; field in their &lt;code&gt;PersistentVolumeSpec&lt;/code&gt;, and &lt;code&gt;&amp;quot;Block&amp;quot;&lt;/code&gt; type PVCs can only bind to &lt;code&gt;&amp;quot;Block&amp;quot;&lt;/code&gt; type PVs and &lt;code&gt;&amp;quot;Filesystem&amp;quot;&lt;/code&gt; PVCs can only bind to &lt;code&gt;&amp;quot;Filesystem&amp;quot;&lt;/code&gt; PVs.&lt;/p&gt;

&lt;p&gt;Secondly, when using a raw block volume in your Pods, you must specify a &lt;code&gt;VolumeDevice&lt;/code&gt; in the Container portion of the &lt;code&gt;PodSpec&lt;/code&gt; rather than a &lt;code&gt;VolumeMount&lt;/code&gt;. &lt;code&gt;VolumeDevices&lt;/code&gt; have &lt;code&gt;devicePaths&lt;/code&gt; instead of &lt;code&gt;mountPaths&lt;/code&gt;, and inside the container, applications will see a device at that path instead of a mounted file system.&lt;/p&gt;

&lt;p&gt;Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-new-raw-block-pvc&#34;&gt;Creating a new raw block PVC&lt;/h2&gt;

&lt;p&gt;First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Block
  storageClassName: my-sc
  resources:
    requests:
    storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-a-raw-block-pvc&#34;&gt;Using a raw block PVC&lt;/h2&gt;

&lt;p&gt;When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: busybox
      command:
        - sleep
        - “3600”
      volumeDevices:
        - devicePath: /dev/block
          name: my-volume
      imagePullPolicy: IfNotPresent
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-raw-block-devices-to-my-csi-plugin&#34;&gt;As a storage vendor, how do I add support for raw block devices to my CSI plugin?&lt;/h2&gt;

&lt;p&gt;Raw block support for CSI plugins is still alpha, but support can be added today. The &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;CSI specification&lt;/a&gt; details how to handle requests for volume that have the &lt;code&gt;BlockVolume&lt;/code&gt; capability instead of the &lt;code&gt;MountVolume&lt;/code&gt; capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see &lt;a href=&#34;https://kubernetes-csi.github.io/docs/raw-block.html&#34; target=&#34;_blank&#34;&gt;documentation here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;issues-gotchas&#34;&gt;Issues/gotchas&lt;/h2&gt;

&lt;p&gt;Because block devices are actually devices, it’s possible to do low-level actions on them from inside containers that wouldn’t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.&lt;/p&gt;

&lt;p&gt;By default, Linux won’t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the &lt;code&gt;SYS_RAWIO&lt;/code&gt; capability to the container security context to allow this. See documentation &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, while Kubernetes is guaranteed to deliver a block device to the container, there’s no guarantee that it’s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support&#34;&gt;Raw Block Volume Support&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How do I get involved?&lt;/p&gt;

&lt;p&gt;Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a&gt;
&lt;a href=&#34;https://github.com/container-storage-interface/community/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors who helped add block volume support to Kubernetes including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ben Swartzlander (&lt;a href=&#34;https://github.com/bswartz&#34; target=&#34;_blank&#34;&gt;https://github.com/bswartz&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Brad Childs (&lt;a href=&#34;https://github.com/childsb&#34; target=&#34;_blank&#34;&gt;https://github.com/childsb&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Erin Boyd (&lt;a href=&#34;https://github.com/erinboyd&#34; target=&#34;_blank&#34;&gt;https://github.com/erinboyd&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Masaki Kimura (&lt;a href=&#34;https://github.com/mkimuram&#34; target=&#34;_blank&#34;&gt;https://github.com/mkimuram&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Matthew Wong (&lt;a href=&#34;https://github.com/wongma7&#34; target=&#34;_blank&#34;&gt;https://github.com/wongma7&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;https://github.com/msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mitsuhiro Tanino (&lt;a href=&#34;https://github.com/mtanino&#34; target=&#34;_blank&#34;&gt;https://github.com/mtanino&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;https://github.com/saad-ali&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Automate Operations on your Cluster with OperatorHub.io</title>
      <link>https://kubernetes.io/blog/2019/02/28/automate-operations-on-your-cluster-with-operatorhub.io/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/02/28/automate-operations-on-your-cluster-with-operatorhub.io/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;
Diane Mueller, Director of Community Development, Cloud Platforms, Red Hat&lt;/p&gt;

&lt;p&gt;One of the important challenges facing developers and Kubernetes administrators has been a lack of ability to quickly find common services that are operationally ready for Kubernetes. Typically, the presence of an Operator for a specific service - a pattern that was introduced in 2016 and has gained momentum - is a good signal for the operational readiness of the service on Kubernetes. However, there has to date not existed a registry of Operators to simplify the discovery of such services.&lt;/p&gt;

&lt;p&gt;To help address this challenge, today Red Hat is launching OperatorHub.io in collaboration with AWS, Google Cloud and Microsoft. OperatorHub.io enables developers and Kubernetes administrators to find and install curated Operator-backed services with a base level of documentation, active maintainership by communities or vendors, basic testing, and packaging for optimized life-cycle management on Kubernetes.&lt;/p&gt;

&lt;p&gt;The Operators currently in OperatorHub.io are just the start. We invite the Kubernetes community to join us in building a vibrant community for Operators by developing, packaging, and publishing Operators on OperatorHub.io.&lt;/p&gt;

&lt;h2 id=&#34;what-does-operatorhub-io-provide&#34;&gt;What does OperatorHub.io provide?&lt;/h2&gt;

&lt;p&gt;OperatorHub.io is designed to address the needs of both Kubernetes developers and users. For the former it provides a common registry where they can publish their Operators alongside with descriptions, relevant details like version, image, code repository and have them be readily packaged for installation. They can also update already published Operators to new versions when they are released.&lt;/p&gt;

&lt;p&gt;Users get the ability to discover and download Operators at a central location, that has content which has been screened for the previously mentioned criteria and scanned for known vulnerabilities. In addition, developers can guide users of their Operators with prescriptive examples of the &lt;code&gt;CustomResources&lt;/code&gt; that they introduce to interact with the application.&lt;/p&gt;

&lt;h2 id=&#34;what-is-an-operator&#34;&gt;What is an Operator?&lt;/h2&gt;

&lt;p&gt;Operators were first introduced in 2016 by CoreOS and have been used by Red Hat and the Kubernetes community as a way to package, deploy and manage a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and well-known tooling, like kubectl.&lt;/p&gt;

&lt;p&gt;An Operator is implemented as a custom controller that watches for certain Kubernetes resources to appear, be modified or deleted. These are typically &lt;code&gt;CustomResourceDefinitions&lt;/code&gt; that the Operator “owns.” In the spec properties of these objects the user declares the desired state of the application or the operation. The Operator’s reconciliation loop will pick these up and perform the required actions to achieve the desired state. For example, the intent to create a highly available etcd cluster could be expressed by creating an new resource of type &lt;code&gt;EtcdCluster&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: &amp;quot;etcd.database.coreos.com/v1beta2&amp;quot;
kind: &amp;quot;EtcdCluster&amp;quot;
metadata:
  name: &amp;quot;my-etcd-cluster&amp;quot;
spec:
  size: 3
  version: &amp;quot;3.3.12&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;EtcdOperator&lt;/code&gt; would be responsible for creating a 3-node etcd cluster running version v3.3.12 as a result. Similarly, an object of type &lt;code&gt;EtcdBackup&lt;/code&gt; could be defined to express the intent to create a consistent backup of the etcd database to an S3 bucket.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-create-and-run-an-operator&#34;&gt;How do I create and run an Operator?&lt;/h2&gt;

&lt;p&gt;One way to get started is with the &lt;a href=&#34;https://github.com/operator-framework&#34; target=&#34;_blank&#34;&gt;Operator Framework&lt;/a&gt;, an open source toolkit that provides an SDK, lifecycle management, metering and monitoring capabilities. It enables developers to build, test, and package Operators. Operators can be implemented in several programming and automation languages, including Go, Helm, and Ansible, all three of which are supported directly by the SDK.&lt;/p&gt;

&lt;p&gt;If you are interested in creating your own Operator, we recommend checking out the Operator Framework to &lt;a href=&#34;https://github.com/operator-framework/getting-started&#34; target=&#34;_blank&#34;&gt;get started&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Operators vary in where they fall along &lt;a href=&#34;https://github.com/operator-framework/operator-sdk/blob/master/doc/images/operator-capability-level.png&#34; target=&#34;_blank&#34;&gt;the capability spectrum&lt;/a&gt; ranging from basic functionality to having specific operational logic for an application to automate advanced scenarios like backup, restore or tuning. Beyond basic installation, advanced Operators are designed to handle upgrades more seamlessly and react to failures automatically. Currently, Operators on OperatorHub.io span the maturity spectrum, but we anticipate their continuing maturation over time.&lt;/p&gt;

&lt;p&gt;While Operators on OperatorHub.io don’t need to be implemented using the SDK, they are packaged for deployment through the &lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager&#34; target=&#34;_blank&#34;&gt;Operator Lifecycle Manager&lt;/a&gt; (OLM). The format mainly consists of a YAML manifest referred to as &lt;code&gt;[ClusterServiceVersion]&lt;/code&gt;(&lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager/blob/master/doc/design/building-your-csv.md&#34; target=&#34;_blank&#34;&gt;https://github.com/operator-framework/operator-lifecycle-manager/blob/master/doc/design/building-your-csv.md&lt;/a&gt;) which provides information about the &lt;code&gt;CustomResourceDefinitions&lt;/code&gt; the Operator owns or requires, which RBAC definition it needs, where the image is stored, etc. This file is usually accompanied by additional YAML files which define the Operators’ own CRDs. This information is processed by OLM at the time a user requests to install an Operator to provide dependency resolution and automation.&lt;/p&gt;

&lt;h2 id=&#34;what-does-listing-of-an-operator-on-operatorhub-io-mean&#34;&gt;What does listing of an Operator on OperatorHub.io mean?&lt;/h2&gt;

&lt;p&gt;To be listed, Operators must successfully show cluster lifecycle features, be packaged as a CSV to be maintained through OLM, and have acceptable documentation for its intended users.&lt;/p&gt;

&lt;p&gt;Some examples of Operators that are currently listed on OperatorHub.io include: Amazon Web Services Operator, Couchbase Autonomous Operator, CrunchyData’s PostgreSQL, etcd Operator, Jaeger Operator for Kubernetes, Kubernetes Federation Operator, MongoDB Enterprise Operator, Percona MySQL Operator, PlanetScale’s Vitess Operator, Prometheus Operator, and Redis Operator.&lt;/p&gt;

&lt;h2 id=&#34;want-to-add-your-operator-to-operatorhub-io-follow-these-steps&#34;&gt;Want to add your Operator to OperatorHub.io? Follow these steps&lt;/h2&gt;

&lt;p&gt;If you have an existing Operator, follow the &lt;a href=&#34;https://www.operatorhub.io/contribute&#34; target=&#34;_blank&#34;&gt;contribution guide&lt;/a&gt; using a fork of the &lt;a href=&#34;https://github.com/operator-framework/community-operators/&#34; target=&#34;_blank&#34;&gt;community-operators&lt;/a&gt; repository. Each contribution contains the CSV, all of the &lt;code&gt;CustomResourceDefinitions&lt;/code&gt;, access control rules and references to the container image needed to install and run your Operator, plus other info like a description of its features and supported Kubernetes versions. A complete example, including multiple versions of the Operator, can be found with the &lt;a href=&#34;https://github.com/operator-framework/community-operators/tree/master/community-operators/etcd&#34; target=&#34;_blank&#34;&gt;EtcdOperator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After testing out your Operator on your own cluster, submit a PR to the &lt;a href=&#34;https://github.com/operator-framework/community-operators&#34; target=&#34;_blank&#34;&gt;community repository&lt;/a&gt; with all of YAML files following &lt;a href=&#34;https://github.com/operator-framework/community-operators#adding-your-operator&#34; target=&#34;_blank&#34;&gt;this directory structure&lt;/a&gt;. Subsequent versions of the Operator can be published in the same way. At first this will be reviewed manually, but automation is on the way. After it’s merged by the maintainers, it will show up on OperatorHub.io along with its documentation and a convenient installation method.&lt;/p&gt;

&lt;h2 id=&#34;want-to-learn-more&#34;&gt;Want to learn more?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Attend one of the upcoming Kubernetes Operator Framework hands-on workshops at &lt;a href=&#34;https://www.socallinuxexpo.org/scale/17x/presentations/workshop-kubernetes-operator-framework&#34; target=&#34;_blank&#34;&gt;ScaleX&lt;/a&gt; in Pasadena on March 7 and at the &lt;a href=&#34;https://commons.openshift.org/gatherings/Santa_Clara_2019.html&#34; target=&#34;_blank&#34;&gt;OpenShift Commons Gathering on Operating at Scale in Santa Clara on March 11&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Listen to this &lt;a href=&#34;https://www.youtube.com/watch?v=GgEKEYH9MMM&amp;amp;feature=youtu.be&#34; target=&#34;_blank&#34;&gt;OpenShift Commons Briefing on “The State of Operators” with Daniel Messer and Diane Mueller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join in on the online conversations in the community &lt;a href=&#34;https://kubernetes.slack.com/messages/CAW0GV7A5&#34; target=&#34;_blank&#34;&gt;Kubernetes-Operator Slack Channel&lt;/a&gt; and the &lt;a href=&#34;https://groups.google.com/forum/#!forum/operator-framework&#34; target=&#34;_blank&#34;&gt;Operator Framework Google Group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finally, read up on how to add your Operator to OperatorHub.io: &lt;a href=&#34;https://operatorhub.io/contribute&#34; target=&#34;_blank&#34;&gt;https://operatorhub.io/contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Building a Kubernetes Edge (Ingress) Control Plane for Envoy v2</title>
      <link>https://kubernetes.io/blog/2019/02/12/building-a-kubernetes-edge-control-plane-for-envoy-v2/</link>
      <pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/02/12/building-a-kubernetes-edge-control-plane-for-envoy-v2/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;
Daniel Bryant, Product Architect, Datawire;
Flynn, Ambassador Lead Developer, Datawire;
Richard Li, CEO and Co-founder, Datawire&lt;/p&gt;

&lt;p&gt;Kubernetes has become the de facto runtime for container-based microservice applications, but this orchestration framework alone does not provide all of the infrastructure necessary for running a distributed system. Microservices typically communicate through Layer 7 protocols such as HTTP, gRPC, or WebSockets, and therefore having the ability to make routing decisions, manipulate protocol metadata, and observe at this layer is vital. However, traditional load balancers and edge proxies have predominantly focused on L3/4 traffic. This is where the &lt;a href=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34;&gt;Envoy Proxy&lt;/a&gt; comes into play.&lt;/p&gt;

&lt;p&gt;Envoy proxy was designed as a &lt;a href=&#34;https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a&#34; target=&#34;_blank&#34;&gt;universal data plane&lt;/a&gt; from the ground-up by the Lyft Engineering team for today&amp;rsquo;s distributed, L7-centric world, with broad support for L7 protocols, a real-time API for managing its configuration, first-class observability, and high performance within a small memory footprint. However, Envoy&amp;rsquo;s vast feature set and flexibility of operation also makes its configuration highly complicated &amp;ndash; this is evident from looking at its rich but verbose &lt;a href=&#34;https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc&#34; target=&#34;_blank&#34;&gt;control plane&lt;/a&gt; syntax.&lt;/p&gt;

&lt;p&gt;With the open source &lt;a href=&#34;https://www.getambassador.io&#34; target=&#34;_blank&#34;&gt;Ambassador API Gateway&lt;/a&gt;, we wanted to tackle the challenge of creating a new control plane that focuses on the use case of deploying Envoy as an forward-facing edge proxy within a Kubernetes cluster, in a way that is idiomatic to Kubernetes operators. In this article, we&amp;rsquo;ll walk through two major iterations of the Ambassador design, and how we integrated Ambassador with Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;ambassador-pre-2019-envoy-v1-apis-jinja-template-files-and-hot-restarts&#34;&gt;Ambassador pre-2019: Envoy v1 APIs, Jinja Template Files, and Hot Restarts&lt;/h2&gt;

&lt;p&gt;Ambassador itself is deployed within a container as a Kubernetes service, and uses annotations added to Kubernetes Services as its &lt;a href=&#34;https://www.getambassador.io/reference/configuration&#34; target=&#34;_blank&#34;&gt;core configuration model&lt;/a&gt;. This approach &lt;a href=&#34;https://www.getambassador.io/concepts/developers&#34; target=&#34;_blank&#34;&gt;enables application developers to manage routing&lt;/a&gt; as part of the Kubernetes service definition. We explicitly decided to go down this route because of &lt;a href=&#34;https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d&#34; target=&#34;_blank&#34;&gt;limitations&lt;/a&gt; in the current &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress API spec&lt;/a&gt;, and we liked the simplicity of extending Kubernetes services, rather than introducing another custom resource type. An example of an Ambassador annotation can be seen here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  annotations:
    getambassador.io/config: |
      ---
        apiVersion: ambassador/v0
        kind:  Mapping
        name:  my_service_mapping
        prefix: /my-service/
        service: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Translating this simple Ambassador annotation config into valid &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/v1.6.0/configuration/overview/v1_overview&#34; target=&#34;_blank&#34;&gt;Envoy v1&lt;/a&gt; config was not a trivial task. By design, Ambassador&amp;rsquo;s configuration isn&amp;rsquo;t based on the same conceptual model as Envoy&amp;rsquo;s configuration &amp;ndash; we deliberately wanted to aggregate and simplify operations and config. Therefore, translating between one set of concepts to the other involves a fair amount of logic within Ambassador.&lt;/p&gt;

&lt;p&gt;In this first iteration of Ambassador we created a Python-based service that watched the Kubernetes API for changes to Service objects. When new or updated Ambassador annotations were detected, these were translated from the Ambassador syntax into an intermediate representation (IR) which embodied our core configuration model and concepts. Next, Ambassador translated this IR into a representative Envoy configuration which was saved as a file within pods associated with the running Ambassador k8s Service. Ambassador then &amp;ldquo;hot-restarted&amp;rdquo; the Envoy process running within the Ambassador pods, which triggered the loading of the new configuration.&lt;/p&gt;

&lt;p&gt;There were many benefits with this initial implementation. The mechanics involved were fundamentally simple, the transformation of Ambassador config into Envoy config was reliable, and the file-based hot restart integration with Envoy was dependable.&lt;/p&gt;

&lt;p&gt;However, there were also notable challenges with this version of Ambassador. First, although the hot restart was effective for the majority of our customers&amp;rsquo; use cases, it was not very fast, and some customers (particularly those with huge application deployments) found it was limiting the frequency with which they could change their configuration. Hot restart can also drop connections, especially long-lived connections like WebSockets or gRPC streams.&lt;/p&gt;

&lt;p&gt;More crucially, though, the first implementation of the IR allowed rapid prototyping but was primitive enough that it proved very difficult to make substantial changes. While this was a pain point from the beginning, it became a critical issue as Envoy shifted to the &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/configuration/overview/v2_overview&#34; target=&#34;_blank&#34;&gt;Envoy v2 API&lt;/a&gt;. It was clear that the v2 API would offer Ambassador many benefits &amp;ndash; as Matt Klein outlined in his blog post, &amp;ldquo;&lt;a href=&#34;https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a&#34; target=&#34;_blank&#34;&gt;The universal data plane API&lt;/a&gt;&amp;rdquo; &amp;ndash; including access to new features and a solution to the connection-drop problem noted above, but it was also clear that the existing IR implementation was not capable of making the leap.&lt;/p&gt;

&lt;h2 id=&#34;ambassador-v0-50-envoy-v2-apis-ads-testing-with-kat-and-golang&#34;&gt;Ambassador &amp;gt;= v0.50: Envoy v2 APIs (ADS), Testing with KAT, and Golang&lt;/h2&gt;

&lt;p&gt;In consultation with the &lt;a href=&#34;http://d6e.co/slack&#34; target=&#34;_blank&#34;&gt;Ambassador community&lt;/a&gt;, the &lt;a href=&#34;https://www.datawire.io&#34; target=&#34;_blank&#34;&gt;Datawire&lt;/a&gt; team undertook a redesign of the internals of Ambassador in 2018. This was driven by two key goals. First, we wanted to integrate Envoy&amp;rsquo;s v2 configuration format, which would enable the support of features such as &lt;a href=&#34;https://www.getambassador.io/user-guide/sni/&#34; target=&#34;_blank&#34;&gt;SNI&lt;/a&gt;, &lt;a href=&#34;https://www.getambassador.io/user-guide/rate-limiting&#34; target=&#34;_blank&#34;&gt;rate limiting&lt;/a&gt; and &lt;a href=&#34;https://www.getambassador.io/user-guide/auth-tutorial&#34; target=&#34;_blank&#34;&gt;gRPC authentication APIs&lt;/a&gt;. Second, we also wanted to do much more robust semantic validation of Envoy configuration due to its increasing complexity (particularly when operating with large-scale application deployments).&lt;/p&gt;

&lt;h3 id=&#34;initial-stages&#34;&gt;Initial stages&lt;/h3&gt;

&lt;p&gt;We started by restructuring the Ambassador internals more along the lines of a multipass compiler. The class hierarchy was made to more closely mirror the separation of concerns between the Ambassador configuration resources, the IR, and the Envoy configuration resources. Core parts of Ambassador were also redesigned to facilitate contributions from the community outside Datawire. We decided to take this approach for several reasons. First, Envoy Proxy is a very fast moving project, and we realized that we needed an approach where a seemingly minor Envoy configuration change didn&amp;rsquo;t result in days of reengineering within Ambassador. In addition, we wanted to be able to provide semantic verification of configuration.&lt;/p&gt;

&lt;p&gt;As we started working more closely with Envoy v2, a testing challenge was quickly identified. As more and more features were being supported in Ambassador, more and more bugs appeared in Ambassador&amp;rsquo;s handling of less common but completely valid combinations of features. This drove to creation of a new testing requirement that meant Ambassador&amp;rsquo;s test suite needed to be reworked to automatically manage many combinations of features, rather than relying on humans to write each test individually. Moreover, we wanted the test suite to be fast in order to maximize engineering productivity.&lt;/p&gt;

&lt;p&gt;Thus, as part of the Ambassador rearchitecture, we introduced the &lt;a href=&#34;https://github.com/datawire/ambassador/tree/master/python/kat&#34; target=&#34;_blank&#34;&gt;Kubernetes Acceptance Test (KAT)&lt;/a&gt; framework. KAT is an extensible test framework that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deploys a bunch of services (along with Ambassador) to a Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Run a series of verification queries against the spun up APIs&lt;/li&gt;
&lt;li&gt;Perform a bunch of assertions on those query results&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;KAT is designed for performance &amp;ndash; it batches test setup upfront, and then runs all the queries in step 3 asynchronously with a high performance client. The traffic driver in KAT runs locally using &lt;a href=&#34;https://www.telepresence.io&#34; target=&#34;_blank&#34;&gt;Telepresence&lt;/a&gt;, which makes it easier to debug issues.&lt;/p&gt;

&lt;h3 id=&#34;introducing-golang-to-the-ambassador-stack&#34;&gt;Introducing Golang to the Ambassador Stack&lt;/h3&gt;

&lt;p&gt;With the KAT test framework in place, we quickly ran into some issues with Envoy v2 configuration and hot restart, which presented the opportunity to switch to use Envoy’s Aggregated Discovery Service (ADS) APIs instead of hot restart. This completely eliminated the requirement for restart on configuration changes, which we found could lead to dropped connection under high loads or long-lived connections.&lt;/p&gt;

&lt;p&gt;However, we faced an interesting question as we considered the move to the ADS. The ADS is not as simple as one might expect: there are explicit ordering dependencies when sending updates to Envoy. The Envoy project has reference implementations of the ordering logic, but only in Go and Java, where Ambassador was primarily in Python. We agonized a bit, and decided that the simplest way forward was to accept the polyglot nature of our world, and do our ADS implementation in Go.&lt;/p&gt;

&lt;p&gt;We also found, with KAT,  that our testing had reached the point where Python’s performance with many network connections was a limitation, so we took advantage of Go here, as well, writing KAT’s querying and backend services primarily in Go. After all, what’s another Golang dependency when you’ve already taken the plunge?&lt;/p&gt;

&lt;p&gt;With a new test framework, new IR generating valid Envoy v2 configuration, and the ADS, we thought we were done with the major architectural changes in Ambassador 0.50. Alas, we hit one more issue. On the Azure Kubernetes Service, Ambassador annotation changes were no longer being detected.&lt;/p&gt;

&lt;p&gt;Working with the highly-responsive AKS engineering team, we were able to identify the issue &amp;ndash; namely, the Kubernetes API server in AKS is exposed through a chain of proxies, requiring clients to be updating to understand how to connect using the FQDN of the API server, which is provided through a mutating webhook in AKS. Unfortunately, support for this feature was not available in the official Kubernetes Python client, so this was the third spot where we chose to switch to Go instead of Python.&lt;/p&gt;

&lt;p&gt;This raises the interesting question of, “why not ditch all the Python code, and just rewrite Ambassador entirely in Go?” It’s a valid question. The main concern with a rewrite is that Ambassador and Envoy operate at different conceptual levels rather than simply expressing the same concepts with different syntax. Being certain that we’ve expressed the conceptual bridges in a new language is not a trivial challenge, and not something to undertake without already having really excellent test coverage in place&lt;/p&gt;

&lt;p&gt;At this point, we use Go to coverage very specific, well-contained functions that can be verified for correctness much more easily that we could verify a complete Golang rewrite. In the future, who knows? But for 0.50.0, this functional split let us both take advantage of Golang’s strengths, while letting us retain more confidence about all the changes already in 0.50.&lt;/p&gt;

&lt;h2 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve learned a lot in the process of building &lt;a href=&#34;https://blog.getambassador.io/ambassador-0-50-ga-release-notes-sni-new-authservice-and-envoy-v2-support-3b30a4d04c81&#34; target=&#34;_blank&#34;&gt;Ambassador 0.50&lt;/a&gt;. Some of our key takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes and Envoy are very powerful frameworks, but they are also extremely fast moving targets &amp;ndash; there is sometimes no substitute for reading the source code and talking to the maintainers (who are fortunately all quite accessible!)&lt;/li&gt;
&lt;li&gt;The best supported libraries in the Kubernetes / Envoy ecosystem are written in Go. While we love Python, we have had to adopt Go so that we&amp;rsquo;re not forced to maintain too many components ourselves.&lt;/li&gt;
&lt;li&gt;Redesigning a test harness is sometimes necessary to move your software forward.&lt;/li&gt;
&lt;li&gt;The real cost in redesigning a test harness is often in porting your old tests to the new harness implementation.&lt;/li&gt;
&lt;li&gt;Designing (and implementing) an effective control plane for the edge proxy use case has been challenging, and the feedback from the open source community around Kubernetes, Envoy and Ambassador has been extremely useful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Migrating Ambassador to the Envoy v2 configuration and ADS APIs was a long and difficult journey that required lots of architecture and design discussions and plenty of coding, but early feedback from results have been positive. &lt;a href=&#34;https://blog.getambassador.io/announcing-ambassador-0-50-8dffab5b05e0&#34; target=&#34;_blank&#34;&gt;Ambassador 0.50 is available now&lt;/a&gt;, so you can take it for a test run and share your feedback with the community on our &lt;a href=&#34;http://d6e.co/slack&#34; target=&#34;_blank&#34;&gt;Slack channel&lt;/a&gt; or on &lt;a href=&#34;https://www.twitter.com/getambassadorio&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Runc and CVE-2019-5736</title>
      <link>https://kubernetes.io/blog/2019/02/11/runc-and-cve-2019-5736/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/02/11/runc-and-cve-2019-5736/</guid>
      <description>
        
        
        

&lt;p&gt;This morning &lt;a href=&#34;https://www.openwall.com/lists/oss-security/2019/02/11/2&#34; target=&#34;_blank&#34;&gt;a container escape vulnerability in runc was announced&lt;/a&gt;. We wanted to provide some guidance to Kubernetes users to ensure everyone is safe and secure.&lt;/p&gt;

&lt;h2 id=&#34;what-is-runc&#34;&gt;What Is Runc?&lt;/h2&gt;

&lt;p&gt;Very briefly, runc is the low-level tool which does the heavy lifting of spawning a Linux container. Other tools like Docker, Containerd, and CRI-O sit on top of runc to deal with things like data formatting and serialization, but runc is at the heart of all of these systems.&lt;/p&gt;

&lt;p&gt;Kubernetes in turn sits on top of those tools, and so while no part of Kubernetes itself is vulnerable, most Kubernetes installations are using runc under the hood.&lt;/p&gt;

&lt;h3 id=&#34;what-is-the-vulnerability&#34;&gt;What Is The Vulnerability?&lt;/h3&gt;

&lt;p&gt;While full details are still embargoed to give people time to patch, the rough version is that when running a process as root (UID 0) inside a container, that process can exploit a bug in runc to gain root privileges on the host running the container. This then allows them unlimited access to the server as well as any other containers on that server.&lt;/p&gt;

&lt;p&gt;If the process inside the container is either trusted (something you know is not hostile) or is not running as UID 0, then the vulnerability does not apply. It can also be prevented by SELinux, if an appropriate policy has been applied. RedHat Enterprise Linux and CentOS both include appropriate SELinux permissions with their packages and so are believed to be unaffected if SELinux is enabled.&lt;/p&gt;

&lt;p&gt;The most common source of risk is attacker-controller container images, such as unvetted images from public repositories.&lt;/p&gt;

&lt;h3 id=&#34;what-should-i-do&#34;&gt;What Should I Do?&lt;/h3&gt;

&lt;p&gt;As with all security issues, the two main options are to mitigate the vulnerability or upgrade your version of runc to one that includes the fix.&lt;/p&gt;

&lt;p&gt;As the exploit requires UID 0 within the container, a direct mitigation is to ensure all your containers are running as a non-0 user. This can be set within the container image, or via your pod specification:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;run-as-uid&lt;span style=&#34;color:#666&#34;&gt;-1000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can also be enforced globally using a PodSecurityPolicy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;policy/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PodSecurityPolicy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;non-root&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;allowPrivilegeEscalation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Require the container to run without root privileges.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;rule:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;MustRunAsNonRoot&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Setting a policy like this is highly encouraged given the overall risks of running as UID 0 inside a container.&lt;/p&gt;

&lt;p&gt;Another potential mitigation is to ensure all your container images are vetted and trusted. This can be accomplished by building all your images yourself, or by vetting the contents of an image and then pinning to the image version hash (&lt;code&gt;image: external/someimage@sha256:7832659873hacdef&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Upgrading runc can generally be accomplished by upgrading the package &lt;code&gt;runc&lt;/code&gt; for your distribution or by upgrading your OS image if using immutable images. This is a list of known safe versions for various distributions and platforms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu - &lt;a href=&#34;https://people.canonical.com/~ubuntu-security/cve/2019/CVE-2019-5736.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;runc 1.0.0~rc4+dfsg1-6ubuntu0.18.10.1&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Debian - &lt;a href=&#34;https://security-tracker.debian.org/tracker/CVE-2019-5736&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;runc 1.0.0~rc6+dfsg1-2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RedHat Enterprise Linux - &lt;a href=&#34;https://access.redhat.com/security/vulnerabilities/runcescape&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docker 1.13.1-91.git07f3374.el7&lt;/code&gt;&lt;/a&gt; (if SELinux is disabled)&lt;/li&gt;
&lt;li&gt;Amazon Linux - &lt;a href=&#34;https://alas.aws.amazon.com/ALAS-2019-1156.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docker 18.06.1ce-7.25.amzn1.x86_64&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoreOS - Stable: &lt;a href=&#34;https://coreos.com/releases/#1967.5.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;1967.5.0&lt;/code&gt;&lt;/a&gt; / Beta: &lt;a href=&#34;https://coreos.com/releases/#2023.2.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;2023.2.0&lt;/code&gt;&lt;/a&gt; / Alpha: &lt;a href=&#34;https://coreos.com/releases/#2051.0.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;2051.0.0&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kops Debian - &lt;a href=&#34;https://github.com/kubernetes/kops/pull/6460&#34; target=&#34;_blank&#34;&gt;in progress&lt;/a&gt; (see &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md&#34; target=&#34;_blank&#34;&gt;advisory&lt;/a&gt; for how to address until Kops Debian is patched)&lt;/li&gt;
&lt;li&gt;Docker - &lt;a href=&#34;https://github.com/docker/docker-ce/releases/tag/v18.09.2&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;18.09.2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some platforms have also posted more specific instructions:&lt;/p&gt;

&lt;h4 id=&#34;google-container-engine-gke&#34;&gt;Google Container Engine (GKE)&lt;/h4&gt;

&lt;p&gt;Google has issued a &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/security-bulletins#february-11-2019-runc&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with more detailed information but in short, if you are using the default GKE node image then you are safe. If you are using an Ubuntu node image then you will need to mitigate or upgrade to an image with a fixed version of runc.&lt;/p&gt;

&lt;h4 id=&#34;amazon-elastic-container-service-for-kubernetes-eks&#34;&gt;Amazon Elastic Container Service for Kubernetes (EKS)&lt;/h4&gt;

&lt;p&gt;Amazon has also issued a &lt;a href=&#34;https://aws.amazon.com/security/security-bulletins/AWS-2019-002/&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with more detailed information. All EKS users should mitigate the issue or upgrade to a new node image.&lt;/p&gt;

&lt;h4 id=&#34;azure-kubernetes-service-aks&#34;&gt;Azure Kubernetes Service (AKS)&lt;/h4&gt;

&lt;p&gt;Microsoft has issued a &lt;a href=&#34;https://azure.microsoft.com/en-us/updates/cve-2019-5736-and-runc-vulnerability/&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with detailed information on mitigating the issue. Microsoft recommends all AKS users to upgrade their cluster to mitigate the issue.&lt;/p&gt;

&lt;h4 id=&#34;kops&#34;&gt;Kops&lt;/h4&gt;

&lt;p&gt;Kops has issued an &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md&#34; target=&#34;_blank&#34;&gt;advisory&lt;/a&gt; with detailed information on mitigating this issue.&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;We don&amp;rsquo;t have specific confirmation that Docker for Mac and Docker for Windows are vulnerable, however it seems likely. Docker has released a fix in &lt;a href=&#34;https://github.com/docker/docker-ce/releases/tag/v18.09.2&#34; target=&#34;_blank&#34;&gt;version 18.09.2&lt;/a&gt; and it is recommended you upgrade to it. This also applies to other deploy systems using Docker under the hood.&lt;/p&gt;

&lt;p&gt;If you are unable to upgrade Docker, the Rancher team has provided backports of the fix for many older versions at &lt;a href=&#34;https://github.com/rancher/runc-cve&#34; target=&#34;_blank&#34;&gt;github.com/rancher/runc-cve&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;getting-more-information&#34;&gt;Getting More Information&lt;/h2&gt;

&lt;p&gt;If you have any further questions about how this vulnerability impacts Kubernetes, please join us at &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;discuss.kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you would like to get in contact with the &lt;a href=&#34;https://github.com/opencontainers/org/blob/master/README.md#communications&#34; target=&#34;_blank&#34;&gt;runc team&lt;/a&gt;, you can reach them on &lt;a href=&#34;https://groups.google.com/a/opencontainers.org/forum/#!forum/dev&#34; target=&#34;_blank&#34;&gt;Google Groups&lt;/a&gt; or &lt;code&gt;#opencontainers&lt;/code&gt; on Freenode IRC.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Poseidon-Firmament Scheduler – Flow Network Graph Based Scheduler</title>
      <link>https://kubernetes.io/blog/2019/02/06/poseidon-firmament-scheduler-flow-network-graph-based-scheduler/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/02/06/poseidon-firmament-scheduler-flow-network-graph-based-scheduler/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Deepak Vij (Huawei), Shivram Shrivastava (Huawei)&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Cluster Management systems such as Mesos, Google Borg, Kubernetes etc. in a cloud scale datacenter environment (also termed as &lt;strong&gt;&lt;em&gt;Datacenter-as-a-Computer&lt;/em&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;em&gt;Warehouse-Scale Computing - WSC&lt;/em&gt;&lt;/strong&gt;) typically manage application workloads by performing tasks such as tracking machine live-ness, starting, monitoring, terminating workloads and more importantly using a &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; to decide on workload placements.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; essentially performs the scheduling of workloads to compute resources – combining the global placement of work across the WSC environment makes the “warehouse-scale computer” more efficient, increases utilization, and saves energy. &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; examples are Google Borg, Kubernetes, Firmament, Mesos, Tarcil, Quasar, Quincy, Swarm, YARN, Nomad, Sparrow, Apollo etc.&lt;/p&gt;

&lt;p&gt;In this blog post, we briefly describe the novel Firmament flow network graph based scheduling approach (&lt;a href=&#34;https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gog&#34; target=&#34;_blank&#34;&gt;OSDI paper&lt;/a&gt;) in Kubernetes. We specifically describe the Firmament Scheduler and how it integrates with the Kubernetes cluster manager using Poseidon as the integration glue. We have seen extremely impressive scheduling throughput performance benchmarking numbers with this novel scheduling approach. Originally, Firmament Scheduler was conceptualized, designed and implemented by University of Cambridge researchers, &lt;a href=&#34;http://www.malteschwarzkopf.de/&#34; target=&#34;_blank&#34;&gt;Malte Schwarzkopf&lt;/a&gt; &amp;amp; &lt;a href=&#34;http://ionelgog.org/&#34; target=&#34;_blank&#34;&gt;Ionel Gog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;poseidon-firmament-scheduler-how-it-works&#34;&gt;Poseidon-Firmament Scheduler – How It Works&lt;/h2&gt;

&lt;p&gt;At a very high level, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; augments the current Kubernetes scheduling capabilities by incorporating novel flow network graph based scheduling capabilities alongside the default Kubernetes Scheduler. It models the scheduling problem as a constraint-based optimization over a flow network graph – by reducing scheduling to a min-cost max-flow optimization problem. Due to the inherent rescheduling capabilities, the new scheduler enables a globally optimal scheduling environment that constantly keeps refining the workloads placements dynamically.&lt;/p&gt;

&lt;h2 id=&#34;key-advantages&#34;&gt;Key Advantages&lt;/h2&gt;

&lt;p&gt;Flow graph scheduling based &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; provides the following key advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Workloads (pods) are bulk scheduled to enable scheduling decisions at massive scale.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Based on the extensive performance test results, Poseidon-Firmament scales much better than Kubernetes default scheduler as the number of nodes increase in a cluster. This is due to the fact that Poseidon-Firmament is able to amortize more and more work across workloads.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Poseidon-Firmament Scheduler outperforms the Kubernetes default scheduler by a wide margin when it comes to throughput performance numbers for scenarios where compute resource requirements are somewhat uniform across jobs (Replicasets/Deployments/Jobs). Poseidon-Firmament scheduler end-to-end throughput performance numbers, including bind time, consistently get better as the number of nodes in a cluster increase. For example, for a 2,700 node cluster (shown in the graphs &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon/blob/master/docs/benchmark/README.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), Poseidon-Firmament scheduler achieves a 7X or greater end-to-end throughput than the Kubernetes default scheduler, which includes bind time.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Availability of complex rule constraints.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scheduling in Poseidon-Firmament is very dynamic; it keeps cluster resources in a global optimal state during every scheduling run.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Highly efficient resource utilizations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;firmament-flow-network-graph-an-overview&#34;&gt;Firmament Flow Network Graph – An Overview&lt;/h2&gt;

&lt;p&gt;Firmament scheduler runs a min-cost flow algorithm over the flow network to find an optimal flow, from which it extracts the implied workload (pod placements). A flow network is a directed graph whose arcs carry flow from source nodes (i.e. pod nodes) to a sink node. A cost and capacity associated with each arc constrain the flow, and specify preferential routes for it.&lt;/p&gt;

&lt;p&gt;Figure 1 below shows an example of a flow network for a cluster with two tasks (workloads or pods) and four machines (nodes) – each workload on the left hand side, is a source of one unit of flow. All such flow must be drained into the sink node (S) for a feasible solution to the optimization problem.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-02-03-poseidon-firmament-scheduler/example-of-a-flow-network.png&#34;
         alt=&#34;Figure 1. Example of a Flow Network&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 1. Example of a Flow Network&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;poseidon-mediation-layer-an-overview&#34;&gt;Poseidon Mediation Layer – An Overview&lt;/h2&gt;

&lt;p&gt;Poseidon is a service that acts as the integration glue for the Firmament scheduler with Kubernetes. It augments the current Kubernetes scheduling capabilities by incorporating new flow network graph based Firmament scheduling capabilities alongside the default Kubernetes Scheduler; multiple schedulers running simultaneously. Figure 2 below describes the high level overall design as far as how Poseidon integration glue works in conjunction with the underlying Firmament flow network graph based scheduler.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-02-03-poseidon-firmament-scheduler/firmament-kubernetes-integration-overview.png&#34;
         alt=&#34;Figure 2. Firmament Kubernetes Integration Overview&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 2. Firmament Kubernetes Integration Overview&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;As part of the Kubernetes multiple schedulers support, each new pod is typically scheduled by the default scheduler, but Kubernetes can be instructed to use another scheduler by specifying the name of another custom scheduler (in our case, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34;&gt;Poseidon-Firmament&lt;/a&gt;) at the time of pod deployment. In this case, the default scheduler will ignore that Pod and allow Poseidon scheduler to schedule the Pod to a relevant node.&lt;/p&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; For details about the design of this project see the &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon/blob/master/docs/design/README.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;possible-use-case-scenarios-when-to-use-it&#34;&gt;Possible Use Case Scenarios – When To Use It&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; enables extremely high throughput scheduling environment at scale due to its bulk scheduling approach superiority versus K8s pod-at-a-time approach. In our extensive tests, we have observed substantial throughput benefits as long as resource requirements (CPU/Memory) for incoming Pods is uniform across jobs (Replicasets/Deployments/Jobs), mainly due to efficient amortization of work across jobs.&lt;/p&gt;

&lt;p&gt;Although, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; is capable of scheduling various types of workloads (service, batch, etc.), following are the few use cases where it excels the most:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For “Big Data/AI” jobs consisting of a large number of tasks, throughput benefits are tremendous.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Substantial throughput benefits also for service or batch job scenarios where workload resource requirements are uniform across jobs (Replicasets/Deplyments/Jobs).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;current-project-stage&#34;&gt;Current Project Stage&lt;/h2&gt;

&lt;p&gt;Currently Poseidon-Firmament project is an incubation project. Alpha Release is available at &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-sigs/poseidon&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Update on Volume Snapshot Alpha for Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Xu (Google), Xing Yang (Huawei), Saad Ali (Google)&lt;/p&gt;

&lt;p&gt;Volume snapshotting support was introduced in Kubernetes v1.12 as an alpha feature. In Kubernetes v1.13, it remains an alpha feature, but a few enhancements were added and some breaking changes were made. This post summarizes the changes.&lt;/p&gt;

&lt;h2 id=&#34;breaking-changes&#34;&gt;Breaking Changes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v1.0.0&#34; target=&#34;_blank&#34;&gt;CSI spec v1.0&lt;/a&gt; introduced a few breaking changes to the volume snapshot feature. CSI driver maintainers should be aware of these changes as they upgrade their drivers to support v1.0.&lt;/p&gt;

&lt;h2 id=&#34;snapshotstatus-replaced-with-boolean-readytouse&#34;&gt;SnapshotStatus replaced with Boolean ReadyToUse&lt;/h2&gt;

&lt;p&gt;CSI v0.3.0, defined a &lt;code&gt;SnapshotStatus&lt;/code&gt; enum in &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; which indicates whether the snapshot is &lt;code&gt;READY&lt;/code&gt;, &lt;code&gt;UPLOADING&lt;/code&gt;, or &lt;code&gt;ERROR_UPLOADING&lt;/code&gt;. In CSI v1.0, &lt;code&gt;SnapshotStatus&lt;/code&gt; has been removed from &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; and replaced with a &lt;code&gt;boolean ReadyToUse&lt;/code&gt;. A &lt;code&gt;ReadyToUse&lt;/code&gt; value of &lt;code&gt;true&lt;/code&gt; indicates that post snapshot processing (such as uploading) is complete and the snapshot is ready to be used as a source to create a volume.&lt;/p&gt;

&lt;p&gt;Storage systems that need to do post snapshot processing (such as uploading after the snapshot is cut) should return a successful &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; with the &lt;code&gt;ReadyToUse&lt;/code&gt; field set to &lt;code&gt;false&lt;/code&gt; as soon as the snapshot has been taken.  This indicates that the Container Orchestration System (CO) can resume any workload that was quiesced for the snapshot to be taken. The CO can then repeatedly call &lt;code&gt;CreateSnapshot&lt;/code&gt; until the &lt;code&gt;ReadyToUse&lt;/code&gt; field is set to &lt;code&gt;true&lt;/code&gt; or the call returns an error indicating a problem in processing. The CSI &lt;code&gt;ListSnapshot&lt;/code&gt; call could be used along with &lt;code&gt;snapshot_id&lt;/code&gt; filtering to determine if the snapshot is ready to use, but is not recommended because it provides no way to detect errors during processing (the &lt;code&gt;ReadyToUse&lt;/code&gt; field simply remains &lt;code&gt;false&lt;/code&gt; indefinitely).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1&#34; target=&#34;_blank&#34;&gt;v1.x.x releases&lt;/a&gt; of the CSI external-snapshotter sidecar container already handle this change by calling &lt;code&gt;CreateSnapshot&lt;/code&gt; instead of &lt;code&gt;ListSnapshots&lt;/code&gt; to check if a snapshot is ready to use. When upgrading their drivers to CSI 1.0, driver maintainers should use the appropriate 1.0 compatible sidecar container.&lt;/p&gt;

&lt;p&gt;To be consistent with the change in the CSI spec, the &lt;code&gt;Ready&lt;/code&gt; field in the &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object has been renamed to &lt;code&gt;ReadyToUse&lt;/code&gt;. This change is visible to the user when running &lt;code&gt;kubectl describe volumesnapshot&lt;/code&gt; to view the details of a snapshot.&lt;/p&gt;

&lt;h2 id=&#34;timestamp-data-type&#34;&gt;Timestamp Data Type&lt;/h2&gt;

&lt;p&gt;The creation time of a snapshot is available to Kubernetes admins as part of the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; API object. This field is populated using the &lt;code&gt;creation_time&lt;/code&gt; field in the CSI &lt;code&gt;CreateSnapshotResponse&lt;/code&gt;. In CSI v1.0, this &lt;code&gt;creation_time&lt;/code&gt; field type was changed to &lt;a href=&#34;https://godoc.org/github.com/golang/protobuf/ptypes/timestamp&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;.google.protobuf.Timestamp&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;int64&lt;/code&gt;. When upgrading drivers to CSI 1.0, driver maintainers must make changes accordingly. The &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1&#34; target=&#34;_blank&#34;&gt;v1.x.x releases&lt;/a&gt; of the CSI external-snapshotter sidecar container has been updated to handle this change.&lt;/p&gt;

&lt;h2 id=&#34;deprecations&#34;&gt;Deprecations&lt;/h2&gt;

&lt;p&gt;The following &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; parameters are deprecated and will be removed in a future release.  They will be replaced with parameters listed in the &lt;code&gt;Replacement&lt;/code&gt; section below.&lt;/p&gt;

&lt;p&gt;Deprecated
Replacement
csiSnapshotterSecretName
csi.storage.k8s.io/snapshotter-secret-name
csiSnapshotterSecretNameSpace
csi.storage.k8s.io/snapshotter-secret-namespace&lt;/p&gt;

&lt;h2 id=&#34;new-features&#34;&gt;New Features&lt;/h2&gt;

&lt;h3 id=&#34;snapshotcontent-deletion-retain-policy&#34;&gt;SnapshotContent Deletion/Retain Policy&lt;/h3&gt;

&lt;p&gt;As described in the &lt;a href=&#34;https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;initial blog post announcing the snapshot alpha&lt;/a&gt;, the Kubernetes snapshot APIs are similar to the PV/PVC APIs: just like a volume is represented by a bound PVC and PV pair, a snapshot is represented by a bound &lt;code&gt;VolumeSnapshot&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; pair.&lt;/p&gt;

&lt;p&gt;With PV/PVC pairs, when a user is done with a volume, they can delete the PVC. And the reclaim policy on the PV determines what happens to the PV (whether it is also deleted or retained).&lt;/p&gt;

&lt;p&gt;In the initial alpha release, snapshots did not support the ability to specify a reclaim policy. Instead when a snapshot object was deleted it always resulted in the snapshot being deleted.  In Kubernetes v1.13, a snapshot content &lt;code&gt;DeletionPolicy&lt;/code&gt; was added. It enables an admin to configure what what happens to a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; after the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object it is bound to is deleted. The &lt;code&gt;DeletionPolicy&lt;/code&gt; of a volume snapshot can either be &lt;code&gt;Retain&lt;/code&gt; or &lt;code&gt;Delete&lt;/code&gt;. If the value is not specified, the default depends on whether the &lt;code&gt;SnapshotContent&lt;/code&gt; object was created via static binding or dynamic provisioning.&lt;/p&gt;

&lt;h3 id=&#34;retain&#34;&gt;Retain&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;Retain&lt;/code&gt; policy allows for manual reclamation of the resource. If a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is statically created and bound, the default &lt;code&gt;DeletionPolicy&lt;/code&gt; is &lt;code&gt;Retain&lt;/code&gt;. When the &lt;code&gt;VolumeSnapshot&lt;/code&gt; is deleted, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; continues to exist and the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is considered “released”. But it is not available for binding to other &lt;code&gt;VolumeSnapshot&lt;/code&gt; objects because it contains data. It is up to an administrator to decide how to handle the remaining API object and resource cleanup.&lt;/p&gt;

&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;

&lt;p&gt;A &lt;code&gt;Delete&lt;/code&gt; policy enables automatic deletion of the bound &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object from Kubernetes and the associated storage asset in the external infrastructure (such as an AWS EBS snapshot or GCE PD snapshot, etc.). Snapshots that are dynamically provisioned inherit the deletion policy of their &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;&lt;code&gt;VolumeSnapshotClass&lt;/code&gt;&lt;/a&gt;, which defaults to &lt;code&gt;Delete&lt;/code&gt;. The administrator should configure the &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; with the desired retention policy. The policy may be changed for individual &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; after it is created by patching the object.&lt;/p&gt;

&lt;p&gt;The following example demonstrates how to check the deletion policy of a dynamically provisioned &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f ./examples/kubernetes/demo-defaultsnapshotclass.yaml
$ kubectl create -f ./examples/kubernetes/demo-snapshot.yaml
$ kubectl get volumesnapshots demo-snapshot-podpvc -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  creationTimestamp: &amp;quot;2018-11-27T23:57:09Z&amp;quot;
...
spec:
  snapshotClassName: default-snapshot-class
  snapshotContentName: snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002
  source:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: podpvc
status:
…
$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
…
spec:
  csiVolumeSnapshotSource:
    creationTime: 1546469777852000000
    driver: pd.csi.storage.gke.io
    restoreSize: 6442450944
    snapshotHandle: projects/jing-k8s-dev/global/snapshots/snapshot-26cd0db3-f2a0-11e8-8be6-42010a800002
  deletionPolicy: Delete
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
    resourceVersion: &amp;quot;21117&amp;quot;
    uid: ae400e9f-f28b-11e8-8be6-42010a800002
  snapshotClassName: default-snapshot-class
  volumeSnapshotRef:
    apiVersion: snapshot.storage.k8s.io/v1alpha1
    kind: VolumeSnapshot
    name: demo-snapshot-podpvc
    namespace: default
    resourceVersion: &amp;quot;6948065&amp;quot;
    uid: 26cd0db3-f2a0-11e8-8be6-42010a800002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User can change the deletion policy by using patch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;deletionPolicy&amp;quot;:&amp;quot;Retain&amp;quot;}}&#39; --type=merge

$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
...
spec:
  csiVolumeSnapshotSource:
...
  deletionPolicy: Retain
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;snapshot-object-in-use-protection&#34;&gt;Snapshot Object in Use Protection&lt;/h2&gt;

&lt;p&gt;The purpose of the Snapshot Object in Use Protection feature is to ensure that in-use snapshot API objects are not removed from the system (as this may result in data loss). There are two cases that require “in-use” protection:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If a volume snapshot is in active use by a persistent volume claim as a source to create a volume.&lt;/li&gt;
&lt;li&gt;If a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; API object is bound to a VolumeSnapshot API object, the content object is considered in use.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If a user deletes a &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object in active use by a PVC, the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is not removed immediately. Instead, removal of the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is postponed until the &lt;code&gt;VolumeSnapshot&lt;/code&gt; is no longer actively used by any PVCs. Similarly, if an admin deletes a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; that is bound to a &lt;code&gt;VolumeSnapshot&lt;/code&gt;, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is not removed immediately. Instead, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; removal is postponed until the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is not bound to the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-kubernetes-snapshots&#34;&gt;Which volume plugins support Kubernetes Snapshots?&lt;/h2&gt;

&lt;p&gt;Snapshots are only supported for CSI drivers (not for in-tree or FlexVolume). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.&lt;/p&gt;

&lt;p&gt;As of the publishing of this blog post, the following CSI drivers support snapshots:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34; target=&#34;_blank&#34;&gt;GCE Persistent Disk CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/opensds/nbp/tree/master/csi/server&#34; target=&#34;_blank&#34;&gt;OpenSDS CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ceph/ceph-csi/tree/master/pkg/rbd&#34; target=&#34;_blank&#34;&gt;Ceph RBD CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/libopenstorage/openstorage/tree/master/csi&#34; target=&#34;_blank&#34;&gt;Portworx CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gluster/gluster-csi-driver&#34; target=&#34;_blank&#34;&gt;GlusterFS CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/digitalocean/csi-digitalocean&#34; target=&#34;_blank&#34;&gt;DigitalOcean CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/embercsi/ember-csi&#34; target=&#34;_blank&#34;&gt;Ember CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder&#34; target=&#34;_blank&#34;&gt;Cinder CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Datera/datera-csi&#34; target=&#34;_blank&#34;&gt;Datera CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Nexenta/nexentastor-csi-driver&#34; target=&#34;_blank&#34;&gt;NexentaStor CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Snapshot support for other &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;drivers&lt;/a&gt; is pending, and should be available soon. Read the “Container Storage Interface (CSI) for Kubernetes GA” blog post to learn more about CSI and how to deploy CSI drivers.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to beta in either 1.15 or 1.16. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, and more.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;The code repository for snapshot APIs and controller is here: &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-csi/external-snapshotter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;http://k8s.io/docs/concepts/storage/volume-snapshots&#34; target=&#34;_blank&#34;&gt;http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors that helped add CSI v1.0 support and improve the snapshot feature in this release, including Saad Ali (&lt;a href=&#34;https://github.com/saadali&#34; target=&#34;_blank&#34;&gt;saadali&lt;/a&gt;), Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;), Deep Debroy (&lt;a href=&#34;https://github.com/ddebroy&#34; target=&#34;_blank&#34;&gt;ddebroy&lt;/a&gt;), James DeFelice (&lt;a href=&#34;https://github.com/jdef&#34; target=&#34;_blank&#34;&gt;jdef&lt;/a&gt;), John Griffith (&lt;a href=&#34;https://github.com/j-griffith&#34; target=&#34;_blank&#34;&gt;j-griffith&lt;/a&gt;), Julian Hjortshoj (&lt;a href=&#34;https://github.com/julian-hj&#34; target=&#34;_blank&#34;&gt;julian-hj&lt;/a&gt;), Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;), Patrick Ohly (&lt;a href=&#34;https://github.com/pohly&#34; target=&#34;_blank&#34;&gt;pohly&lt;/a&gt;), Luis Pabon (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;), Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;), Jing Xu (&lt;a href=&#34;https://github.com/jingxu97&#34; target=&#34;_blank&#34;&gt;jingxu97&lt;/a&gt;), Shiwei Xu (&lt;a href=&#34;https://github.com/wackxu&#34; target=&#34;_blank&#34;&gt;wackxu&lt;/a&gt;), Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;), Jie Yu (&lt;a href=&#34;https://github.com/jieyu&#34; target=&#34;_blank&#34;&gt;jieyu&lt;/a&gt;), David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;We also hold regular &lt;a href=&#34;https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;SIG-Storage Snapshot Working Group meetings&lt;/a&gt;. New attendees are welcome to join for design and development discussions.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Container Storage Interface (CSI) for Kubernetes GA</title>
      <link>https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-kubernetes.png&#34; alt=&#34;Kubernetes Logo&#34; /&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-logo.png&#34; alt=&#34;CSI Logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Saad Ali, Senior Software Engineer, Google&lt;/p&gt;

&lt;p&gt;The Kubernetes implementation of the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;Container Storage Interface&lt;/a&gt; (CSI) has been promoted to GA in the Kubernetes v1.13 release. Support for CSI was &lt;a href=&#34;http://blog.kubernetes.io/2018/01/introducing-container-storage-interface.html&#34; target=&#34;_blank&#34;&gt;introduced as alpha&lt;/a&gt; in Kubernetes v1.9 release, and &lt;a href=&#34;https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/&#34; target=&#34;_blank&#34;&gt;promoted to beta&lt;/a&gt; in the Kubernetes v1.10 release.&lt;/p&gt;

&lt;p&gt;The GA milestone indicates that Kubernetes users may depend on the feature and its API without fear of backwards incompatible changes in future causing regressions. GA features are protected by the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;Kubernetes deprecation policy&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-csi&#34;&gt;Why CSI?&lt;/h2&gt;

&lt;p&gt;Although prior to CSI Kubernetes provided a powerful volume plugin system, it was challenging to add support for new volume plugins to Kubernetes: volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain.&lt;/p&gt;

&lt;p&gt;CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new&#34;&gt;What’s new?&lt;/h2&gt;

&lt;p&gt;With the promotion to GA, the Kubernetes implementation of CSI introduces the following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes is now compatible with CSI spec &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v1.0.0&#34; target=&#34;_blank&#34;&gt;v1.0&lt;/a&gt; and &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.3.0&#34; target=&#34;_blank&#34;&gt;v0.3&lt;/a&gt; (instead of CSI spec &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.2.0&#34; target=&#34;_blank&#34;&gt;v0.2&lt;/a&gt;).

&lt;ul&gt;
&lt;li&gt;There were breaking changes between CSI spec v0.3.0 and v1.0.0, but Kubernetes v1.13 supports both versions so either version will work with Kubernetes v1.13.&lt;/li&gt;
&lt;li&gt;Please note that with the release of the CSI 1.0 API, support for CSI drivers using 0.3 and older releases of the CSI API is deprecated, and is planned to be removed in Kubernetes v1.15.&lt;/li&gt;
&lt;li&gt;There were no breaking changes between CSI spec v0.2 and v0.3, so v0.2 drivers should also work with Kubernetes v1.10.0+.&lt;/li&gt;
&lt;li&gt;There were breaking changes between the CSI spec v0.1 and v0.2, so very old drivers implementing CSI 0.1 must be updated to be at least 0.2 compatible before use with Kubernetes v1.10.0+.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; object (introduced in v1.9 in the storage v1alpha1 group, and added to the v1beta1 group in v1.10) has been added to the storage v1 group in v1.13.&lt;/li&gt;
&lt;li&gt;The Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; volume type has been promoted to GA.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration&#34;&gt;Kubelet device plugin registration mechanism&lt;/a&gt;, which is the means by which kubelet discovers new CSI drivers, has been promoted to GA in Kubernetes v1.13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-deploy-a-csi-driver&#34;&gt;How to deploy a CSI driver?&lt;/h2&gt;

&lt;p&gt;Kubernetes users interested in how to deploy or manage an existing CSI driver on Kubernetes should look at the documentation provided by the author of the CSI driver.&lt;/p&gt;

&lt;h2 id=&#34;how-to-use-a-csi-volume&#34;&gt;How to use a CSI volume?&lt;/h2&gt;

&lt;p&gt;Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects: &lt;code&gt;PersistentVolumeClaims&lt;/code&gt;, &lt;code&gt;PersistentVolumes&lt;/code&gt;, and &lt;code&gt;StorageClasses&lt;/code&gt;. Documented &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#csi&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although the Kubernetes implementation of CSI is a GA feature in Kubernetes v1.13, it may require the following flag:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;API server binary and kubelet binaries:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--allow-privileged=true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Most CSI plugins will require bidirectional mount propagation, which can only be enabled for privileged pods. Privileged pods are only permitted on clusters where this flag has been set to true (this is the default in some environments like GCE, GKE, and kubeadm).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dynamic-provisioning&#34;&gt;Dynamic Provisioning&lt;/h3&gt;

&lt;p&gt;You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a &lt;code&gt;StorageClass&lt;/code&gt; pointing to the CSI plugin.&lt;/p&gt;

&lt;p&gt;The following StorageClass, for example, enables dynamic creation of “&lt;code&gt;fast-storage&lt;/code&gt;” volumes by a CSI volume plugin called “&lt;code&gt;csi-driver.example.com&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-storage
provisioner: csi-driver.example.com
parameters:
  type: pd-ssd
  csi.storage.k8s.io/provisioner-secret-name: mysecret
  csi.storage.k8s.io/provisioner-secret-namespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;New for GA, the &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;CSI external-provisioner&lt;/a&gt; (v1.0.1+) reserves the parameter keys prefixed with &lt;code&gt;csi.storage.k8s.io/&lt;/code&gt;. If the keys do not correspond to a set of known keys the values are simply ignored (and not passed to the CSI driver). The older secret parameter keys (&lt;code&gt;csiProvisionerSecretName&lt;/code&gt;, &lt;code&gt;csiProvisionerSecretNamespace&lt;/code&gt;, etc.) are also supported by CSI external-provisioner v1.0.1 but are deprecated and may be removed in future releases of the CSI external-provisioner.&lt;/p&gt;

&lt;p&gt;Dynamic provisioning is triggered by the creation of a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object. The following &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, for example, triggers dynamic provisioning using the &lt;code&gt;StorageClass&lt;/code&gt; above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-request-for-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When volume provisioning is invoked, the parameter type: &lt;code&gt;pd-ssd&lt;/code&gt; and the secret any referenced secret(s) are passed to the CSI plugin &lt;code&gt;csi-driver.example.com&lt;/code&gt; via a &lt;code&gt;CreateVolume&lt;/code&gt; call. In response, the external volume plugin provisions a new volume and then automatically create a &lt;code&gt;PersistentVolume&lt;/code&gt; object to represent the new volume. Kubernetes then binds the new &lt;code&gt;PersistentVolume&lt;/code&gt; object to the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, making it ready to use.&lt;/p&gt;

&lt;p&gt;If the &lt;code&gt;fast-storage  StorageClass&lt;/code&gt; is marked as “default”, there is no need to include the &lt;code&gt;storageClassName&lt;/code&gt; in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, it will be used by default.&lt;/p&gt;

&lt;h3 id=&#34;pre-provisioned-volumes&#34;&gt;Pre-Provisioned Volumes&lt;/h3&gt;

&lt;p&gt;You can always expose a pre-existing volume in Kubernetes by manually creating a PersistentVolume object to represent the existing volume. The following &lt;code&gt;PersistentVolume&lt;/code&gt;, for example, exposes a volume with the name “&lt;code&gt;existingVolumeName&lt;/code&gt;” belonging to a CSI storage plugin called “&lt;code&gt;csi-driver.example.com&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manually-created-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: csi-driver.example.com
    volumeHandle: existingVolumeName
    readOnly: false
    fsType: ext4
    volumeAttributes:
      foo: bar
    controllerPublishSecretRef:
      name: mysecret1
      namespace: mynamespace
    nodeStageSecretRef:
      name: mysecret2
      namespace: mynamespace
    nodePublishSecretRef
      name: mysecret3
      namespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;attaching-and-mounting&#34;&gt;Attaching and Mounting&lt;/h3&gt;

&lt;p&gt;You can reference a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; that is bound to a CSI volume in any pod or pod template.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
      - mountPath: &amp;quot;/var/www/html&amp;quot;
        name: my-csi-volume
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: my-request-for-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (&lt;code&gt;ControllerPublishVolume&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, &lt;code&gt;NodePublishVolume&lt;/code&gt;, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.&lt;/p&gt;

&lt;p&gt;For more details please see the CSI implementation &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#csi&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-to-write-a-csi-driver&#34;&gt;How to write a CSI Driver?&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://kubernetes-csi.github.io/&#34; target=&#34;_blank&#34;&gt;kubernetes-csi&lt;/a&gt; site details how to develop, deploy, and test a CSI driver on Kubernetes. In general, CSI Drivers should be deployed on Kubernetes along with the following sidecar (helper) containers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34; target=&#34;_blank&#34;&gt;external-attacher&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; objects and triggers &lt;code&gt;ControllerPublish&lt;/code&gt; and &lt;code&gt;ControllerUnpublish&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;external-provisioner&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; objects and triggers &lt;code&gt;CreateVolume&lt;/code&gt; and &lt;code&gt;DeleteVolume&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/node-driver-registrar&#34; target=&#34;_blank&#34;&gt;node-driver-registrar&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Registers the CSI driver with kubelet using the &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration&#34;&gt;Kubelet device plugin mechanism&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/cluster-driver-registrar&#34; target=&#34;_blank&#34;&gt;cluster-driver-registrar&lt;/a&gt; (Alpha)

&lt;ul&gt;
&lt;li&gt;Registers a CSI Driver with the Kubernetes cluster by creating a &lt;code&gt;CSIDriver&lt;/code&gt; object which enables the driver to customize how Kubernetes interacts with it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;external-snapshotter&lt;/a&gt; (Alpha)

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;VolumeSnapshot&lt;/code&gt; CRD objects and triggers &lt;code&gt;CreateSnapshot&lt;/code&gt; and &lt;code&gt;DeleteSnapshot&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/livenessprobe&#34; target=&#34;_blank&#34;&gt;livenessprobe&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;May be included in a CSI plugin pod to enable the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34;&gt;Kubernetes Liveness Probe&lt;/a&gt; mechanism.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;list-of-csi-drivers&#34;&gt;List of CSI Drivers&lt;/h2&gt;

&lt;p&gt;CSI drivers are developed and maintained by third parties. You can find a non-definitive list of CSI drivers &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-about-in-tree-volume-plugins&#34;&gt;What about in-tree volume plugins?&lt;/h2&gt;

&lt;p&gt;There is a plan to migrate most of the persistent, remote in-tree volume plugins to CSI. For more details see &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;limitations-of-ga&#34;&gt;Limitations of GA&lt;/h2&gt;

&lt;p&gt;The GA implementation of CSI has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ephemeral local volumes must create a PVC (pod inline referencing of CSI volumes is not supported).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Work on moving Kubernetes CSI features that are still alpha to beta:

&lt;ul&gt;
&lt;li&gt;Raw block volumes&lt;/li&gt;
&lt;li&gt;Topology awareness (the ability for Kubernetes to understand and influence where a CSI volume is provisioned (zone, regions, etc.).&lt;/li&gt;
&lt;li&gt;Features depending on CSI CRDs (e.g. “Skip attach” and “Pod info on mount”).&lt;/li&gt;
&lt;li&gt;Volume Snapshots&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Work on completing support for local ephemeral volumes.&lt;/li&gt;
&lt;li&gt;Work on migrating remote persistent in-tree volume plugins to CSI.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved?&lt;/h2&gt;

&lt;p&gt;The Kubernetes Slack channel &lt;a href=&#34;https://kubernetes.slack.com/messages/C8EJ01Z46/details/&#34; target=&#34;_blank&#34;&gt;wg-csi&lt;/a&gt; and the Google group &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-storage-wg-csi&#34; target=&#34;_blank&#34;&gt;kubernetes-sig-storage-wg-csi&lt;/a&gt; along with any of the standard &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34; target=&#34;_blank&#34;&gt;SIG storage communication channels&lt;/a&gt; are all great mediums to reach out to the SIG Storage team.&lt;/p&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the new contributors who stepped up this quarter to help the project reach GA:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Serguei Bezverkhi (&lt;a href=&#34;https://github.com/sbezverk&#34; target=&#34;_blank&#34;&gt;sbezverk&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Masaki Kimura (&lt;a href=&#34;https://github.com/mkimuram&#34; target=&#34;_blank&#34;&gt;mkimuram&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Patrick Ohly (&lt;a href=&#34;https://github.com/pohly&#34; target=&#34;_blank&#34;&gt;pohly&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Luis Pabón (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Vladimir Vivien (&lt;a href=&#34;https://github.com/vladimirvivien&#34; target=&#34;_blank&#34;&gt;vladimirvivien&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: APIServer dry-run and kubectl diff</title>
      <link>https://kubernetes.io/blog/2019/01/14/apiserver-dry-run-and-kubectl-diff/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/01/14/apiserver-dry-run-and-kubectl-diff/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Antoine Pelisse (Google Cloud, @apelisse)&lt;/p&gt;

&lt;p&gt;Declarative configuration management, also known as configuration-as-code, is
one of the key strengths of Kubernetes. It allows users to commit the desired state of
the cluster, and to keep track of the different versions, improve auditing and
automation through CI/CD pipelines. The &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-wg-apply&#34; target=&#34;_blank&#34;&gt;Apply working-group&lt;/a&gt;
is working on fixing some of the gaps, and is happy to announce that Kubernetes
1.13 promoted server-side dry-run and &lt;code&gt;kubectl diff&lt;/code&gt; to beta. These
two features are big improvements for the Kubernetes declarative model.&lt;/p&gt;

&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;A few pieces are still missing in order to have a seamless declarative
experience with Kubernetes, and we tried to address some of these:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;While compilers and linters do a good job to detect errors in pull-requests
for code, a good validation is missing for Kubernetes configuration files.
The existing solution is to run &lt;code&gt;kubectl apply --dry-run&lt;/code&gt;, but this runs a
&lt;em&gt;local&lt;/em&gt; dry-run that doesn&amp;rsquo;t talk to the server: it doesn&amp;rsquo;t have server
validation and doesn&amp;rsquo;t go through validating admission controllers. As an
example, Custom resource names are only validated on the server so a local
dry-run won&amp;rsquo;t help.&lt;/li&gt;
&lt;li&gt;It can be difficult to know how your object is going to be applied by the
server for multiple reasons:

&lt;ul&gt;
&lt;li&gt;Defaulting will set some fields to potentially unexpected values,&lt;/li&gt;
&lt;li&gt;Mutating webhooks might set fields or clobber/change some values.&lt;/li&gt;
&lt;li&gt;Patch and merges can have surprising effects and result in unexpected
objects. For example, it can be hard to know how lists are going to be
ordered once merged.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The working group has tried to address these problems.&lt;/p&gt;

&lt;h2 id=&#34;apiserver-dry-run&#34;&gt;APIServer dry-run&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-concepts/#dry-run&#34;&gt;APIServer dry-run&lt;/a&gt; was implemented to address these two problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it allows individual requests to the apiserver to be marked as &amp;ldquo;dry-run&amp;rdquo;,&lt;/li&gt;
&lt;li&gt;the apiserver guarantees that dry-run requests won&amp;rsquo;t be persisted to storage,&lt;/li&gt;
&lt;li&gt;the request is still processed as typical request: the fields are
defaulted, the object is validated, it goes through the validation admission
chain, and through the mutating admission chain, and then the final object is
returned to the user as it normally would, without being persisted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While dynamic admission controllers are not supposed to have side-effects on
each request, dry-run requests are only processed if all admission controllers
explicitly announce that they don&amp;rsquo;t have any dry-run side-effects.&lt;/p&gt;

&lt;h3 id=&#34;how-to-enable-it&#34;&gt;How to enable it&lt;/h3&gt;

&lt;p&gt;Server-side dry-run is enabled through a feature-gate. Now that the feature is
Beta in 1.13, it should be enabled by default, but still can be enabled/disabled
using &lt;code&gt;kube-apiserver --feature-gates DryRun=true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have dynamic admission controllers, you might have to fix them to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Remove any side-effects when the dry-run parameter is specified on the webhook request,&lt;/li&gt;
&lt;li&gt;Specify in the &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#webhook-v1beta1-admissionregistration&#34;&gt;&lt;code&gt;sideEffects&lt;/code&gt;&lt;/a&gt;
field of the &lt;code&gt;admissionregistration.k8s.io/v1beta1.Webhook&lt;/code&gt; object to indicate that the object doesn&amp;rsquo;t
have side-effects on dry-run (or at all).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-to-use-it&#34;&gt;How to use it&lt;/h3&gt;

&lt;p&gt;You can trigger the feature from kubectl by using &lt;code&gt;kubectl apply
--server-dry-run&lt;/code&gt;, which will decorate the request with the dryRun flag
and return the object as it would have been applied, or an error if it would
have failed.&lt;/p&gt;

&lt;h2 id=&#34;kubectl-diff&#34;&gt;Kubectl diff&lt;/h2&gt;

&lt;p&gt;APIServer dry-run is convenient because it lets you see how the object would be
processed, but it can be hard to identify exactly what changed if the object is
big. &lt;code&gt;kubectl diff&lt;/code&gt; does exactly what you want by showing the differences between
the current &amp;ldquo;live&amp;rdquo; object and the new &amp;ldquo;dry-run&amp;rdquo; object. It makes it very
convenient to focus on only the changes that are made to the object, how the
server has merged these and how the mutating webhooks affects the output.&lt;/p&gt;

&lt;h3 id=&#34;how-to-use-it-1&#34;&gt;How to use it&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;kubectl diff&lt;/code&gt; is meant to be as similar as possible to &lt;code&gt;kubectl apply&lt;/code&gt;:
&lt;code&gt;kubectl diff -f some-resources.yaml&lt;/code&gt; will show a diff for the resources in the yaml file. One can even use the diff program of their choice by using the KUBECTL_EXTERNAL_DIFF environment variable, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECTL_EXTERNAL_DIFF=meld kubectl diff -f some-resources.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;The working group is still busy trying to improve some of these things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server-side apply is trying to improve the apply scenario, by adding owner
semantics to fields! It&amp;rsquo;s also going to improve support for CRDs and unions!&lt;/li&gt;
&lt;li&gt;Some kubectl apply features are missing from diff and could be useful, like the ability
to filter by label, or to display pruned resources.&lt;/li&gt;
&lt;li&gt;Eventually, kubectl diff will use server-side apply!&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Federation Evolution</title>
      <link>https://kubernetes.io/blog/2018/12/12/kubernetes-federation-evolution/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/12/12/kubernetes-federation-evolution/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Irfan Ur Rehman (Huawei), Paul Morie (RedHat) and Shashidhara T D (Huawei)&lt;/p&gt;

&lt;p&gt;Kubernetes provides great primitives for deploying applications to a cluster: it can be as simple as &lt;code&gt;kubectl create -f app.yaml&lt;/code&gt;. Deploy apps across multiple clusters has never been that simple. How should app workloads be distributed? Should the app resources be replicated into all clusters, replicated into selected clusters, or partitioned into clusters? How is  access to the clusters managed? What happens if some of the resources that a user wants to distribute pre-exist, in some or all of the clusters, in some form?&lt;/p&gt;

&lt;p&gt;In SIG Multicluster, our journey has revealed that there are multiple possible models to solve these problems and there probably is no single best-fit, all-scenario solution. &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34;&gt;Federation&lt;/a&gt;, however, is the single biggest Kubernetes open source sub-project, and has seen the maximum interest and contribution from the community in this problem space. The project initially reused the Kubernetes API to do away with any added usage complexity for an existing Kubernetes user. This approach was not viable, because of the problems summarised below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Difficulties in re-implementing the Kubernetes API at the cluster level, as federation-specific extensions were stored in annotations.&lt;/li&gt;
&lt;li&gt;Limited flexibility in federated types, placement and reconciliation, due to 1:1 emulation of the Kubernetes API.&lt;/li&gt;
&lt;li&gt;No settled path to GA, and general confusion on API maturity; for example, Deployments are GA in Kubernetes but not even Beta in Federation v1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ideas have evolved further with a federation-specific API architecture and a community effort which now continues as Federation v2.&lt;/p&gt;

&lt;h1 id=&#34;conceptual-overview&#34;&gt;Conceptual Overview&lt;/h1&gt;

&lt;p&gt;Because Federation attempts to address a complex set of problems, it pays to break the different parts of those problems down. Let’s take a look at the different high-level areas involved:
&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-11-Kubernetes-Federation-Evolution/concepts.png&#34;
         alt=&#34;Kubernetes Federation v2 Concepts&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Kubernetes Federation v2 Concepts&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;federating-arbitrary-resources&#34;&gt;Federating arbitrary resources&lt;/h2&gt;

&lt;p&gt;One of the main goals of Federation is to be able to define the APIs and API groups which encompass basic tenets needed to federate any given Kubernetes resource. This is crucial, due to the popularity of CustomResourceDefinitions as a way to extend Kubernetes with new APIs.&lt;/p&gt;

&lt;p&gt;The workgroup arrived at a common definition of the federation API and API groups as &lt;em&gt;&amp;lsquo;a mechanism that distributes “normal” Kubernetes API resources into different clusters&amp;rsquo;&lt;/em&gt;. The distribution in its most simple form could be imagined as &lt;strong&gt;&lt;em&gt;simple propagation&lt;/em&gt;&lt;/strong&gt; of this &lt;em&gt;&amp;lsquo;normal Kubernetes API resource&amp;rsquo;&lt;/em&gt; across the federated clusters. A thoughtful reader can certainly discern more complicated mechanisms, other than this simple propagation of the Kubernetes resources.&lt;/p&gt;

&lt;p&gt;During the journey of defining building blocks of the federation APIs, one of the near term goals also evolved as &lt;em&gt;&amp;lsquo;to be able to create a simple federation a.k.a. simple propagation of any Kubernetes resource or a CRD, writing almost zero code&amp;rsquo;&lt;/em&gt;. What ensued further was a core API group defining the building blocks as a &lt;code&gt;Template&lt;/code&gt; resource, a &lt;code&gt;Placement&lt;/code&gt; resource and an &lt;code&gt;Override&lt;/code&gt; resource per given Kubernetes resource, a &lt;code&gt;TypeConfig&lt;/code&gt; to specify sync or no sync for the given resource and associated controller(s) to carry out the sync. More details follow &lt;a href=&#34;#federating-resources-the-details&#34;&gt;in the next section&lt;/a&gt;. Further sections will also talk about being able to follow a layered behaviour with higher-level federation APIs consuming the behaviour of these core building blocks, and users being able to consume whole or part of the API and associated controllers. Lastly, this architecture also allows the users to write additional controllers or replace the available reference controllers with their own, to carry out desired behaviour.&lt;/p&gt;

&lt;p&gt;The ability to &lt;em&gt;&amp;lsquo;easily federate arbitrary Kubernetes resources&amp;rsquo;&lt;/em&gt;, and a decoupled API, divided into building blocks APIs, higher level APIs and possible user intended types, presented such that different users can consume parts and write controllers composing solutions specific to them, makes a compelling case for Federation v2.&lt;/p&gt;

&lt;h2 id=&#34;federating-resources-the-details&#34;&gt;Federating resources: the details&lt;/h2&gt;

&lt;p&gt;Fundamentally, federation must be configured with two types of information:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which API types federation should handle&lt;/li&gt;
&lt;li&gt;Which clusters federation should target for distributing those resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each API type that federation handles, different parts of the declared state live in different API resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Template&lt;/code&gt; type holds the base specification of the resource - for example, a type called &lt;code&gt;FederatedReplicaSet&lt;/code&gt; holds the base specification of a &lt;code&gt;ReplicaSet&lt;/code&gt; that should be distributed to the targeted clusters&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Placement&lt;/code&gt; type holds the specification of the clusters the resource should be distributed to - for example, a type called &lt;code&gt;FederatedReplicaSetPlacement&lt;/code&gt; holds information about which clusters &lt;code&gt;FederatedReplicaSets&lt;/code&gt; should be distributed to&lt;/li&gt;
&lt;li&gt;An optional &lt;code&gt;Overrides&lt;/code&gt; type holds the specification of how the &lt;code&gt;Template&lt;/code&gt; resource should be varied in some clusters - for example, a type called &lt;code&gt;FederatedReplicaSetOverrides&lt;/code&gt; holds information about how a &lt;code&gt;FederatedReplicaSet&lt;/code&gt; should be varied in certain clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These types are all associated by name - meaning that for a particular Template resource with name &lt;code&gt;foo&lt;/code&gt;, the Placement and Override information for that resource are contained by the Override and Placement resources with the name &lt;code&gt;foo&lt;/code&gt; and in the same namespace as the Template.&lt;/p&gt;

&lt;h2 id=&#34;higher-level-behaviour&#34;&gt;Higher-level behaviour&lt;/h2&gt;

&lt;p&gt;The architecture of the v2 API allows higher-level APIs to be constructed using the mechanics provided by the core API types (&lt;code&gt;Template&lt;/code&gt;, &lt;code&gt;Placement&lt;/code&gt; and &lt;code&gt;Override&lt;/code&gt;), and associated controllers, for a given resource. In the community we uncovered a few use cases and implemented the higher-level APIs and associated controllers useful for those cases. Some of these types described in further sections also provide an useful reference to anybody interested in solving more complex use cases, building on top of the mechanics already available with the v2 API.&lt;/p&gt;

&lt;h3 id=&#34;replicaschedulingpreference&#34;&gt;ReplicaSchedulingPreference&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;ReplicaSchedulingPreference&lt;/code&gt; provides an automated mechanism of distributing and maintaining total number of replicas for Deployment or ReplicaSet-based federated workloads into federated clusters. This is based on high-level user preferences given by the user. These preferences include the semantics of &lt;em&gt;weighted distribution&lt;/em&gt; and &lt;em&gt;limits&lt;/em&gt; (min and max) for distributing the replicas. These also include semantics to allow redistribution of replicas dynamically in case some replica Pods remain unscheduled in some clusters, for example due to insufficient resources in that cluster.
More details can be found at the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#replicaschedulingpreference&#34; target=&#34;_blank&#34;&gt;user guide for ReplicaSchedulingPreferences&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;federated-services-cross-cluster-service-discovery&#34;&gt;Federated services &amp;amp; cross-cluster service discovery&lt;/h3&gt;

&lt;p&gt;Kubernetes Services are very useful in constructing a microservices architecture. There is a clear desire to deploy services across cluster, zone, region and cloud boundaries. Services that span clusters provide geographic distribution, enable hybrid and multi-cloud scenarios and improve the level of high availability beyond single cluster deployments. Customers who want their services to span one or more (possibly remote) clusters, need them to be reachable in a consistent manner from both within and outside their clusters.&lt;/p&gt;

&lt;p&gt;Federated &lt;code&gt;Service&lt;/code&gt;, at its core, contains a &lt;code&gt;Template&lt;/code&gt; (a definition of a Kubernetes Service), a &lt;code&gt;Placement&lt;/code&gt; (which clusters to be deployed into), an &lt;code&gt;Override&lt;/code&gt; (optional variation in particular clusters) and a &lt;code&gt;ServiceDNSRecord&lt;/code&gt; (specifying details on how to discover it).&lt;/p&gt;

&lt;p&gt;Note: The federated service has to be of type &lt;code&gt;LoadBalancer&lt;/code&gt; in order for it to be discoverable across clusters.&lt;/p&gt;

&lt;h4 id=&#34;discovering-a-federated-service-from-pods-inside-your-federated-clusters&#34;&gt;Discovering a federated service from Pods inside your federated clusters&lt;/h4&gt;

&lt;p&gt;By default, Kubernetes clusters come preconfigured with a cluster-local DNS server, as well as an intelligently constructed DNS search path, which together ensure that DNS queries like &lt;code&gt;myservice&lt;/code&gt;, &lt;code&gt;myservice.mynamespace&lt;/code&gt;, or &lt;code&gt;some-other-service.other-namespace&lt;/code&gt;, issued by software running inside Pods, are automatically expanded and resolved correctly to the appropriate IP of Services running in the local cluster.&lt;/p&gt;

&lt;p&gt;With the introduction of federated services and cross-cluster service discovery, this concept is extended to cover Kubernetes Services running in any other cluster across your cluster federation, globally. To take advantage of this extended range, you use a slightly different DNS name (e.g. &lt;code&gt;myservice.mynamespace.myfederation&lt;/code&gt;) to resolve federated services. Using a different DNS name also avoids having your existing applications accidentally traversing cross-zone or cross-region networks and you incurring perhaps unwanted network charges or latency, without you explicitly opting in to this behavior.&lt;/p&gt;

&lt;p&gt;Lets consider an example, using a service named &lt;code&gt;nginx&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A Pod in a cluster in the &lt;code&gt;us-central1-a&lt;/code&gt; availability zone needs to contact our &lt;code&gt;nginx&lt;/code&gt; service. Rather than use the service’s traditional cluster-local DNS name (&lt;code&gt;nginx.mynamespace&lt;/code&gt;, which is automatically expanded to &lt;code&gt;nginx.mynamespace.svc.cluster.local&lt;/code&gt;) it can now use the service’s federated DNS name, which is &lt;code&gt;nginx.mynamespace.myfederation&lt;/code&gt;. This will be automatically expanded and resolved to the closest healthy shard of my &lt;code&gt;nginx&lt;/code&gt; service, wherever in the world that may be. If a healthy shard exists in the local cluster, that service’s cluster-local IP address will be returned (by the cluster-local DNS). This is exactly equivalent to non-federated service resolution.&lt;/p&gt;

&lt;p&gt;If the Service does not exist in the local cluster (or it exists but has no healthy backend pods), the DNS query is automatically expanded to &lt;code&gt;nginx.mynamespace.myfederation.svc.us-central1-a.us-central1.example.com&lt;/code&gt;. Behind the scenes, this finds the external IP of one of the shards closest to my availability zone. This expansion is performed automatically by the cluster-local DNS server, which returns the associated CNAME record. This results in a traversal of the hierarchy of DNS records, and ends up at one of the external IP’s of the federated service nearby.&lt;/p&gt;

&lt;p&gt;It is also possible to target service shards in availability zones and regions other than the ones local to a Pod by specifying the appropriate DNS names explicitly, and not relying on automatic DNS expansion. For example, &lt;code&gt;nginx.mynamespace.myfederation.svc.europe-west1.example.com&lt;/code&gt;will resolve to all of the currently healthy service shards in Europe, even if the Pod issuing the lookup is located in the U.S., and irrespective of whether or not there are healthy shards of the service in the U.S. This is useful for remote monitoring and other similar applications.&lt;/p&gt;

&lt;h4 id=&#34;discovering-a-federated-service-from-other-clients-outside-your-federated-clusters&#34;&gt;Discovering a federated service from other clients outside your federated clusters&lt;/h4&gt;

&lt;p&gt;For external clients, automatic DNS expansion described is not currently possible. External clients need to specify one of the fully qualified DNS names of the federated service, be that a zonal, regional or global name. For convenience reasons, it is often a good idea to manually configure additional static CNAME records in your service, for example:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SHORT NAME&lt;/th&gt;
&lt;th&gt;CNAME&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;eu.nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.europe-west1.example.com&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;us.nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.us-central1.example.com&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.example.com&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;That way, your clients can always use the short form on the left, and always be automatically routed to the closest healthy shard on their home continent. All of the required failover is handled for you automatically by Kubernetes cluster federation.&lt;/p&gt;

&lt;p&gt;As further reading, a more elaborate example for users is available in the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/servicedns-with-externaldns.md&#34; target=&#34;_blank&#34;&gt;Multi-Cluster Service DNS with ExternalDNS guide&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;try-it-yourself&#34;&gt;Try it yourself&lt;/h1&gt;

&lt;p&gt;To get started with Federation v2, please refer to the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md&#34; target=&#34;_blank&#34;&gt;user guide&lt;/a&gt;. Deployment can be accomplished with a &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/charts/federation-v2/README.md&#34; target=&#34;_blank&#34;&gt;Helm chart&lt;/a&gt;, and once the control plane is available, the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#example&#34; target=&#34;_blank&#34;&gt;user guide’s example&lt;/a&gt; can be used to get some hands-on experience with using Federation V2.&lt;/p&gt;

&lt;p&gt;Federation v2 can be deployed in both &lt;em&gt;cluster-scoped&lt;/em&gt; and &lt;em&gt;namespace-scoped&lt;/em&gt; configurations.  A cluster-scoped deployment will require cluster-admin privileges to both host and member clusters, and may be a good fit for evaluating federation on clusters that are not running critical workloads. Namespace-scoped deployment requires access to only a single namespace on host and member clusters, and is a better fit for evaluating federation on clusters running workloads.  Most of the user guide refers to cluster-scoped deployment, with the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#namespaced-federation&#34; target=&#34;_blank&#34;&gt;namespaced federation&lt;/a&gt; section documenting how use of a namespaced deployment differs.  The same cluster can host multiple federations, and clusters can be part of multiple federations when using namespaced federation.&lt;/p&gt;

&lt;h1 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h1&gt;

&lt;p&gt;As we noted in the beginning of this post, the multicluster problem space is extremely broad. It can be difficult to know exactly how to handle such broad problem spaces without concrete pieces of software to frame those conversations around. Our hope in the Federation working group is that Federation v2 can be a concrete artifact to frame discussions around. We would love to know experiences that folks have had in this problem space, how they feel about Federation v2, and what use-cases they’re interested in exploring in the future.&lt;/p&gt;

&lt;p&gt;Please feel welcome to join us at the &lt;a href=&#34;https://kubernetes.slack.com/messages/C09R1PJR3&#34; target=&#34;_blank&#34;&gt;sig-multicluster slack channel&lt;/a&gt; or at &lt;a href=&#34;https://docs.google.com/document/d/1FQx0BPlkkl1Bn0c9ocVBxYIKojpmrS1CFP5h0DI68AE/edit&#34; target=&#34;_blank&#34;&gt;Federation working group meetings&lt;/a&gt; on Wednesdays at 07:30 PST.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
